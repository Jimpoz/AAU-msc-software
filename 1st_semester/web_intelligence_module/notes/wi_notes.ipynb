{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4800eab8",
   "metadata": {},
   "source": [
    "# COURSE OVERVIEW\n",
    "\n",
    "# The core topics that will be covered in the course include:\n",
    "- Information retrieval, search, and ranking\n",
    "- Neural networks\n",
    "- Language models\n",
    "- Recommender systems\n",
    "- Network modelling and embedding\n",
    "\n",
    "# LECTURE 02/09/2025\n",
    "--- \n",
    "## What is the Web?\n",
    "The Web is a\n",
    "- global decentralized hypertext-based information system.\n",
    "- Hyperlinks are used to navigate from one document to another.\n",
    "- This information space build on a set of technical standards for\n",
    "the identification, retrieval and representation of content.\n",
    "---\n",
    "## World Wide Web (WWW)\n",
    "- The World Wide Web (referred to as WWW or W3 or\n",
    "simply Web), developed by Tim Berners-Lee in 1989 at\n",
    "CERN, Switzerland\n",
    "- The project document described a \"hypertext project\"\n",
    "called \"WorldWideWeb\" in which a \"web\" of\n",
    "\"hypertext documents\" could be viewed by\n",
    "“browsers”.[1]\n",
    "- First Website in 1991: info.cern.ch\n",
    "- First Webpage address:\n",
    "http://info.cern.ch/hypertext/WWW/TheProject.html\n",
    "- First Browser\n",
    "- In 1993: Graphical Web Browsers such as\n",
    "Mosiac and Netscape Navigator were made\n",
    "accessible outside of academia.\n",
    "- In 1994: W3C (World Wide Web Consortium)\n",
    "was founded by Tim Berners Lee. W3C\n",
    "publishes recommendations, that are\n",
    "considered web standards.\n",
    "- Web standards are blueprints –or building\n",
    "blocks– of a consistent and harmonious\n",
    "digitally connected world. They are\n",
    "implemented in browsers, blogs, search\n",
    "engines, and other software that power our\n",
    "experience on the web.\n",
    "---\n",
    "## World Wide Web - Data Model\n",
    "\n",
    "W3 data model enables:\n",
    "- Information need only be represented once, as a reference may be made instead of making a copy.\n",
    "- Links allow the topology of the information to evolve, so modelling the state of human knowledge at any time is without constraint.\n",
    "- The web stretches seamlessly from small personal notes on the local workstation to large databases on other continents.\n",
    "- Indexes such as phone books are presented as documents, and so may themselves be found by searches and/or following links.\n",
    "- The documents in the web do not have to exist as files; they can be \"virtual\" documents generated by a server in response to a query\n",
    "or document name. They can therefore represent views of databases, or snapshots of changing data (such as the weather forecasts,\n",
    "financial information, etc.).\n",
    "Advantages:\n",
    "- Information access doesn’t require expert knowledge\n",
    "- Information Retrieval via search engines\n",
    "---\n",
    "## World Wide Web - Components\n",
    "The World Wide Web (WWW) comprise of different components:\n",
    "- Identification: Universal Resource Identifiers (URIs)- Address system;\n",
    "globally unique identification of the web resources.\n",
    "For e.g., the URI of the main page for the first WWW project is\n",
    "http://info.cern.ch/hypertext/WWW/TheProject.html\n",
    "- Interaction: Hypertext Transfer Protocol (HTTP) - network protocol used\n",
    "for transferring information/interacting between the web resources. The\n",
    "data transferred can be plain text, hypertext, images, etc.\n",
    "- Content Format: Hypertext Markup Language (HTML) - a markup\n",
    "language, used to define the structure and the content of the webpage.\n",
    "HTML supports various content types, including text, images, video,\n",
    "audio, scripts, and hyperlinks for easy web resource access.\n",
    "---\n",
    "## Topology of the Web\n",
    "\n",
    "The web's structure can be broken down into three overlapping layers:\n",
    "1. Classic Document Web (Web 1.0): A network of static pages connected by hyperlinks. Its topology is a simple web of linked documents.\n",
    "2. Social & Application Web (Web 2.0): A platform for dynamic applications where the connections are user interactions, social ties, and data exchange via APIs.\n",
    "3. Web of Data (Semantic Web): A network of machine-readable data, not pages. The connections are defined relationships between concepts, forming a global knowledge graph.\n",
    "\n",
    "These layers coexist and intersect. A modern website is a document (1), that hosts an interactive application (2), and contains structured data for machines (3).\n",
    "\n",
    "---\n",
    "\n",
    "## What is Web Intelligence?\n",
    "- Intelligent ways to extract information and knowledge from the web:\n",
    "- finding relevant information available on the web\n",
    "- obtaining new knowledge by analyzing web data: the web itself, but also how it evolves, and how users interact on and with the\n",
    "web\n",
    "Some applications:\n",
    "- Intelligent Search\n",
    "- Recommender Systems\n",
    "- Business Analytics\n",
    "- Crowd Sourcing\n",
    "- Not so nice ones: advertising, manipulation, surveillance\n",
    "---\n",
    "## Intelligent Search\n",
    "\n",
    "- Keyword Search: the words in the query appear frequently in\n",
    "the document, in any order (bag of words).\n",
    "- Disadvantages:\n",
    "- May not retrieve relevant documents that include\n",
    "synonymous terms (e.g., cannot distinguish between\n",
    "“restaurant” and “café”)\n",
    "- May retrieve irrelevant documents that include\n",
    "ambiguous terms (e.g., cannot distinguish between “bat”\n",
    "mammal and “bat” baseball)\n",
    "- Beyond Keywords:\n",
    "- Considering the meaning of the words used\n",
    "- Adapting to user feedback (direct or indirect)\n",
    "- Considering the authority of the source\n",
    "---\n",
    "## Recommender Systems\n",
    "Kinds of recommendations:\n",
    "- Product Based (collaborative filtering):\n",
    "similar books\n",
    "- User- Based (content based filtering):\n",
    "based on search history\n",
    "- Hybrid\n",
    "---\n",
    "## Node classification\n",
    "We can represent a domain as a network of nodes, each node represents a different category.\n",
    "- Example: suppose aau.dk is a domain and is connected by multiple nodes, then each color represents something different like research projects, educational programs, etc...\n",
    "---\n",
    "## Levels of the Web\n",
    "We can distinguish 3 levels of modeling and analytics:\n",
    "1. Web Content\n",
    "2. Web Structure\n",
    "3. Web Dynamics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87199867",
   "metadata": {},
   "source": [
    "# LECTURE 09/09/2025\n",
    "---\n",
    "# NLP BASICS\n",
    "\n",
    "Natural Language Processing is a sub field of Artificial Intelligence  and Computational LInguistics, comprising of computational methods for understanding or generating Natural Languages.\n",
    "The goals of this process is to:\n",
    "- read human language into machine understandable\n",
    "- decipher human languages\n",
    "- understand and makes sense of the text\n",
    "\n",
    "Natural Languages consist of:\n",
    "- phonology: sounds of the words\n",
    "- semantics: meaning of the words\n",
    "- syntax: grammatical rules according to which words are put together\n",
    "\n",
    "It is really difficult to analyze the text because of the ambiguity of the language, for example a word could have 2 different meanings depending on the context, or the positioning of a word could change the meaning, etc...\n",
    "\n",
    "The data of languages is unstructured and in order to make it machine understandable is to tokenize it however this process is really language dependant.\n",
    "\n",
    "---\n",
    "## Tokenization\n",
    "\n",
    "Given the phrase:\n",
    "\n",
    "J.K. Rowling’s book, Harry Potter and the Philosopher's Stone was published in 1997. It’s still a bestseller and truly amazing\n",
    "how much people enjoy it!\n",
    "\n",
    "- Word level tokenization:\n",
    "Tokenized words: [\"J.K.\", \"Rowling’s\", \"book\", \",\", \"Harry\", \"Potter\", \"and\", \"the\", \"Philosopher's\", \"Stone\", \"was\", \"published\", \"in\",\n",
    "\"1997\", \".\", \"It’s\", \"still\", \"a\", \"bestseller\", \"and\", \"truly\", \"amazing\", \"how\", \"much\", \"people\", \"enjoy\", \"it\", \"!\"]\n",
    "- Handling contractions:\n",
    "The contraction \"it’s” needs to be split into two tokens: ['it', \"'s\"]. Some systems might tokenize this as two tokens\n",
    "- Punctuation handling:\n",
    "Commas, apostrophes, and periods are separated from words, ensuring they are individual tokens\n",
    "- Named entity recognition (NER):\n",
    "The book title and the author are multi-word entities, which ideally should be handled as a single token or entity for proper semantic\n",
    "analysis.\n",
    "  - Harry Potter and the Philosopher's Stone\n",
    "  - J.K. Rowlin\n",
    "\n",
    "Most tokenizers are rule-based and have differental conventions for example for the world \"don't\":\n",
    "- Peen Treebank: it will divide the word into \"do\" and \"n't\"\n",
    "- Moses: it will divide the word into \"don\" and \"'t\"\n",
    "In this case moses is rule-based.\n",
    "\n",
    "Also we need to take into consideration composed words or names since the proces does involte removal of white spaces but it does not add them.\n",
    "Another important factor is the encoding of the text since the same symbol could have different ASCII values (in decimal) depending on the encoding.\n",
    "\n",
    "### Tokenization in Encoding\n",
    "\n",
    "**Tokenization** in encoding assigns a unique value (token) to each word so that the system can recognize and process words based on their assigned values.\n",
    "Example:\n",
    "\n",
    "I live in Copenhagen = [“I”, ”live”, “in”, “Copenhagen”]\\\n",
    "I = 1\\\n",
    "live = 2\\\n",
    "in = 3\\\n",
    "Copenhagen = 4\\\n",
    "\n",
    "We live in Copenhagen = [“We”, ”live”, “in”, “Copenhagen”]\\\n",
    "We = 5\\\n",
    "live = 2\\\n",
    "in = 3\\\n",
    "Copenhagen = 4\\\n",
    "\n",
    "---\n",
    "\n",
    "## Normalization\n",
    "\n",
    "This process transforms distinct 'equivalent' tokens into one normalized form, e.g.:\n",
    "- lower-casing: This -> this\n",
    "- use non-hyphonated words: anti-discriminatory -> antidiscriminatory\n",
    "- delete periods: U.S.A. -> USA or usa\n",
    "\n",
    "However we need to balance the normalization because the result will change depending on how the amount of normalization:\n",
    "- more normalization: means that if you search the word U.S.A. it also returns the results for usa or USA\n",
    "- less normalization: if you search for the word C.A.T. it doesn't show the results of cat\n",
    "\n",
    "---\n",
    "\n",
    "## Stop words\n",
    "These are common words that usually do not add significant meaning to a sentence and can be filtered out to reduce noise in text analysis.\n",
    "However the removal of stop words could cause issues.\n",
    "For example in a phrase such as \"to be or not to be\" which is full of stop words but are important part of the expression.\n",
    "\n",
    "---\n",
    "\n",
    "## Stemming\n",
    "It's a similar process to normalization in which it reduces and removes their different grammatical variants and transforms them into their underlying word.\n",
    "Here are a few examples:\n",
    "- learn, learnds, learned -> learn\n",
    "- organize, organizer, organization -> organ\n",
    "\n",
    "The terms that result from stemming will be included in the index.\n",
    "\n",
    "### Porter introduced an algorithm for the english language (1980):\n",
    "\n",
    "- A rule-based stemmer with rules for mostly suffix-stripping such as:\n",
    "- “ing” → “-” connecting → connect\n",
    "- “sses” → “ss” caresses → caress\n",
    "- “ies” → “i” ponies → poni\n",
    "- “s” → “-” cats → cat\n",
    "- `[C](VC){m}[V]`\n",
    "where C is the consonant, V is the vowel i.e., A,E,I,O,U and other than Y preceded by a consonant, m>0 is the number of times (VC) occurs.\n",
    "- m = 0, TREE, BY\n",
    "- m = 1, TROUBLE, OATS, TREES\n",
    "- m = 2, TROUBLES, OATEN, PRIVATE\n",
    "- The rules for removing suffix: (condition) S1 -> S2\n",
    "\n",
    "Porter’s Stemming Algorithm\n",
    "- `[C](VC){m}[V]`\n",
    "- Stem the word “REPLACEMENT”\n",
    "- m >1, remove EMENT\n",
    "- Generates a stem “replac”\n",
    "- Advantage:\n",
    "  - It produces the best output as compared to other stemmers, and it has less error rate\n",
    "- Disadvantage:\n",
    "  - Morphological variants produced are not always real words (produces stems)\n",
    "\n",
    "Use the algorithm to stem the word:\n",
    "MULTIDIMENSIONAL\n",
    "Remove common suffix: from this word there is no ing, sses, ies, s, ation, ization, etc...\n",
    "Removal of the suffix AL\n",
    "  - M U L T I D I M E N S I O N A L\n",
    "  - C V C C V C V C V C C V V C\n",
    "  -   V C   V C V C V C     V C\n",
    "  - m = 5\n",
    "\n",
    "There are three criteria for evaluating stemmers:\n",
    "- Correctness\n",
    "- Efficiency of the task\n",
    "- Compression performance\n",
    "\n",
    "However stemming is also language dependant (for example it does not work with Chinese), also stemming could cause an error depending on the amount:\n",
    "- overstemming: can remove too much from a word changing its meaning.\n",
    "- understemming: if wrongly reduced it can lead to different root words.\n",
    "\n",
    "---\n",
    "\n",
    "## Zipf's law\n",
    "Zipf's flaw in NLP describes the power-law distribution of word frequencies in a corpus, stating that the frequency of a word is inversely proportional to its rank.\n",
    "\n",
    "Example:\n",
    "“The cat sat on the mat, and the dog sat on the mat.”\n",
    "Tokenize:\n",
    "[“The”, “cat”, “sat”, “on”, “the”, “mat”, “,”, “and”, “the”, “dog”, “sat”, “on”,“the”, “mat”, ”.”] -> results in 13\n",
    "Type: a unique word\n",
    "Token: an instance of a type in a corpus (including repetitions)\n",
    "Word Frequencies:\n",
    "the: 3\n",
    "sat: 2\n",
    "on: 2\n",
    "mat: 2\n",
    "cat: 1\n",
    "dog: 1\n",
    "and: 1\n",
    "\n",
    "So the type/token ratio is 7/13, therefore more data -> lower type/token ratio\n",
    "\n",
    "Positive Implications of Zipf's law:\n",
    "- Any document/text will contain a number of words that are very common.\n",
    "- Help us understand the structure (and possibly meaning) of the text\n",
    "Negative Implications:\n",
    "- Any document/text will have a large number of rare words known as Out-of-vocabulary (OOV) words\n",
    "Handling OOV words:\n",
    "- Replace the set of OOV words in the training data with an unknown word token (-UNK)\n",
    "- Use of statistical models\n",
    "- Use sub-string based representations such as Byte-Pair Encoding\n",
    "Byte-Pair Encoding: learn which character sequences are common in the vocabulary of the language, and treat those common sequences\n",
    "as atomic units of the vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocessing\n",
    "Is the process in which a sentence is broken down through:\n",
    "- Tokenization\n",
    "- Normalization\n",
    "- Stop word removal\n",
    "- Stemming\n",
    "\n",
    "Definitions: \n",
    "- Corpus: collection of documents (e.g., all web-pages that have been crawled)\n",
    "- Raw corpora have only minimal (or no) processing:\\\n",
    "Sentence boundaries may or may not be identified.\\\n",
    "There may or may not be metadata.\\\n",
    "Typos (written text) or disfluencies (spoken language) may or may not be corrected.\n",
    "- Annotated corpora contain some labels (e.g. POS tags, sentiment labels), or linguistic structures (e.g syntax trees,\n",
    "semantic interpretations), etc.\n",
    "- Vocabulary: all terms that appear in the corpus\n",
    "\n",
    "---\n",
    "\n",
    "## Part of Speech (POS)\n",
    "Part-of-speech tagging, or POS tagging, is a task that entails\n",
    "classifying words in a text according to their grammatical\n",
    "categories (such as noun, verb, and adjective).\\\n",
    "Advantages:\n",
    "- Helps in identifying the syntactic structure\n",
    "- POS tagging often helps disambiguate the meaning of such\n",
    "words based on their role in the sentence.\n",
    "- Aids Named Entity Recognition\n",
    "\n",
    "---\n",
    "\n",
    "## String Similarity\n",
    "String similarity can be measured through:\n",
    "- Hamming Distance: between two equal-length strings of symbols is the number of positions at which the corresponding symbols are different.\n",
    "- Levenshtein Distance: measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another.\n",
    "- Jaccard Similarity: is a statistic to determine the similarity or the overlap between the two sets. Jaccard similarity = $\\frac{A \\cap B}{A \\cup B}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf0bf7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1498ddb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078d38d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b289176",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b376f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3a8deb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f03f0793",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f6ec100",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45407d1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
