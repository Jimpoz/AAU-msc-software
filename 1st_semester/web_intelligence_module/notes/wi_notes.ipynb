{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4800eab8",
   "metadata": {},
   "source": [
    "# COURSE OVERVIEW\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87199867",
   "metadata": {},
   "source": [
    "# LECTURE 09/09/2025\n",
    "\n",
    "# NLP BASICS\n",
    "\n",
    "Natural Language Processing is a sub field of Artificial Intelligence  and Computational LInguistics, comprising of computational methods for understanding or generating Natural Languages.\n",
    "The goals of this process is to:\n",
    "- read human language into machine understandable\n",
    "- decipher human languages\n",
    "- understand and makes sense of the text\n",
    "\n",
    "Natural Languages consist of:\n",
    "- phonology: sounds of the words\n",
    "- semantics: meaning of the words\n",
    "- syntax: grammatical rules according to which words are put together\n",
    "\n",
    "It is really difficult to analyze the text because of the ambiguity of the language, for example a word could have 2 different meanings depending on the context, or the positioning of a word could change the meaning, etc...\n",
    "\n",
    "The data of languages is unstructured and in order to make it machine understandable is to tokenize it however this process is really language dependant.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Given the phrase:\n",
    "\n",
    "J.K. Rowling’s book, Harry Potter and the Philosopher's Stone was published in 1997. It’s still a bestseller and truly amazing\n",
    "how much people enjoy it!\n",
    "\n",
    "- Word level tokenization:\n",
    "Tokenized words: [\"J.K.\", \"Rowling’s\", \"book\", \",\", \"Harry\", \"Potter\", \"and\", \"the\", \"Philosopher's\", \"Stone\", \"was\", \"published\", \"in\",\n",
    "\"1997\", \".\", \"It’s\", \"still\", \"a\", \"bestseller\", \"and\", \"truly\", \"amazing\", \"how\", \"much\", \"people\", \"enjoy\", \"it\", \"!\"]\n",
    "- Handling contractions:\n",
    "The contraction \"it’s” needs to be split into two tokens: ['it', \"'s\"]. Some systems might tokenize this as two tokens\n",
    "- Punctuation handling:\n",
    "Commas, apostrophes, and periods are separated from words, ensuring they are individual tokens\n",
    "- Named entity recognition (NER):\n",
    "The book title and the author are multi-word entities, which ideally should be handled as a single token or entity for proper semantic\n",
    "analysis.\n",
    "  - Harry Potter and the Philosopher's Stone\n",
    "  - J.K. Rowlin\n",
    "\n",
    "Most tokenizers are rule-based and have differental conventions for example for the world \"don't\":\n",
    "- Peen Treebank: it will divide the word into \"do\" and \"n't\"\n",
    "- Moses: it will divide the word into \"don\" and \"'t\"\n",
    "In this case moses is rule-based.\n",
    "\n",
    "Also we need to take into consideration composed words or names since the proces does involte removal of white spaces but it does not add them.\n",
    "Another important factor is the encoding of the text since the same symbol could have different ASCII values (in decimal) depending on the encoding.\n",
    "\n",
    "### Tokenization in Encoding\n",
    "\n",
    "**Tokenization** in encoding assigns a unique value (token) to each word so that the system can recognize and process words based on their assigned values.\n",
    "Example:\n",
    "\n",
    "I live in Copenhagen = [“I”, ”live”, “in”, “Copenhagen”]\n",
    "I = 1\n",
    "live = 2\n",
    "in = 3\n",
    "Copenhagen = 4\n",
    "\n",
    "We live in Copenhagen = [“We”, ”live”, “in”, “Copenhagen”]\n",
    "We = 5\n",
    "live = 2\n",
    "in = 3\n",
    "Copenhagen = 4\n",
    "\n",
    "\n",
    "## Normalization\n",
    "\n",
    "This process transforms distinct 'equivalent' tokens into one normalized form, e.g.:\n",
    "- lower-casing: This -> this\n",
    "- use non-hyphonated words: anti-discriminatory -> antidiscriminatory\n",
    "- delete periods: U.S.A. -> USA or usa\n",
    "\n",
    "However we need to balance the normalization because the result will change depending on how the amount of normalization:\n",
    "- more normalization: means that if you search the word U.S.A. it also returns the results for usa or USA\n",
    "- less normalization: if you search for the word C.A.T. it doesn't show the results of cat\n",
    "\n",
    "## Stop words\n",
    "These are common words that usually do not add significant meaning to a sentence and can be filtered out to reduce noise in text analysis.\n",
    "However the removal of stop words could cause issues.\n",
    "For example in a phrase such as \"to be or not to be\" which is full of stop words but are important part of the expression.\n",
    "\n",
    "## Stemming\n",
    "It's a similar process to normalization in which it reduces and removes their different grammatical variants and transforms them into their underlying word.\n",
    "Here are a few examples:\n",
    "- learn, learnds, learned -> learn\n",
    "- organize, organizer, organization -> organ\n",
    "\n",
    "The terms that result from stemming will be included in the index.\n",
    "\n",
    "### Porter introduced an algorithm for the english language (1980):\n",
    "\n",
    "- A rule-based stemmer with rules for mostly suffix-stripping such as:\n",
    "- “ing” → “-” connecting → connect\n",
    "- “sses” → “ss” caresses → caress\n",
    "- “ies” → “i” ponies → poni\n",
    "- “s” → “-” cats → cat\n",
    "- [C](VC){m}[V]\n",
    "where C is the consonant, V is the vowel i.e., A,E,I,O,U and other than Y preceded by a consonant, m>0 is the number of times (VC) occurs.\n",
    "- m = 0, TREE, BY\n",
    "- m = 1, TROUBLE, OATS, TREES\n",
    "- m = 2, TROUBLES, OATEN, PRIVATE\n",
    "- The rules for removing suffix: (condition) S1 -> S2\n",
    "- \n",
    "Porter’s Stemming Algorithm\n",
    "- [C](VC){m}[V]\n",
    "- Stem the word “REPLACEMENT”\n",
    "- m >1, remove EMENT\n",
    "- Generates a stem “replac”\n",
    "- Advantage:\n",
    "  - It produces the best output as compared to other stemmers, and it has less error rate\n",
    "- Disadvantage:\n",
    "  - Morphological variants produced are not always real words (produces stems)\n",
    "\n",
    "Use the algorithm to stem the word:\n",
    "MULTIDIMENSIONAL\n",
    "Remove common suffix: from this word there is no ing, sses, ies, s, ation, ization, etc...\n",
    "Removal of the suffix AL\n",
    "  - M U L T I D I M E N S I O N A L\n",
    "  - C V C C V C V C V C C V V C\n",
    "  -   V C   V C V C V C     V C\n",
    "  - m = 5\n",
    "\n",
    "There are three criteria for evaluating stemmers:\n",
    "- Correctness\n",
    "- Efficiency of the task\n",
    "- Compression performance\n",
    "\n",
    "However stemming is also language dependant (for example it does not work with Chinese), also stemming could cause an error depending on the amount:\n",
    "- overstemming: can remove too much from a word changing its meaning.\n",
    "- understemming: if wrongly reduced it can lead to different root words.\n",
    "\n",
    "\n",
    "\n",
    "## Zipf's law\n",
    "Zipf's flaw in NLP describes the power-law distribution of word frequencies in a corpus, stating that the frequency of a word is inversely proportional to its rank.\n",
    "\n",
    "Example:\n",
    "“The cat sat on the mat, and the dog sat on the mat.”\n",
    "Tokenize:\n",
    "[“The”, “cat”, “sat”, “on”, “the”, “mat”, “,”, “and”, “the”, “dog”, “sat”, “on”,“the”, “mat”, ”.”] -> results in 13\n",
    "Type: a unique word\n",
    "Token: an instance of a type in a corpus (including repetitions)\n",
    "Word Frequencies:\n",
    "the: 3\n",
    "sat: 2\n",
    "on: 2\n",
    "mat: 2\n",
    "cat: 1\n",
    "dog: 1\n",
    "and: 1\n",
    "\n",
    "So the type/token ratio is 7/13, therefore more data -> lower type/token ratio\n",
    "\n",
    "Positive Implications of Zipf's law:\n",
    "- Any document/text will contain a number of words that are very common.\n",
    "- Help us understand the structure (and possibly meaning) of the text\n",
    "Negative Implications:\n",
    "- Any document/text will have a large number of rare words known as Out-of-vocabulary (OOV) words\n",
    "Handling OOV words:\n",
    "- Replace the set of OOV words in the training data with an unknown word token (-UNK)\n",
    "- Use of statistical models\n",
    "- Use sub-string based representations such as Byte-Pair Encoding\n",
    "Byte-Pair Encoding: learn which character sequences are common in the vocabulary of the language, and treat those common sequences\n",
    "as atomic units of the vocabulary\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "\n",
    "## Part of Speech (POS)\n",
    "\n",
    "\n",
    "## String Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf0bf7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1498ddb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078d38d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b289176",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b376f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3a8deb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f03f0793",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f6ec100",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45407d1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
