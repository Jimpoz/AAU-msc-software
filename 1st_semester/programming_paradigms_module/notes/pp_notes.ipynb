{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f14ac55",
   "metadata": {},
   "source": [
    "# LECTURE 10/09/2025\n",
    "\n",
    "\n",
    "### Algorithms\n",
    "An algorithm is a mathematical definition where computation is seen as a sequence of steps constructing the final result.\n",
    "\n",
    "### Fondumental formalisms:\n",
    "- General recursive functions\n",
    "- lambda calculus\n",
    "- rewriting systems\n",
    "- formal semantics\n",
    "\n",
    "### Declarative programming (imperative): functional, logic, constraint based etc...\n",
    "Declarative programming is not efficient due to the fact that it cannot use ephemeral data strucuter like arrays.\n",
    "\n",
    "Persistent data structures address this issue:\n",
    "- their interfaces only exposes operations that do not modify the structure in place but instead return new modified structures\n",
    "- their implementations achieves the complexity of the best known ephemeral structures\n",
    "\n",
    "In  imperative programming and classical algorithms these data structures can be relevant:\n",
    "- backtracking are trivial\n",
    "- complete history of updates of the structure\n",
    "- no in-place modification allows memory sharing between successive versions\n",
    "- concurrency\n",
    "\n",
    "\n",
    "A stack is a persistent data structure with the following operations:\n",
    "- init\n",
    "- push(v)\n",
    "- top\n",
    "- pop\n",
    "\n",
    "Persistent stack implemented as a linked list:\n",
    "\n",
    "```\n",
    "\n",
    "class Stack{\n",
    "    private int hd; \n",
    "    private Stack tl;\n",
    "    static Stack empty = null;\n",
    "    static Stack push(int v, Stack s){\n",
    "        Stack t = new Stack; t.hd = v; t.tl = s; return t;\n",
    "    }\n",
    "\n",
    "    static int top(Stack s){return s.hd};\n",
    "    static Stack pop(Stack s){return s.tl};\n",
    "}\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "- Immutability: operations do not overwrite state, they return a new value and it is not possible to mutate a shared node afterwards\n",
    "- Structural sharing: new versions reuse unchanged parts\n",
    "- Persistence: old versions remain accessible\n",
    "- Safe and efficient persistence: immutability and structural sharing OR fat node fersioning with bounded histories\n",
    "\n",
    "If immutability is missing then a write can mutate a node that newer versions still point to and the old snapshots will get corrupted.\n",
    "If there is no safe sharing then the updates will copy the whole structure (O(n)) resulting in poor time/space.\n",
    "\n",
    "## Dobkin and Lipton’s algorithm\n",
    "\n",
    "**Preprocessing** of $n$ line segments involves creating a grid to quickly locate any point.\n",
    "\n",
    "* First, we sort all the x-coordinates of the segment endpoints. These vertical lines create $O(n)$ \"slabs\" or vertical strips.\n",
    "* Within each slab, the segments don't cross, so their vertical order is fixed. We create a sorted list of all segments for each slab.\n",
    "\n",
    "This preparation takes $O(n^2)$ space because each of the $O(n)$ slabs might contain a list of all $O(n)$ segments.\n",
    "\n",
    "**Querying** for a point $P = (x, y)$:\n",
    "\n",
    "1.  We use binary search on the x-coordinates to find which slab contains $P$. This takes $O(\\log n)$ time.\n",
    "2.  Within that slab's sorted list, we use another binary search to find the two segments immediately above and below $P$. This also takes $O(\\log n)$ time.\n",
    "\n",
    "The total query time is a very fast **$O(\\log n)$**, but the initial space and time cost is high.\n",
    "\n",
    "\n",
    "\n",
    "### Reducing space and preprocessing time\n",
    "\n",
    "The key insight is that adjacent slabs are nearly identical. When moving from one slab to the next, only a few segments begin or end, while the rest maintain their relative vertical order. Storing a separate, full list for each slab is incredibly redundant.\n",
    "\n",
    "The solution is to use a **persistent data structure**. Instead of a simple array, we'll store each slab's segment list in a persistent balanced binary search tree. This allows adjacent slabs to share most of their structure, drastically reducing memory.\n",
    "\n",
    "---\n",
    "\n",
    "## Persistent insertion in a BST\n",
    "\n",
    "A persistent BST never modifies its nodes. Instead, an update returns a new tree with the change, leaving the original intact. This is achieved through **path copying**.\n",
    "\n",
    "1.  **Search**: To insert a new segment `g`, we first search the tree for its correct position, just like a normal BST.\n",
    "2.  **Copy Path**: As we traverse from the root to the insertion point, we create a copy of each node along the path. These new nodes point to the original, untouched subtrees that were not on the path. This is called **structural sharing**.\n",
    "3.  **Insert**: At the end of the new path, we add the new node for `g`.\n",
    "4.  **Rebalance**: If the tree becomes unbalanced, we perform rotations on the *newly copied path*, again without altering the original tree.\n",
    "\n",
    "This operation creates a new root, which represents the updated tree. The time and space cost for a single insertion is only **$O(\\log n)$**.\n",
    "\n",
    "\n",
    "\n",
    "## Improved point location algorithm\n",
    "\n",
    "This approach, known as the **sweep-line algorithm**, refines the slab method by using a persistent BST.\n",
    "\n",
    "**Preprocessing:**\n",
    "\n",
    "1.  We process the segment endpoints in order from left to right, as if a vertical line is \"sweeping\" across the plane.\n",
    "2.  When the sweep-line encounters the start of a segment, we **persistently insert** it into a balanced BST, which keeps the segments sorted by their vertical position. When it encounters the end of a segment, we persistently delete it.\n",
    "3.  We store a reference to the root of the BST at each endpoint. Each of these stored roots represents the state of the segments within a slab.\n",
    "\n",
    "This preprocessing takes **$O(n \\log n)$** time and space because each of the $2n$ endpoints requires one persistent tree operation, costing $O(\\log n)$ each.\n",
    "\n",
    "**Querying** for a point $P = (x, y)$:\n",
    "\n",
    "1.  We first use binary search on the sorted list of x-coordinates (our \"slabs\") to find the correct version of the BST.\n",
    "2.  Then, we search within that specific BST to find the segments above and below $P$.\n",
    "\n",
    "The total query time remains an optimal **$O(\\log n)$**.\n",
    "\n",
    "---\n",
    "\n",
    "## The Sarnak-Tarjan algorithm\n",
    "\n",
    "Sarnak and Tarjan further optimized the space from $O(n \\log n)$ down to a linear **$O(n)$** by using a different implementation of persistent BSTs known as the \"**fat nodes**\" technique.\n",
    "\n",
    "Instead of copying nodes on every update, this method modifies nodes in-place but keeps a history of the changes. Each field in a node (like `left`, `right`, or `color`) is replaced with a list of its historical values, each tagged with a version number or \"time.\"\n",
    "\n",
    "### Example of BST with fat nodes\n",
    "\n",
    "Imagine a node's `left` child pointer. In a normal BST, it holds one value. In a fat node, it holds a history:\n",
    "\n",
    "* `left`: `[(version 1, NodeA), (version 5, NodeB), (version 12, NodeC)]`\n",
    "\n",
    "To find the `left` child at version 10, you search this list to find the latest entry with a version less than or equal to 10 (which would be `NodeB`).\n",
    "\n",
    "### Memory usage\n",
    "\n",
    "With fat nodes, each field modification adds one entry to a history list, costing $O(1)$ space. After $n$ insertions and $n$ deletions in a balanced BST, which require an amortized $O(1)$ updates each, the total space used is only **$O(n)$**. This is a significant improvement over the $O(n \\log n)$ space required by the path-copying method.\n",
    "\n",
    "### Access time in the structure\n",
    "\n",
    "A naive implementation of fat nodes would be slow. Accessing a field's value at a specific time would require a search through its modification history, taking $O(\\log k)$ time, where $k$ is the number of modifications. This could slow down the total BST search time to $O(\\log^2 n)$.\n",
    "\n",
    "Sarnak and Tarjan's crucial optimization is a **hybrid approach**: they allow a node's history list to grow only to a certain size. When a history list becomes full, the entire fat node is copied, and its history is consolidated. This bounds the history search time, bringing the overall query time back to an optimal **$O(\\log n)$**.\n",
    "\n",
    "The result is the best of both worlds: a data structure for planar point location with **$O(n)$** space and **$O(\\log n)$** query time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455e454",
   "metadata": {},
   "source": [
    "# LECTURE 24/09/2025\n",
    "\n",
    "## Lambda calculus\n",
    "The lambda calculus is a model of computation, it only has one type of value (functions) and it focuses on variable binding and sustition as computation.\n",
    "\n",
    "Studying λ-calculus provides a clear foundation for understanding first-class functions, lexical scoping, and programming language semantics (operational, denotational, and type-theoretic). It has significantly influenced languages such as Lisp, ML, Haskell, and JavaScript, and most programming language calculi can be seen as extensions of λ-calculus.\n",
    "\n",
    "### Syntax of λ-calculus\n",
    "\n",
    "e ::= x | λx.e | ee\n",
    "\n",
    "- Variables: x,y,z,f,g,...\n",
    "- Abstraction: λx.e\n",
    "- Application: ${e_1}{e_2}$\n",
    "- Parentheses associate left: a b c = (a b) c\n",
    "\n",
    "### Reading λ-calculus in JS\n",
    "\n",
    "| λ-calculus                  | JS                         |\n",
    "|-----------------------------|----------------------------|\n",
    "| λx.(2+x)                    | x => 2 + x                 |\n",
    "| (λx.(2+x))5                 | (x => 2 + x)(5)            |\n",
    "| (λf.(f3))(λx.(x+1))         | (f => f(3))(x => x + 1)    |\n",
    "\n",
    "---\n",
    "\n",
    "### Function composition example  \n",
    "Build f o f and apply to x  \n",
    "\n",
    "| JS                                      | λ-calculus                            |\n",
    "|-----------------------------------------|---------------------------------------|\n",
    "| f => (x => f(f(x)))                     | λf.λx.f(x)                            |\n",
    "| ((f => (x => f(f(x))))(x => x+1))(4)    | ((λf.λx.f(fx))(λx.x+1))4              |\n",
    "\n",
    "---\n",
    "\n",
    "### Free and bound variables  \n",
    "\n",
    "| Free vars                               | Bound vars                            |\n",
    "|-----------------------------------------|---------------------------------------|\n",
    "| FV(x)={x}                               | BV(x) = 0                             |\n",
    "| FV(λx.e)=FV(e)\\{x}                      | BV(λx.e)=$BV(e)\\cup{x}$               |\n",
    "| FV(${e_1}{e_2}$)=${FV(e_1)}\\cup{FV(e_2)}$ | BV(${e_1}{e_2}$)=${BV(e_1)}\\cup{BV(e_2)}$ |\n",
    "\n",
    "\n",
    "### Substitution\n",
    "$e_1[x:=e_2]$ replace every free x in $e_1$ by $e_2$\n",
    "\n",
    "### Evaluation (β-reduction):\n",
    "$(λx . e_1) e_2 → e_1 [x := e_2]$\n",
    "\n",
    "### Capture-avoiding substitution\n",
    "\n",
    "$x [x := e] = e$\n",
    "\n",
    "$y [x := e] = y if y!= x$\n",
    "\n",
    "$({e_1}{e_2}) [x := e] = ({e_1} [x := e]) ({e_2} [x := e])$\n",
    "\n",
    "$(λy . e_1) [x := e] = λy . e_1 [x := e]$\n",
    "\n",
    "\n",
    "problems solved:\n",
    "\n",
    "1. \n",
    "$(λx.2 + x) 5$\n",
    "\n",
    "lambda reduction: $(2+x)[x:=5]$\n",
    "\n",
    "substitution (2+x): $(2 [x := 5]) + (x [x := 5]) [because x is substituted by [x:=5]]$\n",
    "\n",
    "$(2[x:=5])$ becomes 2\n",
    "\n",
    "$(x[x:=5])$ given the rule $x [x := e] = e$\n",
    "\n",
    "2. \n",
    "$(λf . f 3) (λx . x + 1)$\n",
    "\n",
    "(λparameter . body) argument\n",
    "\n",
    "The rule for β-reduction says that this simplifies to one action:\n",
    "\n",
    "body [parameter := argument]\n",
    "\n",
    "$f3 (f:=(λx . x + 1))$\n",
    "\n",
    "$((λx . x + 1) 3)$\n",
    "\n",
    "$(x+1)[x:=3]$\n",
    "\n",
    "$3+1=4$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c933db2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9c4b6b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "116740fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b022ee45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2539701",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9db2a1bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b364e1c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
