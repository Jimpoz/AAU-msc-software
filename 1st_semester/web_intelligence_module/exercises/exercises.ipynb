{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca55a7b0",
   "metadata": {},
   "source": [
    "# LECTURE 09/09/2025\n",
    "\n",
    "# QUESTIONS\n",
    "\n",
    "1. Perform all the preprocessing steps for data tokenisation, normalisation, and stopword removal in the following text. \n",
    "Further determine the type/token ratio, and rank the words based on their frequencies:\\\n",
    "\n",
    "\"Hi there! Did you hear about the new café that just opened downtown?\" asked Sarah.\\\n",
    "\"No, I haven't. What's so special about it?\" replied John, looking curious.\\\n",
    "\"It's supposed to have the best espresso in town—people are raving about it!\" Sarah responded enthusiastically.\\\n",
    "\"Really? I’m always on the lookout for good coffee. When should we check it out?\" John asked.\\\n",
    "\"How about this Saturday? I heard they have a live jazz band in the evening,\" Sarah suggested.\\\n",
    "\"That sounds fantastic! I’m in. Let’s meet there at 7 PM, then,\" John agreed.\\\n",
    "\"Perfect! I'll text you the address later. By the way, did you see the new movie trailer for 'The Last Horizon'? It looks amazing, don't you think?\" Sarah inquired.\\\n",
    "\"Oh, I did! It looks intense. I can’t wait to see it,\" John said with excitement.\\\n",
    "\n",
    "2. Stem the following words using Porter Stemmer\n",
    "   1. Excitement\n",
    "   2. Misunderstanding\n",
    "   3. Transcontinental\n",
    "   4. Substitutional\n",
    "3. Determine the Levenshtein distance for the following:\\\n",
    "“Anthropomorphization” and “Anthropomorphism\"\\\n",
    "“Overcompensation\" and \"Overcompensate”\\\n",
    "“Counterproductive\" and \"Counterproductivity”\n",
    "4. Determine the Hamming distances for the following:\n",
    "M4ch1n3L3arn1ng and M4ch1n3L3arN1ng\n",
    "GATTACA and GATCTAC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561acb1",
   "metadata": {},
   "source": [
    "# ANSWERS\n",
    "\n",
    "1.Tokenization\n",
    "First and foremost we need to remove the special character such as: \",\", \"!\", etc...\\\n",
    "Then we'll need to normalize the text to lowercase.\n",
    "It will result in this list:\n",
    "\n",
    "['hi', 'there', 'did', 'you', 'hear', 'about', 'the', 'new', 'cafe', 'that', 'just', 'opened', 'downtown', 'asked', 'sarah', 'no', 'i', 'havent', 'whats', 'so', 'special', 'about', 'it', 'replied', 'john', 'looking', 'curious', 'its', 'supposed', 'to', 'have', 'the', 'best', 'espresso', 'in', 'town', 'people', 'are', 'raving', 'about', 'it', 'sarah', 'responded', 'enthusiastically', 'really', 'i’m', 'always', 'on', 'the', 'lookout', 'for', 'good', 'coffee', 'when', 'should', 'we', 'check', 'it', 'out', 'john', 'asked', 'how', 'about', 'this', 'saturday', 'i', 'heard', 'they', 'have', 'a', 'live', 'jazz', 'band', 'in', 'the', 'evening', 'sarah', 'suggested', 'that', 'sounds', 'fantastic', 'i’m', 'in', 'let’s', 'meet', 'there', 'at', '7', 'pm', 'then', 'john', 'agreed', 'perfect', 'ill', 'text', 'you', 'the', 'address', 'later', 'by', 'the', 'way', 'did', 'you', 'see', 'the', 'new', 'movie', 'trailer', 'for', 'the', 'last', 'horizon', 'it', 'looks', 'amazing', 'dont', 'you', 'think', 'sarah', 'inquired', 'oh', 'i', 'did', 'it', 'looks', 'intense', 'i', 'can’t', 'wait', 'to', 'see', 'it', 'john', 'said', 'with', 'excitement']\n",
    "\n",
    "After this we'll need to remove common stopwords:\\\n",
    "[a, about, an, are, as, at, be, by, for, from, how, i, in, is, it, on, that, the, this, to, was, what, when, where, who, will, with, you]\n",
    "\n",
    "Now the list results in:\n",
    "['hi', 'hear', 'new', 'cafe', 'just', 'opened', 'downtown', 'asked', 'sarah', 'no', 'havent', 'whats', 'special', 'replied', 'john', 'looking', 'curious', 'its', 'supposed', 'best', 'espresso', 'town', 'people', 'raving', 'sarah', 'responded', 'enthusiastically', 'really', 'i’m', 'always', 'lookout', 'good', 'coffee', 'should', 'we', 'check', 'john', 'asked', 'saturday', 'heard', 'they', 'have', 'live', 'jazz', 'band', 'evening', 'sarah', 'suggested', 'sounds', 'fantastic', 'i’m', 'let’s', 'meet', '7', 'pm', 'then', 'john', 'agreed', 'perfect', 'ill', 'text', 'address', 'later', 'way', 'did', 'see', 'new', 'movie', 'trailer', 'last', 'horizon', 'looks', 'amazing', 'dont', 'think', 'sarah', 'inquired', 'oh', 'did', 'looks', 'intense', 'can’t', 'wait', 'see', 'john', 'said', 'excitement']\n",
    "\n",
    "Now we can calculate the type/token ratio:\\\n",
    "Types: 62\\\n",
    "Tokens: 74\\\n",
    "Type/Token ratio = 62/74 = 0.8378\n",
    "\n",
    "Now we can rank the words based on their frequencies:\n",
    "- sarah: 4\n",
    "- john: 4\n",
    "- asked: 2\n",
    "- did: 2\n",
    "- i’m: 2\n",
    "- looks: 2\n",
    "- new: 2\n",
    "- see: 2\n",
    "\n",
    "The other words are all 1 occurrence each\n",
    "\n",
    "2.Using Porter's Stemmer to calculate the m\n",
    "\n",
    "Excitement\\\n",
    "Removal of suffix -ment\\\n",
    "E X C I T E\\\n",
    "V C C V C V -> m = 2\n",
    "\n",
    "Misunderstanding\\\n",
    "Removal of the prefix Mis-\\\n",
    "Removal of the suffix -ing\\\n",
    "U N D E R S T A N D\\\n",
    "V C C V C C C V C C -> m = 3\n",
    "\n",
    "Transcontinental\\\n",
    "Removal of the prefix Trans-\\\n",
    "Removal of the suffix -al\\\n",
    "C O N T I N E N T\\\n",
    "C V C C V C V C -> m = 3\n",
    "\n",
    "Substitutional\\\n",
    "Removal of the suffix -al\\\n",
    "S U B S T I T U T I O N\\\n",
    "C V C C C V C V C V V C -> m = 4\n",
    "\n",
    "3.Calculate the Levenshtein distance\n",
    "\n",
    "“Anthropomorphization” and “Anthropomorphism\"\\\n",
    "Anthropomorphi is shared in these words so we'll need to substitute\\\n",
    "z -> s\\\n",
    "a -> m\\\n",
    "And delete \"tion\"\\\n",
    "Levenshtein distance = 6 (2 substitutions and 4 deletions)\n",
    "\n",
    "“Overcompensation\" and \"Overcompensate”\\\n",
    "Overcompensat is shared so we'll substitute\\\n",
    "i -> e\\\n",
    "And delete \"on\"\\\n",
    "Levenshtein distance = 3 (1 substitution and 2 deletions)\n",
    "\n",
    "“Counterproductive\" and \"Counterproductivity\"\\\n",
    "Counterproductiv is shared so we'll substitute\\\n",
    "e -> i\\\n",
    "And add \"ty\"\\\n",
    "Levenshtein distance = 3 (1 substitution and 2 additions)\n",
    "\n",
    "4.Determine Hamming distance\n",
    "\n",
    "M4ch1n3L3arn1ng\\\n",
    "M4ch1n3L3arN1ng -> distance = 1 (differs at the n before the 1 it's case sensitive)\n",
    "\n",
    "\n",
    "\n",
    "GATTACA\\\n",
    "GATCTAC -> distance = 4 (differs at the last 4 characters of the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf8243",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# LECTURE 16/09/2025\n",
    "\n",
    "## EXERCISE\n",
    "\n",
    "Your task is to build an information retrieval system for a small collection of documents. The system\n",
    "should:\n",
    "1. Calculate the TF-IDF for each document in the corpus.\n",
    "2. Use the Vector Space Model (VSM) to rank documents based on their cosine similarity to a\n",
    "given query.\n",
    "3. Incorporate query expansion using relevant terms from feedback.\\\n",
    "Corpus of Documents:\n",
    "   1. Document 1:\n",
    "    \"Machine learning models are powerful tools for data-driven decision-making.\"\n",
    "   2. Document 2:\n",
    "    \"Deep learning, a subset of machine learning, has revolutionized many industries.\"\n",
    "   3. Document 3:\n",
    "    \"Neural networks are essential for deep learning, which is a branch of artificial intelligence.\"\n",
    "   4. Document 4:\n",
    "    \"Machine learning can be applied to both supervised and unsupervised tasks.\"\n",
    "   5. Document 5:\n",
    "    \"Data science involves machine learning, statistics, and big data technologies.\"\n",
    "Task:\n",
    "You are given the following query:\n",
    "● Query: \"machine learning for decision-making\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c4eed",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "### 1. TF-IDF Calculation\n",
    "\n",
    "First, we process the documents and calculate the Term Frequency (TF) and Inverse Document Frequency (IDF) for each unique term.\n",
    "\n",
    "* **Term Frequency (TF)**: The number of times a term appears in a document. (simpler raw count)\n",
    "* **Inverse Document Frequency (IDF)**: A measure of how important a term is across the whole collection of documents.\n",
    "\n",
    "---\n",
    "\n",
    "#### A. Term Frequency (TF) Table\n",
    "\n",
    "This table shows the raw count of each term in each document.\n",
    "\n",
    "| Term              | D1  | D2  | D3  | D4  | D5  |\n",
    "| :---------------- | :-: | :-: | :-: | :-: | :-: |\n",
    "| machine           | 1   | 1   | 0   | 1   | 1   |\n",
    "| learning          | 1   | 2   | 1   | 1   | 1   |\n",
    "| decision-making   | 1   | 0   | 0   | 0   | 0   |\n",
    "| models            | 1   | 0   | 0   | 0   | 0   |\n",
    "| deep              | 0   | 1   | 1   | 0   | 0   |\n",
    "| data              | 0   | 0   | 0   | 0   | 2   |\n",
    "| ... *(and so on)* |     |     |     |     |     |\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Inverse Document Frequency (IDF) Calculation Examples\n",
    "\n",
    "The formula is $IDF(t) = \\ln(\\frac{N}{df_t})$, where $N=5$ (total documents) and $df_t$ is the number of documents containing the term $t$.\n",
    "\n",
    "* **Example for \"decision-making\" (a rare term):**\n",
    "    * Appears in only 1 document, so $df_t = 1$.\n",
    "    * $IDF = \\ln(\\frac{5}{1}) = \\ln(5) \\approx 1.609$. This high score reflects its importance.\n",
    "\n",
    "* **Example for \"machine\" (a common term):**\n",
    "    * Appears in 4 documents, so $df_t = 4$.\n",
    "    * $IDF = \\ln(\\frac{5}{4}) = \\ln(1.25) \\approx 0.223$. This lower score indicates it's less unique.\n",
    "\n",
    "* **Example for \"learning\" (a ubiquitous term):**\n",
    "    * Appears in all 5 documents, so $df_t = 5$.\n",
    "    * $IDF = \\ln(\\frac{5}{5}) = \\ln(1) = 0$. A score of 0 means the term has no power to distinguish between documents in this specific collection.\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Final TF-IDF Vectors\n",
    "\n",
    "The final TF-IDF score is $TF \\times IDF$. These scores form the vector for each document.\n",
    "\n",
    "| Term              | D1    | D2    | D3    | D4    | D5    |\n",
    "| :---------------- | :---: | :---: | :---: | :---: | :---: |\n",
    "| **machine** | 0.223 | 0.223 | 0     | 0.223 | 0.223 |\n",
    "| **learning** | 0     | 0     | 0     | 0     | 0     |\n",
    "| **decision-making** | 1.609 | 0     | 0     | 0     | 0     |\n",
    "| **models** | 1.609 | 0     | 0     | 0     | 0     |\n",
    "| **data-driven** | 1.609 | 0     | 0     | 0     | 0     |\n",
    "| **deep** | 0     | 0.916 | 0.916 | 0     | 0     |\n",
    "| **data** | 0     | 0     | 0     | 0     | 3.218 |\n",
    "| ...               | ...   | ...   | ...   | ...   | ...   |\n",
    "\n",
    "***\n",
    "\n",
    "### 2. Initial Ranking with Vector Space Model\n",
    "\n",
    "We use the Vector Space Model (VSM) to rank documents by calculating the **cosine similarity** between the query vector ($Q$) and each document vector ($D$).\n",
    "\n",
    "\n",
    "\n",
    "The formula is:\n",
    "$$\\text{similarity}(Q, D) = \\frac{Q \\cdot D}{\\|Q\\| \\cdot \\|D\\|}$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Calculation Example: `sim(Q, D1)`\n",
    "\n",
    "Let's break down the calculation for the query and Document 1.\n",
    "\n",
    "1.  **Dot Product ($Q \\cdot D1$):** We multiply the matching term scores and add them up.\n",
    "    * The only matching terms with non-zero scores are 'machine' and 'decision-making'.\n",
    "    * $(0.223 \\times 0.223) + (1.609 \\times 1.609) = 0.0497 + 2.5889 = 2.6386$\n",
    "\n",
    "2.  **Vector Magnitudes ($\\|Q\\|$ and $\\|D1\\|$):** We find the square root of the sum of the squares of all term scores for each vector.\n",
    "    * $\\|Q\\| = \\sqrt{0.223^2 + 1.609^2} = \\sqrt{2.6386} \\approx 1.624$\n",
    "    * $\\|D1\\| = \\sqrt{0.223^2 + 1.609^2 + 1.609^2 + ...} = \\sqrt{12.9942} \\approx 3.605$\n",
    "\n",
    "3.  **Final Score:** Divide the dot product by the product of the magnitudes.\n",
    "    * $$\\frac{2.6386}{1.624 \\times 3.605} = \\frac{2.6386}{5.8545} \\approx \\bf 0.451$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Initial Ranking\n",
    "\n",
    "After performing this calculation for all documents, we get the following scores:\n",
    "\n",
    "1.  **Document 1** (Score: 0.451)\n",
    "2.  **Document 2** (Score: 0.010)\n",
    "3.  **Document 4** (Score: 0.009)\n",
    "4.  **Document 5** (Score: 0.007)\n",
    "5.  **Document 3** (Score: 0.000)\n",
    "\n",
    "As expected, **Document 1** is ranked highest.\n",
    "\n",
    "***\n",
    "\n",
    "### 3. Query Expansion and Final Ranking\n",
    "\n",
    "To improve results, we use relevance feedback from the top document (D1) to expand the query.\n",
    "\n",
    "* **Original Query:** \"machine learning decision-making\"\n",
    "* **Expanded Query (Q_exp):** \"machine learning decision-making **models data-driven**\"\n",
    "\n",
    "We then recalculate the cosine similarity with this new, more descriptive query vector.\n",
    "\n",
    "---\n",
    "\n",
    "#### Calculation Example: `sim(Q_exp, D1)`\n",
    "\n",
    "Let's break down the new calculation with the expanded query ($Q_{exp}$).\n",
    "\n",
    "1.  **Dot Product ($Q_{exp} \\cdot D1$):** We now have more matching terms.\n",
    "    * $(0.223 \\times 0.223) + (1.609 \\times 1.609) + (1.609 \\times 1.609) + (1.609 \\times 1.609) = 7.8164$\n",
    "\n",
    "2.  **Vector Magnitudes ($\\|Q_{exp}\\|$ and $\\|D1\\|$):** The magnitude of D1 is unchanged, but the query vector's magnitude increases.\n",
    "    * $\\|Q_{exp}\\| = \\sqrt{0.223^2 + 1.609^2 + 1.609^2 + 1.609^2} = \\sqrt{7.8164} \\approx 2.796$\n",
    "    * $\\|D1\\| \\approx 3.605$ (same as before)\n",
    "\n",
    "3.  **Final Score:**\n",
    "    * $$\\frac{7.8164}{2.796 \\times 3.605} = \\frac{7.8164}{10.0796} \\approx \\bf 0.775$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Ranking\n",
    "\n",
    "1.  **Document 1** (Score: 0.775)\n",
    "2.  **Document 2** (Score: 0.006)\n",
    "3.  **Document 4** (Score: 0.005)\n",
    "4.  **Document 5** (Score: 0.004)\n",
    "5.  **Document 3** (Score: 0.000)\n",
    "\n",
    "The query expansion successfully **amplified the relevance score of Document 1** from 0.451 to 0.775, confirming its top position with much greater certainty.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d7d53",
   "metadata": {},
   "source": [
    "# LECTURE 23/09/2025\n",
    "\n",
    "## EXERCISES\n",
    "\n",
    "Consider these documents:\n",
    "- Doc 1 breakthrough drug for schizophrenia\n",
    "- Doc 2 new schizophrenia drug\n",
    "- Doc 3 new approach for treatment of schizophrenia\n",
    "- Doc 4 new hopes for schizophrenia patients\n",
    "1. Compute the term-document incidence matrix for this document collection.\n",
    "2. Compute the inverted index representation for this collection\n",
    "3. Compute Merge Algorithm for the query “new schizophrenia”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1c0e1",
   "metadata": {},
   "source": [
    "# Information Retrieval Example\n",
    "\n",
    "\n",
    "### 1. Term–Document Incidence Matrix\n",
    "\n",
    "Vocabulary: `approach, breakthrough, drug, for, hopes, new, patients, schizophrenia, treatment`\n",
    "\n",
    "| Term / Doc | Doc1 | Doc2 | Doc3 | Doc4 |\n",
    "|-------------|:----:|:----:|:----:|:----:|\n",
    "| approach | 0 | 0 | 1 | 0 |\n",
    "| breakthrough | 1 | 0 | 0 | 0 |\n",
    "| drug | 1 | 1 | 0 | 0 |\n",
    "| for | 1 | 0 | 1 | 0 |\n",
    "| hopes | 0 | 0 | 0 | 1 |\n",
    "| new | 0 | 1 | 1 | 1 |\n",
    "| patients | 0 | 0 | 0 | 1 |\n",
    "| schizophrenia | 1 | 1 | 1 | 1 |\n",
    "| treatment | 0 | 0 | 1 | 0 |\n",
    "\n",
    "\n",
    "### 2. Inverted Index Representation\n",
    "\n",
    "| Term | Postings List (DocIDs) | df |\n",
    "|------|------------------------|----|\n",
    "| approach | [3] | 1 |\n",
    "| breakthrough | [1] | 1 |\n",
    "| drug | [1, 2] | 2 |\n",
    "| for | [1, 3] | 2 |\n",
    "| hopes | [4] | 1 |\n",
    "| new | [2, 3, 4] | 3 |\n",
    "| patients | [4] | 1 |\n",
    "| schizophrenia | [1, 2, 3, 4] | 4 |\n",
    "| treatment | [3] | 1 |\n",
    "\n",
    "\n",
    "### 3. Merge Algorithm for Query: “new schizophrenia”\n",
    "\n",
    "### Posting Lists\n",
    "- **new** → [2, 3, 4]  \n",
    "- **schizophrenia** → [1, 2, 3, 4]\n",
    "\n",
    "### Merge (AND query)\n",
    "Step-by-step comparison:\n",
    "\n",
    "| Step | Pointer(new) | Pointer(schizophrenia) | Action | Output |\n",
    "|------|---------------|------------------------|---------|---------|\n",
    "| 1 | 2 | 1 | 1 < 2 → advance schizophrenia | — |\n",
    "| 2 | 2 | 2 | match → add 2 | [2] |\n",
    "| 3 | 3 | 3 | match → add 3 | [2, 3] |\n",
    "| 4 | 4 | 4 | match → add 4 | [2, 3, 4] |\n",
    "\n",
    "**Result:** Documents `[2, 3, 4]` contain both terms.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Answer\n",
    "\n",
    "- **Term–Document Matrix:** as above  \n",
    "- **Inverted Index:** as above  \n",
    "- **Query Result (`new AND schizophrenia`):** Doc2, Doc3, Doc4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503803b0",
   "metadata": {},
   "source": [
    "# LECTURE 14/10/2025\n",
    "\n",
    "Corpus:\n",
    "- I saw the boy\n",
    "- the man is working\n",
    "- I walked in the street\n",
    "\n",
    "Compute the following:\n",
    "- Unigram Counts\n",
    "- Maximum Likelihood for Bigrams\n",
    "- Compute the probability of the sentence: I saw the man\n",
    "- Compute the probability of the sentence: I saw the man in the street\n",
    "- Another corpus:\n",
    "    - I love machine learning\n",
    "    - I love deep learning\n",
    "    - Deep learning is great\n",
    "    - Machine learning is fun\n",
    "Compute MLE for Trigrams for this corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57371f30",
   "metadata": {},
   "source": [
    "## SOLUTIONS\n",
    "\n",
    "\n",
    "---\n",
    "#### **Unigram Counts**\n",
    "First, we find the vocabulary and count the occurrences of each word.\n",
    "\n",
    "| Word | Count |\n",
    "| :--- | :---: |\n",
    "| I | 2 |\n",
    "| saw | 1 |\n",
    "| the | 3 |\n",
    "| boy | 1 |\n",
    "| man | 1 |\n",
    "| is | 1 |\n",
    "| working | 1 |\n",
    "| walked | 1 |\n",
    "| in | 1 |\n",
    "| street | 1 |\n",
    "| **Total Words**| **14** |\n",
    "\n",
    "---\n",
    "#### **Maximum Likelihood for Bigrams**\n",
    "To calculate bigram probabilities, we add start `<s>` and end `</s>` tokens to our sentences. The Maximum Likelihood Estimate (MLE) is calculated as:\n",
    "$$P(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}$$\n",
    "\n",
    "Here are the computed MLEs for all non-zero bigrams in the corpus:\n",
    "\n",
    "| Bigram Probability | Calculation | Result |\n",
    "| :--- | :---: | :---: |\n",
    "| `P(I | <s>)` | 2 / 3 | 0.67 |\n",
    "| `P(the | <s>)` | 1 / 3 | 0.33 |\n",
    "| `P(saw | I)` | 1 / 2 | 0.5 |\n",
    "| `P(walked | I)` | 1 / 2 | 0.5 |\n",
    "| `P(the | saw)` | 1 / 1 | 1.0 |\n",
    "| `P(boy | the)` | 1 / 3 | 0.33 |\n",
    "| `P(man | the)` | 1 / 3 | 0.33 |\n",
    "| `P(street | the)` | 1 / 3 | 0.33 |\n",
    "| `P(is | man)` | 1 / 1 | 1.0 |\n",
    "| `P(working | is)` | 1 / 1 | 1.0 |\n",
    "| `P(in | walked)` | 1 / 1 | 1.0 |\n",
    "| `P(the | in)` | 1 / 1 | 1.0 |\n",
    "| `P(</s> | boy)` | 1 / 1 | 1.0 |\n",
    "| `P(</s> | working)` | 1 / 1 | 1.0 |\n",
    "| `P(</s> | street)` | 1 / 1 | 1.0 |\n",
    "\n",
    "---\n",
    "#### **Compute the probability of the sentence: `I saw the man`**\n",
    "We calculate the probability of the sequence `<s> I saw the man </s>` using the bigram model:\n",
    "\n",
    "$P(\\text{<s> I saw the man </s>}) = P(\\text{I|<s>}) \\times P(\\text{saw|I}) \\times P(\\text{the|saw}) \\times P(\\text{man|the}) \\times P(\\text{</s>|man})$\n",
    "\n",
    "The probability of the bigram `(man, </s>)` is:\n",
    "$P(\\text{</s>|man}) = \\frac{\\text{Count(man, </s>)}}{\\text{Count(man)}} = 0/1 = 0$\n",
    "\n",
    "Since one of the terms is 0, the entire probability is 0.\n",
    "\n",
    "**Final Probability**: $(2/3) \\times (1/2) \\times 1 \\times (1/3) \\times 0 = \\textbf{0}$\n",
    "\n",
    "---\n",
    "#### **Compute the probability of the sentence: `I saw the man in the street`**\n",
    "We calculate the probability of the sequence `<s> I saw the man in the street </s>`. We only need to find the first zero-probability bigram in the chain:\n",
    "\n",
    "$P(\\text{in|man}) = \\frac{\\text{Count(man, in)}}{\\text{Count(man)}} = 0/1 = 0$\n",
    "\n",
    "Because the bigram `(man, in)` was never seen, its probability is 0.\n",
    "\n",
    "**Final Probability**: $\\textbf{0}$\n",
    "\n",
    "*These examples highlight the **sparsity problem** in MLE, where unseen events are given a probability of zero. This is why smoothing techniques are essential.*\n",
    "\n",
    "---\n",
    "### **Corpus 2 Exercises**\n",
    "\n",
    "**Corpus**:\n",
    "* I love machine learning\n",
    "* I love deep learning\n",
    "* Deep learning is great\n",
    "* Machine learning is fun\n",
    "\n",
    "---\n",
    "#### **Compute MLE for Trigrams**\n",
    "For trigrams, we need two start tokens `<s>` and calculate probabilities using the formula:\n",
    "$$P(w_i | w_{i-2}, w_{i-1}) = \\frac{\\text{Count}(w_{i-2}, w_{i-1}, w_i)}{\\text{Count}(w_{i-2}, w_{i-1})}$$\n",
    "*(Note: \"Deep\" and \"deep\" are treated as different words due to case sensitivity.)*\n",
    "\n",
    "| Trigram Probability | Calculation | Result |\n",
    "| :--- | :---: | :---: |\n",
    "| `P(I | <s>, <s>)` | 2 / 4 | 0.5 |\n",
    "| `P(Deep | <s>, <s>)` | 1 / 4 | 0.25 |\n",
    "| `P(Machine | <s>, <s>)`| 1 / 4 | 0.25 |\n",
    "| `P(love | <s>, I)` | 2 / 2 | 1.0 |\n",
    "| `P(learning | <s>, Deep)` | 1 / 1 | 1.0 |\n",
    "| `P(learning | <s>, Machine)`| 1 / 1 | 1.0 |\n",
    "| `P(machine | I, love)` | 1 / 2 | 0.5 |\n",
    "| `P(deep | I, love)` | 1 / 2 | 0.5 |\n",
    "| `P(learning | love, machine)`| 1 / 1 | 1.0 |\n",
    "| `P(learning | love, deep)` | 1 / 1 | 1.0 |\n",
    "| `P(is | Deep, learning)` | 1 / 1 | 1.0 |\n",
    "| `P(is | Machine, learning)`| 1 / 1 | 1.0 |\n",
    "| `P(great | learning, is)` | 1 / 2 | 0.5 |\n",
    "| `P(fun | learning, is)` | 1 / 2 | 0.5 |\n",
    "| `P(</s> | machine, learning)`| 1 / 1 | 1.0 |\n",
    "| `P(</s> | deep, learning)` | 1 / 1 | 1.0 |\n",
    "| `P(</s> | is, great)` | 1 / 1 | 1.0 |\n",
    "| `P(</s> | is, fun)` | 1 / 1 | 1.0 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e68b5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
