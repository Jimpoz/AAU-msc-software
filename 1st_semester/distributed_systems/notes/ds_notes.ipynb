{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2983dd2",
   "metadata": {},
   "source": [
    "## **COURSE OVERVIEW**\n",
    "\n",
    "## **INTRO**\n",
    "### 1. Introduction\n",
    "## **Theory & Algorithms**\n",
    "### 1. Models of DS\n",
    "### 2. Time in DS\n",
    "### 3. Multicast\n",
    "### 4. Consensus\n",
    "### 5. Distributed Mutual Exclusion\n",
    "## **Application**\n",
    "### 7. Distributed Storage\n",
    "### 8. Distributed Computing\n",
    "### 9. Blockchains\n",
    "### 10. Peer-to-Peer Networking\n",
    "### 11. Internet of Things and Routing\n",
    "### 12. Distributed Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27860a15",
   "metadata": {},
   "source": [
    "# Lecture 08/09/2025\n",
    "\n",
    "## What is a distributed systems?\n",
    "A distributed system is one where hardware and software components in/on networked computers communicate and coordinate their activity only by passing messages.\n",
    "\n",
    "## CONCURRENCY\n",
    "\n",
    "Concurrency is the ability of a system to execute multiple tasks or processes simultaneously or at overlapping times, improving efficiency.\n",
    "However it can cause some problems such as:\n",
    "- Deadlocks and livelocks: these are conditions in which the processes do not make progress due to their circumstances.\n",
    "- Non-determinism: occurs when the output or behavior of a concurrent program differs for the same input, depending on the precise, unpredictable timing of events, such as thread interleaving or resource access.\n",
    "\n",
    "Other issues could rise from the absence of shared state that will eventually lead to:\n",
    "- Pass messages to synchronize: for example if 2 people have a shared resource and both are trying to access it at the same time an error could occur for one member of the party.\n",
    "- May not agree on time: \n",
    "\n",
    "Everything can fail in distributed systems:\n",
    "- Devices\n",
    "- Integrity of data\n",
    "- Network\n",
    "    - Security\n",
    "    - Man-in-the-Middle (MITM): attack occurs when an attacker intercepts and potentially alters communication between two legitimate parties, unbeknownst to them.\n",
    "    - Zibantine failure: is a condition of a system, particularly a distributed computing system, where a fault occurs such that different symptoms are presented to different observers, including imperfect information on whether a system component has failed.\n",
    "\n",
    "Distributed systems are used for domain, redundancy, and performance.\n",
    "\n",
    "---\n",
    "## Domain\n",
    "A domain is a specific area of knowledge or activity that a distributed system is designed to address.\n",
    "Some examples of domains are:\n",
    "- The internet\n",
    "- Wireless Mesh Networks\n",
    "- Industrail systems\n",
    "- Ledgers (bitcoin, ethereum)\n",
    "However these domains can encounter some limits that can be physical and logical.\n",
    "- Physical limits: are constraints imposed by the physical properties of the system's components or environment, such as hardware limitations, network bandwidth, latency, and geographical distribution.   \n",
    "- Logical limits: defined by its bounded context. This is a core concept from Domain-Driven Design (DDD). The logical limit of a domain in a distributed system is defined by its bounded context. This is a core concept from Domain-Driven Design (DDD), a software development approach that focuses on aligning software design with the business domain. A bounded context is an explicit boundary within a distributed system where a specific domain model and its language (ubiquitous language) are consistent and applicable.\n",
    "---\n",
    "## Redundancy\n",
    "A system with redundacy means that it has duplicate ocmponents, processes or data.\n",
    "Given these specifications the system will result:\n",
    "- Robust: more resilient to failure\n",
    "- Available: as in system availability which is measured in uptime.\n",
    "\n",
    "A system with redundancy can:\n",
    "- offer 99.9% uptime or \"five nines\": means it's designed to be operational and available 99.9% of the time, with a small amount of planned or unplanned downtime.\n",
    "- be a backup: in an active-passive configuration, the redundant server acts as a hot or cold backup. The primary server handles all the workload, while the backup server waits on standby. Given the duplication of information and components if one fails the other automatically takes over.\n",
    "- be a database: In an active-active configuration, all redundant servers are considered active and work together simultaneously. They are not simply waiting as a backup. Incoming requests are distributed among all the servers using a load balancer.\n",
    "- used in the banking sector: any downtime can lead to significant financial losses.\n",
    "\n",
    "---\n",
    "## Performance\n",
    "To ensure a performant system we need:\n",
    "- Economics\n",
    "- Scalability\n",
    "Here we talk about different topics such as:\n",
    "- Video streaming: requires a lot of procesing power\n",
    "- Cloud computing: Offers on-demand, scalable resources, eliminating the need for upfront hardware investment.\n",
    "- Supercomputers: Excel at massive, specialized calculations but are very expensive and not scalable for general-purpose use.\n",
    "- Many inexpensive vs many expensive specialized: Distributing workloads across many inexpensive machines is often more economical and scalable than using a few expensive, specialized ones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b169279",
   "metadata": {},
   "source": [
    "# Lecture 09/09/2025\n",
    "\n",
    "# Models of distributed systems\n",
    "\n",
    "## Aspects of models\n",
    "\n",
    "Why do we build distributed systems?\n",
    "\n",
    "- **Inherent distribution**: By definition, distributed systems span multiple computers, often connected through networks such as telecommunications systems.\n",
    "- **Reliability**: Even if one node fails, the system as a whole can continue functioning, avoiding single points of failure.\n",
    "- **Performance**: Workloads can be shared among multiple machines, and data can be accessed from geographically closer nodes to reduce latency.\n",
    "- **Scalability for large problems**: Some datasets and computations are simply too large to fit into a single machine, requiring distributed processing.\n",
    "\n",
    "### Modelling the process – API Style\n",
    "\n",
    "A distributed system can be described in terms of modules that exchange **events** through well-defined interfaces:\n",
    "\n",
    "- **Event representation**:  \n",
    "  \\{Event\\_type | Attributes, …\\}  \n",
    "  or  \n",
    "  \\{Process\\_ID | Event\\_type | Attributes, …\\}\n",
    "\n",
    "- **Module behavior**:  \n",
    "  Each module reacts to incoming events and produces outputs according to specified rules:\n",
    "upon event {condition | Event | attributes} such that condition holds do\n",
    "perform some action\n",
    "\n",
    "\n",
    "Multiple modules together (one per process or subsystem) should collectively satisfy desired **global properties** (e.g., safety, liveness).\n",
    "\n",
    "### What we want/will make\n",
    "\n",
    "We aim to:\n",
    "- Design APIs for modules and prove that their composition satisfies global system properties.\n",
    "- Implement modules that guarantee **local properties**.\n",
    "- Use pseudocode and mathematics to formally demonstrate when such guarantees are possible—or prove impossibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Failures\n",
    "\n",
    "Failures are inevitable in distributed systems. They can arise due to hardware breakdowns, software bugs, network disruptions, or even human mistakes. Designing robust systems requires understanding different types of failures and strategies to mitigate them.\n",
    "\n",
    "### Types of failures\n",
    "\n",
    "1. **Crash-stop**: A process halts and all other processes can reliably detect the failure. *Easiest to handle.*\n",
    "2. **Crash-silent**: A process halts but failures cannot be detected reliably.\n",
    "3. **Crash-noisy**: Failures may be detected, but only with eventual accuracy (false positives or delays are possible).\n",
    "4. **Crash-recovery**: Processes may fail and later recover, rejoining the system. Requires care to avoid state inconsistencies.\n",
    "5. **Crash-arbitrary (Byzantine failures)**: Processes behave arbitrarily or maliciously, deviating from the protocol. *Hardest to handle.*\n",
    "6. **Randomized behavior**: Processes make decisions probabilistically. Correctness is argued via probability theory rather than strict guarantees.\n",
    "\n",
    "---\n",
    "\n",
    "## Communication\n",
    "\n",
    "Is communication always required? In distributed systems, yes—but it can be realized in different ways:\n",
    "\n",
    "- **Message passing**:\n",
    "1. Types of links and their potential failures.\n",
    "2. Network topology (commonly assumed fully connected).\n",
    "3. Routing algorithms for multi-hop communication.\n",
    "4. Broadcast and multicast primitives.\n",
    "\n",
    "- **Shared memory**:\n",
    "1. Which process can read or write to which location?\n",
    "2. How do we guarantee reading the *freshest* value? (Consistency models)\n",
    "\n",
    "### On types of links\n",
    "\n",
    "A **link** is a module implementing send/receive operations with certain properties.\n",
    "\n",
    "- **TCP/IP**: Enables reliable communication between a pair of nodes (or none).\n",
    "- **SSH**: Adds protection against corruption, interception, and tampering.\n",
    "\n",
    "**Network reliability models**:\n",
    "1. **Perfect links**: Reliable delivery, no duplication, no spurious messages.\n",
    "2. **Fair-loss links**: Messages may be lost occasionally, but infinitely many attempts guarantee eventual delivery; finite duplication possible.\n",
    "3. **Stubborn links**: Messages are retransmitted until delivery is guaranteed but still no creation (this model is built upon the fair-loss).\n",
    "4. **Logged-perfect links**: Perfect delivery with persistent logs for auditing/recovery.\n",
    "5. **Authenticated links**: Reliable delivery, no duplication, and sender authenticity.\n",
    "\n",
    "### Can networks fail?\n",
    "\n",
    "While TCP/IP and lower-level protocols often give us the illusion of **perfect links** and **fail-stop crashes**, failures still happen.\n",
    "\n",
    "- **Network partitions**: Occur when many links fail simultaneously, dividing the system into disconnected components. This is rare but catastrophic.\n",
    "\n",
    "### Crashes vs Failures\n",
    "\n",
    "Having discussed both **network** and **process** failures, it is important to distinguish between the two levels:\n",
    "\n",
    "- A **process can crash** (e.g., by crashing, halting, or misbehaving).  \n",
    "- A **system fails** when the combination of process crashes and communication assumptions no longer allows correct operation.\n",
    "\n",
    "For the remainder of our discussion, we usually assume **perfect links** (thanks to TCP/IP and lower-level reliability mechanisms). This means that:\n",
    "- Messages are delivered reliably,\n",
    "- No duplicates are created,\n",
    "- No spurious (phantom) messages appear.\n",
    "\n",
    "Under this assumption, we can define **system failure models** in terms of process behavior:\n",
    "\n",
    "- **Fail-stop system**: Processes may experience crash-stop failures, but links are perfect.  \n",
    "- More complex models (e.g., crash-recovery, Byzantine failures) are defined similarly, always considering both the **process failure type** and the **communication assumptions**.\n",
    "\n",
    "In short, a system failure model = (process failure model) + (assumed link properties).\n",
    "\n",
    "---\n",
    "\n",
    "## Timing\n",
    "\n",
    "Timing plays a central role in distributed systems, especially when considering **synchronization** and **failure detection**.\n",
    "\n",
    "- Systems may be **synchronous** (bounded delays) or **asynchronous** (no timing guarantees).\n",
    "- Links are still modeled as modules with send/receive properties.\n",
    "\n",
    "### Synchronous vs. Asynchronous Systems\n",
    "\n",
    "Distributed systems can be broadly classified according to their **timing assumptions**:\n",
    "\n",
    "1. **Asynchronous systems**:\n",
    "   - No bounds on message transmission delays.\n",
    "   - No assumptions about process execution speeds (relative speeds may differ arbitrarily).\n",
    "   - Failure detection is unreliable, since a slow process cannot be distinguished from a failed one.\n",
    "   - Coordination and ordering rely on **logical clocks** (e.g., Lamport clocks, vector clocks), rather than real time.\n",
    "\n",
    "2. **Synchronous systems**:\n",
    "   - Bounds exist on message transmission delays and process execution speeds.\n",
    "   - **Timed failure detection** is possible: if a message or heartbeat is not received within a known bound, a failure can be suspected reliably.\n",
    "   - Transit delays can be measured and incorporated into algorithms.\n",
    "   - Coordination can be based on **real-time clocks** rather than purely logical clocks.\n",
    "   - Performance is often analyzed in terms of **worst-case bounds**, since timing assumptions provide guarantees.\n",
    "   - Processes may maintain **synchronized clocks** (to some degree of precision), enabling algorithms such as consensus and coordinated actions.\n",
    "\n",
    "**Key question**: *Can processes in an asynchronous system with fair-loss links reach agreement (e.g., on coordinated attack time)?*\n",
    "\n",
    "### Proof via contradiction (Two Generals Problem)\n",
    "\n",
    "1. Assume a protocol exists where a fixed sequence of messages guarantees agreement.\n",
    "2. Consider the last message in this sequence that is successfully delivered.\n",
    "3. If this message is lost, the receiving general decides **not** to attack.\n",
    "4. But the sender cannot distinguish whether the message was delivered or lost, so must behave deterministically and decide the same action in both cases.\n",
    "5. This creates a contradiction: one general attacks, the other does not.  \n",
    " $\\Rightarrow$ Perfect agreement is impossible under these assumptions.\n",
    "\n",
    "### Which crash/link/timing assumptions implement distributed systems?\n",
    "\n",
    "A **failure detector** can be modeled as just another module that provides (possibly imperfect) information about which processes are alive. Different combinations of timing assumptions and failure detectors allow different guarantees in distributed systems.  \n",
    "\n",
    "### Example\n",
    "\n",
    "![image](../images/Screenshot%202025-09-09%20at%2009.56.39.png)\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "This algorithm describes a **Perfect Failure Detector** for distributed systems using a heartbeat mechanism.\n",
    "\n",
    "In short, here's what it does:\n",
    "\n",
    "1.  **Sends Heartbeats:** Periodically, on a **timeout**, every process sends a `HEARTBEATREQUEST` message to all other processes in the system.\n",
    "2.  **Waits for Replies:** It assumes no one is alive and waits for `HEARTBEATREPLY` messages. When a process receives a reply, it marks the sender as `alive`.\n",
    "3.  **Detects Failures:** At the next timeout, any process that has not sent a reply is considered to have **crashed**. The algorithm then triggers a `Crash` event for that process.\n",
    "\n",
    "Because it assumes **perfect communication links** (messages are never lost), this method guarantees that a non-responsive process has truly failed, making the failure detection \"perfect.\"\n",
    "\n",
    "### Network latency and bandwith\n",
    "\n",
    "When discussing communication performance, two key metrics matter:\n",
    "\n",
    "- **Latency**: The time it takes for a single message (or bit) to travel from sender to receiver.  \n",
    "- **Bandwidth**: The rate at which data can be transmitted, usually measured in bits per second (bps) or bytes per second (B/s).\n",
    "\n",
    "#### Physical Link\n",
    "Sometimes, surprisingly “low-tech” physical methods can provide high bandwidth, even if latency is poor:\n",
    "- **Hard drives in a van**  \n",
    "- Messengers carrying storage devices  \n",
    "- Smoke signals (extreme latency, minimal bandwidth)  \n",
    "- Radio signals or laser communication\n",
    "\n",
    "#### Network Links\n",
    "More conventional digital communication technologies include:\n",
    "- DSL (Digital Subscriber Line)  \n",
    "- Cellular data (e.g., 3G, 4G, 5G)  \n",
    "- Wi-Fi (various standards)  \n",
    "- Ethernet/fiber cables  \n",
    "- Satellite links  \n",
    "\n",
    "#### Latency examples\n",
    "1. Hard drives transported by van: $\\approx$ 1 day latency  \n",
    "2. Intra-continent fiber-optic cable: $\\approx$ 100 ms latency  \n",
    "\n",
    "#### Bandwidth examples\n",
    "1. Hard drives in a van: $\\frac{50 \\, \\text{TB}}{1 \\, \\text{day}}$ = **very high bandwidth** despite huge latency  \n",
    "2. 3G cellular network: $\\approx 1 \\, \\text{Mbit/s}$ bandwidth  \n",
    "\n",
    "---\n",
    "\n",
    "## Performance\n",
    "\n",
    "### Performance measures\n",
    "\n",
    "- **SLI (Service Level Indicator)**: What aspect of the system do we measure?  \n",
    "Examples: bandwidth, latency, fault tolerance, uptime, failure detection time.\n",
    "- **SLO (Service Level Objective)**: What target values do we aim for?  \n",
    "Example: latency < 200ms.\n",
    "- **SLA (Service Level Agreement)**: An SLO backed with contractual consequences.  \n",
    "Example: \"99% uptime, otherwise partial refund.\"\n",
    "\n",
    "Why should we study these?\n",
    "- Measuring means we can improve\n",
    "- Spend time improving when it is needed.\n",
    "- Reliability is kind of the point with distributed systems.\n",
    "\n",
    "### Reading SLAs\n",
    "\n",
    "When evaluating claims like *“This solution offers 99% uptime”*, consider:\n",
    "\n",
    "- **Sampling frequency**: How often is system availability checked?\n",
    "- **Responsibility scope**: Does the SLA cover only server uptime, or also account for client/network failures?\n",
    "- **Time interval**: Does 99% apply per day, per month, or per year?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c259a",
   "metadata": {},
   "source": [
    "# LECTURE 15/09/2025\n",
    "\n",
    "## The Challenge of Time\n",
    "\n",
    "In distributed systems, we often contrast **synchronous** and **asynchronous** computation. A synchronous system has known, bounded delays for message delivery and process execution. An asynchronous system has no such guarantees. Most real-world systems are asynchronous, which makes coordination difficult. Without certain timing guarantees, some problems are impossible to solve deterministically, a classic example being the **Two Generals' Problem**, which illustrates the impossibility of reaching a consensus over an unreliable channel.\n",
    "\n",
    "### Reasons for Asynchrony\n",
    "Asynchrony isn't an abstract problem; it arises from concrete issues with the physical components of a system: the network and the nodes themselves.\n",
    "\n",
    "#### Network unpredictability:\n",
    "* **Physical failures:** Cables can be damaged (famously by sharks or cut by construction) requiring traffic to be rerouted. 🦈\n",
    "* **Message loss:** Packets can be dropped, requiring retransmission protocols (like TCP) to resend data.\n",
    "* **Congestion:** High traffic can lead to queues and variable delays (latency).\n",
    "* **Re-configuration:** The network topology itself may change, causing temporary disruptions.\n",
    "\n",
    "#### Node unpredictability:\n",
    "* **OS scheduling:** The operating system's scheduler can preempt a process at any time to run another one.\n",
    "* **Garbage collection (GC):** In managed languages (like Java or Go), a \"stop-the-world\" GC pause can halt an application for milliseconds or even seconds.\n",
    "* **Hardware faults:** Nodes can crash, reboot, or suffer from other hardware-related issues.\n",
    "\n",
    "But what if a system were \"perfect\"? Imagine no network loss and perfectly functioning nodes. Could asynchrony still occur? **Yes**. The non-deterministic nature of process scheduling is a fundamental source of asynchrony. A real-world example is the **2012 Knight Capital Group glitch**, where a software deployment error led to an algorithm running haywire. The system's components were working \"correctly,\" but the timing and interaction between processes led to a catastrophic failure, costing the company $440 million in 45 minutes.\n",
    "\n",
    "---\n",
    "\n",
    "## How Do Distributed Systems Use Time?\n",
    "\n",
    "Systems need to measure time for many fundamental operations. Think about how you would implement these on a single computer; in a distributed system, this becomes much harder.\n",
    "\n",
    "1.  **Scheduling and Timeouts:** To run a task for a specific duration or to give up on an operation if a response isn't received within a certain window.\n",
    "2.  **Failure Detection:** Using **heartbeats** (periodic \"I'm alive\" messages) to detect if a node has crashed. If a heartbeat isn't received within a timeout period, the node is presumed dead.\n",
    "3.  **Event Timestamping:** Recording the time an event occurred, which is critical in databases for transaction ordering and data versioning (e.g., using Multi-Version Concurrency Control or MVCC).\n",
    "4.  **Performance Measurement:** Logging and statistics gathering to measure latency, throughput, and other performance metrics.\n",
    "5.  **Data Expiration:** Caching systems use Time-To-Live (TTL) values to expire old data. DNS records and security certificates also have expiration times.\n",
    "6.  **Causal Ordering:** Most importantly, to determine the **order of events** across different nodes to maintain consistency and causality.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Clocks\n",
    "\n",
    "In distributed systems, we primarily talk about two types of clocks. From a practical standpoint, a clock is simply something we can query to get a timestamp.\n",
    "\n",
    "* **Physical Clocks:** These measure the passage of real-world time in units like seconds. They are based on physical phenomena, like the oscillation of a crystal.\n",
    "* **Logical Clocks:** These don't track real time. Instead, they count events (e.g., the number of requests processed) to determine the logical order of operations.\n",
    "\n",
    "---\n",
    "\n",
    "## Physical Clocks: The Quartz Crystal\n",
    "\n",
    "Most computers use quartz clocks. Here's how they work:\n",
    "\n",
    "* A thin slice of quartz crystal is precisely cut to control its oscillation frequency when an electric voltage is applied (the **piezoelectric effect**).\n",
    "* When you boot your computer, it queries a **Real-Time Clock (RTC)**—a small, battery-powered circuit on the motherboard—which has been continuously counting these oscillations.\n",
    "* By counting the cycles, the computer can calculate the elapsed time.\n",
    "\n",
    "However, these clocks aren't perfect:\n",
    "* **Manufacturing variations:** No two crystals are identical.\n",
    "* **Temperature sensitivity:** Frequency changes with temperature.\n",
    "* This imperfection leads to **clock drift**. We measure this in **parts per million (ppm)**. A drift of 1 ppm means the clock is off by one microsecond per second, which adds up to about **32 seconds per year**. A typical computer clock might have a drift of around 50 ppm.\n",
    "\n",
    "Better, but more expensive, alternatives include:\n",
    "* **Atomic clocks:** Extremely precise but very expensive.\n",
    "* **GPS:** Satellites contain atomic clocks. A GPS receiver can use signals from multiple satellites to calculate a very precise time.\n",
    "* **Network Time Protocol (NTP):** Ask another, more accurate server for the time.\n",
    "\n",
    "---\n",
    "\n",
    "## Time Standards and Representations\n",
    "\n",
    "To agree on time, we need standards.\n",
    "\n",
    "* **Solar Time (UT1):** Based on the Earth's rotation. A day is the time between the sun reaching its highest point in the sky on two consecutive days. This is not perfectly stable.\n",
    "* **International Atomic Time (TAI):** Based on the oscillations of a caesium-133 atom. One second is defined as exactly 9,192,631,770 oscillations. TAI is extremely stable.\n",
    "* **Coordinated Universal Time (UTC):** The global standard we all use. It's a compromise: it ticks at the same rate as TAI but is kept within 0.9 seconds of Solar Time (UT1) by adding **leap seconds**.\n",
    "\n",
    "### Leap Seconds\n",
    "To keep UTC aligned with the Earth's wobbly rotation, a second is occasionally added. This happens on June 30 or December 31.\n",
    "* **Positive leap second:** The time `23:59:59` is followed by `23:59:60`, and then `00:00:00`.\n",
    "* **Negative leap second:** `23:59:58` would be followed directly by `00:00:00`. (This has never happened).\n",
    "Leap seconds are a notorious source of bugs in computer systems.\n",
    "\n",
    "### Common Representations\n",
    "* **Unix time:** The number of seconds that have elapsed since `00:00:00 UTC` on 1 January 1970 (the \"epoch\"). Importantly, it **ignores leap seconds**; a day with a leap second is still counted as having 86,400 seconds.\n",
    "* **ISO 8601:** A standard format for representing dates and times, e.g., `2025-09-15T14:30:00Z` (where `Z` indicates UTC).\n",
    "\n",
    "---\n",
    "\n",
    "## Network Time Protocol (NTP)\n",
    "\n",
    "Since computer clocks drift, they need to be periodically corrected. NTP is the most common protocol for this. A client synchronizes its clock with a more accurate time server.\n",
    "\n",
    "The main protocols are **NTP** and the more precise **PTP** (Precision Time Protocol).\n",
    "On Ubuntu/Linux, you can check the time synchronization service with: `systemctl status systemd-timesyncd`.\n",
    "\n",
    "### NTP Synchronization Logic\n",
    "Let's analyze the message exchange between a client and a server.\n",
    "\n",
    "```\n",
    "\\--------t1-------------t4------------\\> NTP CLIENT\n",
    "           \\           /\n",
    "            \\         /\n",
    "\\------------t2-----t3----------------\\> NTP SERVER\n",
    "\n",
    "```\n",
    "\n",
    "* $T_1$: Client sends a request.\n",
    "* $T_2$: Server receives the request.\n",
    "* $T_3$: Server sends a response.\n",
    "* $T_4$: Client receives the response.\n",
    "\n",
    "The client can now calculate two important values:\n",
    "1.  **Round-trip delay ($\\delta$):** This is the total time the messages spent on the network, excluding the server's processing time.\n",
    "    $$\\delta = (T_4 - T_1) - (T_3 - T_2)$$\n",
    "2.  **Clock offset/skew ($\\theta$):** This is the client's best guess of the difference between its clock and the server's clock. Assuming the network delay is symmetric (i.e., the trip to the server takes as long as the trip back), the client calculates its offset as the difference between its local time ($T_4$) and what it thinks the server's time should be ($T_3$ plus half the round-trip delay).\n",
    "    $$\\theta = (T_3 + \\frac{\\delta}{2}) - T_4$$\n",
    "\n",
    "Based on the calculated offset $\\theta$, the client's clock is adjusted:\n",
    "* If $|\\theta| < 125ms$: **Slew** the clock. The clock is gradually sped up or slowed down until it's correct. This avoids sudden time jumps.\n",
    "* If $125ms \\le |\\theta| < 1000s$: **Jump** the clock. The time is set immediately. This can cause issues for applications sensitive to time reversals.\n",
    "* If $|\\theta| \\ge 1000s$: **Ignore**. The offset is too large and is likely an error, so the update is ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## Clock Types Revisited: Monotonic vs. Time-of-Day\n",
    "\n",
    "This brings us to two important types of clocks available in most programming environments.\n",
    "\n",
    "* **Time-of-day Clock:**\n",
    "    * Measures time since a fixed point in the past (e.g., the Unix epoch).\n",
    "    * **Not monotonic:** It can jump forwards or backwards due to NTP adjustments or leap seconds.\n",
    "    * Useful for timestamping events that need to be compared across different nodes.\n",
    "\n",
    "* **Monotonic Clock:**\n",
    "    * Measures time since an arbitrary point in the past (e.g., system boot).\n",
    "    * **Guaranteed to move forward** and is not affected by NTP jumps.\n",
    "    * Perfect for measuring elapsed time (e.g., timeouts) on a single node. You cannot use its value to compare timestamps between different nodes.\n",
    "\n",
    "Relying on physical clocks alone is insufficient for ordering events correctly in a distributed system due to clock skew and network latency.\n",
    "\n",
    "---\n",
    "\n",
    "## The Happens-Before Relation\n",
    "\n",
    "To reason about causality without perfect physical clocks, we use a logical concept called the **happens-before** relation, denoted by $\\rightarrow$. An event is an atomic operation on a single node.\n",
    "\n",
    "We say event **a happens before event b** ($a \\rightarrow b$) if one of the following is true:\n",
    "1.  `a` and `b` happen on the same node, and `a` occurs before `b`.\n",
    "2.  `a` is the sending of a message, and `b` is the receipt of that same message.\n",
    "3.  There exists some event `c` such that $a \\rightarrow c$ and $c \\rightarrow b$ (transitivity).\n",
    "\n",
    "This relation defines a **partial order**. It's possible that neither $a \\rightarrow b$ nor $b \\rightarrow a$. In this case, we say `a` and `b` are **concurrent**, written as $a || b$. This means we cannot determine their causal order from the information we have.\n",
    "\n",
    "This notion of causality is inspired by physics:\n",
    "* Information cannot travel faster than the speed of light. If two events in spacetime are too far apart to influence each other, they are not causally related.\n",
    "* In distributed systems, we replace the speed of light with the speed of messages. If no chain of messages connects event `a` to event `b`, then `a` cannot have caused `b`.\n",
    "\n",
    "---\n",
    "\n",
    "## Safety and Liveness\n",
    "\n",
    "When designing distributed algorithms, we want them to satisfy certain properties across all possible executions. These properties usually fall into two categories:\n",
    "\n",
    "* **Safety:** *Nothing bad ever happens.*\n",
    "    * A safety property, once violated, can never be undone. For example, \"a database will never return incorrect data.\" If it does so even once, the property is broken forever.\n",
    "* **Liveness:** *Something good eventually happens.*\n",
    "    * A liveness property can always be satisfied in the future. For example, \"every request will eventually receive a response.\" Even if a request is waiting, there's always the possibility it will be answered later.\n",
    "\n",
    "### Formal Definitions\n",
    "* **Safety:** A property is a safety property if for any execution where it is violated, there is a finite prefix of that execution after which the violation is guaranteed and unavoidable.\n",
    "* **Liveness:** A property is a liveness property if for any finite (partial) execution, there is at least one possible continuation of that execution where the property is satisfied.\n",
    "\n",
    "### Examples\n",
    "Consider a \"perfect link\" communication channel:\n",
    "* **Safety Property:** A process only receives messages that were actually sent. (Prevents the \"bad thing\" of phantom messages).\n",
    "* **Liveness Property:** If a correct process sends a message to another correct process, the destination eventually receives it. (Ensures the \"good thing\" of message delivery eventually happens).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599cc37",
   "metadata": {},
   "source": [
    "# LECTURE 16/09/2025\n",
    "\n",
    "## Multicast\n",
    "\n",
    "A multicast is a one-to-many communication where a single process sends a message to a specific group, and all members of that group receive it.\n",
    "\n",
    "### What is it?\n",
    "\n",
    "**Examples:**\n",
    "* **Systems needing redundancy:** Algorithms with failover or replication, such as in Databases, DNS, or Banks.\n",
    "* **One-to-many streaming:** Live TV/Radio broadcasts.\n",
    "* **Many-to-many collaboration:** Skype, Teams, TikTok, and Massively Multiplayer Online games (MMOs).\n",
    "\n",
    "**Disclaimer for this lecture:**\n",
    "* We assume groups are **closed and static** (no members joining or leaving).\n",
    "* We will not be discussing multiple overlapping groups.\n",
    "* We will **not assume any special hardware support** for multicast.\n",
    "* **Good news:** All algorithms shown work in both synchronous and asynchronous networks.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "**Assuming:**\n",
    "* We have **reliable 1-to-1 communication** (like TCP) as a building block.\n",
    "* The sending process might crash.\n",
    "* There is no default message ordering.\n",
    "\n",
    "**Guarantees we want:**\n",
    "* If a message is sent, it is **delivered exactly once**.\n",
    "* Messages are eventually delivered to all **non-crashed (correct) processes**.\n",
    "* The system is **fault-tolerant**; if one node fails, the rest can continue.\n",
    "\n",
    "### General broadcast structure\n",
    "\n",
    "We introduce a \"broadcast algorithm\" layer that sits between the application and the network.\n",
    "\n",
    "* **Node 1** doesn't send directly to the network; it tells the **broadcast algorithm** to broadcast a message.\n",
    "* The **broadcast algorithm** handles the logic of sending, re-transmitting, and ordering messages over the network.\n",
    "* The **broadcast algorithm** on the receiving end then **delivers** the message to Node 2.\n",
    "\n",
    "---\n",
    "\n",
    "## Problems - IP Multicast\n",
    "\n",
    "Standard IP Multicast often uses UDP, which offers no guarantees.\n",
    "\n",
    "* **No re-transmission:** Lost packets are gone forever.\n",
    "* **No reception guaranteed:** Messages might never arrive.\n",
    "* **No ordering:** Messages can be delivered in an arbitrary order.\n",
    "\n",
    "We need to build smarter algorithms to solve these problems.\n",
    "\n",
    "### Implementing reliable broadcast algorithms\n",
    "\n",
    "Different algorithms provide different ordering guarantees.\n",
    "\n",
    "* **FIFO broadcast:** If a process sends `m1` before `m2`, they are delivered in that order. Preserves order from a single sender.\n",
    "* **Causal broadcast:** If `broadcast(m1)` *happens-before* `broadcast(m2)`, then `m1` is delivered before `m2` everywhere. Preserves causality across different senders.\n",
    "* **Total order broadcast:** If one node delivers `m1` before `m2`, then *all* nodes must deliver `m1` before `m2`. Everyone agrees on a single, global delivery order.\n",
    "* **FIFO-total order broadcast:** A combination of both FIFO and total order guarantees.\n",
    "\n",
    "---\n",
    "\n",
    "## Hierarchy\n",
    "\n",
    "We can think of these broadcast types as layers, each adding stronger guarantees.\n",
    "\n",
    "* **Best-effort broadcast** is the unreliable base layer. We add re-transmission to get...\n",
    "* **Reliable broadcast**, which guarantees delivery but not order. From there, we can add...\n",
    "* **FIFO broadcast**, which doesn't re-order messages from the same sender. Then...\n",
    "* **Causal broadcast**, which doesn't re-order messages related by the happens-before rule. Finally...\n",
    "* **Total order broadcast**, which ensures all processes deliver messages in the exact same sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## Reliable Multicast\n",
    "\n",
    "### Properties\n",
    "\n",
    "A reliable multicast protocol must have these three properties:\n",
    "\n",
    "* **Integrity:** Messages are delivered at most once (no duplicates).\n",
    "* **Validity:** If a correct process sends a message, it is eventually delivered.\n",
    "* **Agreement:** If a correct process delivers a message, all other correct processes also deliver it.\n",
    "\n",
    "A naive implementation where everyone forwards to everyone else is inefficient ($O(N^2)$ messages). A better approach is **Gossip**, where each node forwards a message to a few random peers. This is far more scalable and works with high probability.\n",
    "\n",
    "---\n",
    "\n",
    "## Ordered Multicast\n",
    "\n",
    "### Details and implications\n",
    "\n",
    "1.  **FIFO and Causal ordering are partial orderings.** They don't specify the order for concurrent multicasts (those not linked by the happens-before relation).\n",
    "2.  **Reliable totally ordered multicast** is often called **atomic multicast**. It's a powerful tool for building consistent distributed systems.\n",
    "3.  **Ordering does not imply reliability.** A protocol could guarantee total order but still fail to deliver a message to a correct process, breaking the \"Agreement\" property.\n",
    "\n",
    "---\n",
    "\n",
    "## FIFO Broadcast\n",
    "\n",
    "Reliable multicast that respects sender order is a FIFO broadcast. This is typically implemented by having the sender add a sequence number to each message. Receivers only deliver messages from a specific sender in the order of their sequence numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## Totally Ordered Multicast\n",
    "\n",
    "This is complex because everyone must agree on a single, global message order.\n",
    "\n",
    "### Totally Ordered Multicast (Sequencer)\n",
    "\n",
    "We elect a single process to act as a **leader** or **sequencer**.\n",
    "\n",
    "1.  Processes send their messages to the sequencer.\n",
    "2.  The sequencer assigns a global, sequential number to each message and broadcasts it to the group.\n",
    "3.  All processes deliver messages in the order dictated by the sequencer.\n",
    "\n",
    "* **Problems:** The sequencer is a performance bottleneck and a single point of failure.\n",
    "\n",
    "### Totally Ordered Multicast (ISIS)\n",
    "\n",
    "A decentralized approach where processes negotiate the order.\n",
    "\n",
    "1.  Process `p` broadcasts a message `m` with a proposed ID.\n",
    "2.  Every other process `q` responds to `p` with its own proposed ID (typically the highest it has seen + 1).\n",
    "3.  Process `p` collects all proposals, picks the largest one as the final ID, and broadcasts this final ID to the group.\n",
    "\n",
    "* **The Trick:** Each process tracks the \"largest proposed ID\" and the \"largest agreed-upon ID\" to ensure it never delivers a message out of order.\n",
    "* **Tradeoff:** This is more robust than a sequencer but requires more communication rounds (3 rounds vs. the sequencer's 2).\n",
    "\n",
    "### Is it really ordered?\n",
    "\n",
    "Let's say process A sends `m` and `n`, and the protocol assigns the final timestamps `1` to `m` and `2` to `n`. A will deliver `m` before `n`. Could process B deliver `n` before `m`?\n",
    "\n",
    "* No. B receives the same final, agreed-upon timestamps of `1` for `m` and `2` for `n` via multicast. It cannot invent different ones.\n",
    "* What if B proposed a timestamp of `3` for `m`? Then the final, agreed-upon timestamp for `m` would have to be at least `3`, but we know it's `1`. This is a contradiction.\n",
    "* What if B wants to deliver `n` (with final timestamp `2`) before it has even heard of `m`? This is not possible, because B would have to participate in the proposal round for `m`. In that round, it would propose a timestamp larger than `2`, leading to `m`'s final timestamp being greater than `2`, which contradicts the fact that it's `1`.\n",
    "\n",
    "The protocol forces all nodes to converge on the same sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## Totaly order broadcast via Lamport clocks\n",
    "\n",
    "We can achieve total order by giving each message a logical timestamp.\n",
    "\n",
    "* **Idea:** Attach a Lamport timestamp to all messages and deliver them in timestamp order.\n",
    "* **Problem:** If I receive a message with timestamp 5, how do I know a message with timestamp 4 won't arrive later?\n",
    "* **Solution:** Use FIFO links. A process can only deliver the message with timestamp 5 after it has received a message with a timestamp *greater than 5* from **every other process**. This confirms no earlier messages are still in transit.\n",
    "\n",
    "---\n",
    "\n",
    "## Causal broadcast via lamport clock\n",
    "\n",
    "Physical clock timestamps may not respect causality. The solution is **Logical Clocks**. They are designed to capture the happens-before relation (`e1` ⇒ `e2` implies `T(e1) < T(e2)`).\n",
    "\n",
    "We will look at two types:\n",
    "1.  Lamport Clocks\n",
    "2.  Vector Clocks\n",
    "\n",
    "---\n",
    "\n",
    "## Lamport clocks Algorithm\n",
    "\n",
    "Each process maintains a single integer counter.\n",
    "\n",
    "* Each process initializes a local clock `t` to 0.\n",
    "* Before any event, a process increments its clock: `t = t + 1`.\n",
    "* When sending a message `m`, it sends the tuple `(t, m)`.\n",
    "* When receiving `(t_msg, m)`, a process updates its clock `t = max(t, t_msg)` and then increments it for the receive event.\n",
    "\n",
    "### Properties\n",
    "\n",
    "* If `a` happens-before `b` (`a` ⇒ `b`), then `L(a) < L(b)`.\n",
    "* However, if `L(a) < L(b)`, it does **not** mean `a` ⇒ `b`. They could be concurrent.\n",
    "\n",
    "---\n",
    "\n",
    "## Vector Clocks\n",
    "\n",
    "Each process `pi` maintains a vector `Vi` of size `N` (number of processes).\n",
    "\n",
    "* Initially, `Vi[j] = 0` for all `j`.\n",
    "* Before an event at `pi`, it increments its own clock entry: `Vi[i] = Vi[i] + 1`.\n",
    "* When sending a message, it attaches its entire vector `V`.\n",
    "* On receiving a message with vector `V'`, the process updates its local vector by taking the element-wise maximum: `Vi[j] = max(Vi[j], V'[j])` for all `j`.\n",
    "\n",
    "**Comparison Rules:**\n",
    "* `V = W` if `V[j] = W[j]` for all `j`.\n",
    "* `V ≤ W` if `V[j] ≤ W[j]` for all `j`.\n",
    "* `V < W` if `V ≤ W` and `V ≠ W`.\n",
    "\n",
    "### Vector Clocks, as used for CO Multicast\n",
    "\n",
    "To ensure causal order, when process `Pj` receives a message `m` from `Pi` (with vector `Vm`), it delays delivery of `m` until **both** conditions are met:\n",
    "\n",
    "1.  `Vm[i] = Vj[i] + 1`\n",
    "    * This ensures `m` is the very next message `Pj` expected from `Pi`.\n",
    "2.  `Vm[k] ≤ Vj[k]` for all `k != i`\n",
    "    * This ensures `Pj` has already delivered all messages that `Pi` had seen before it sent `m`.\n",
    "\n",
    "![image](../images/Screenshot%202025-09-16%20at%2012.41.19.png)\n",
    "\n",
    "As we can see in vector two we need to move the point of V2 = (1,1,0) ahead of (1,0,0) due to the order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b71429",
   "metadata": {},
   "source": [
    "# LECTURE 22/09/2025\n",
    "\n",
    "## Consensus\n",
    "In distributed computing, **consensus** is the fundamental challenge of getting a group of independent processes (or nodes) to **agree on a single value**. This agreed-upon value is final. Think of it as a committee that must vote on and finalize one decision, and once made, it cannot be changed.\n",
    "\n",
    "This is formally equivalent to **total order broadcast**, where processes must agree on the *sequence* of messages to deliver. If they can agree on the first message, then the second, then the third, and so on, they are effectively solving consensus for each message slot in the order.\n",
    "\n",
    "### Practical Examples\n",
    "* **Multicast & Bank Accounts**: Imagine you have $100 in an account replicated across multiple servers. If you deposit $50 and simultaneously withdraw $30, all servers must agree on the order of operations. Do they process the deposit first (balance becomes $120) or the withdrawal first (balance becomes $120)? They must reach a consensus to ensure the final balance is consistent everywhere.\n",
    "* **Redundancy**:\n",
    "    * **Space and Aeronautics**: The flight control computers on a spacecraft or modern airplane must agree on sensor readings and control actions. If one computer thinks the plane should pitch up and another thinks it should pitch down, they must reach a consensus to avoid a catastrophic failure.\n",
    "* **Replication**:\n",
    "    * **Distributed File Systems**: When you write to a file stored on Google Drive or Dropbox, multiple replicas of that data are updated. Consensus ensures all replicas agree on the latest version of the file.\n",
    "    * **Ledger Technology (e.g., Blockchain)**: A blockchain is essentially a chain of consensus decisions. Miners or validators around the world must agree on which block of new transactions is the next one to be added to the chain.\n",
    "\n",
    "### Common algorithms\n",
    "* **Paxos**: A classic and highly influential algorithm for reaching consensus on a single value in an asynchronous system where nodes can crash. **Multi-Paxos** extends this to agree on a sequence of values, effectively creating a total order broadcast.\n",
    "* **Raft, Viewstamped Replication, Zab**: These are more modern algorithms designed to be more understandable and easier to implement than Paxos. They solve total order broadcast by default, often by first electing a stable leader to coordinate decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## System model\n",
    "In distributed systems, we must define the \"rules of the game\" under which an algorithm operates. This is the **system model**, and it typically specifies:\n",
    "* **Network Behavior**: Are messages delivered reliably? Can they be delayed indefinitely (**asynchronous**) or is there a known maximum delay (**synchronous**)?\n",
    "* **Node Behavior**: How can processes fail? Can they simply stop (**crash-fail**) or can they behave maliciously and lie (**Byzantine**)?\n",
    "* **Timing Assumptions**: Do processes have access to synchronized clocks?\n",
    "\n",
    "The choice of system model drastically affects what problems are solvable.\n",
    "\n",
    "---\n",
    "\n",
    "## Reliable consensus vs failures summary\n",
    "Achieving consensus becomes progressively harder as the system becomes less reliable.\n",
    "\n",
    "* **No Failures (Easy Case)**: If no process ever fails, consensus is trivial.\n",
    "    1.  Every process broadcasts its proposed value to all others.\n",
    "    2.  Each process waits until it has received a value from every other process.\n",
    "    3.  Each process applies a simple function (like choosing the minimum value, or the first one received) to its collection of received values. Since everyone has the same set of values, they will all decide on the same outcome.\n",
    "\n",
    "* **With Crash Failures**: If processes can crash, the simple approach fails. A process might wait forever for a message from a crashed node.\n",
    "    * **The core problem**: How do you distinguish a process that is just very **slow** from one that is **dead**? This ambiguity is a central challenge in asynchronous systems.\n",
    "    * **Solution**: You need a **failure detector** mechanism to handle crashed nodes, but these are often imperfect.\n",
    "\n",
    "* **With Lies (Byzantine Failures)**: This is the hardest case. A faulty process can lie, sending value `A` to one node and value `B` to another.\n",
    "    * **The trust problem**: If you receive conflicting information, how do you know who is telling the truth?\n",
    "    * **Impact**: To tolerate these malicious failures, you need more nodes in total. Intuitively, you need enough honest nodes to \"outvote\" the liars. This significantly decreases the number of faulty nodes a system can withstand compared to simple crash failures.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements for consensus\n",
    "For a set of processes `p_i` proposing values, we define a set of formal properties that any correct consensus algorithm must satisfy. Each process has a decision variable `d_i`, initially set to `⊥` (undecided).\n",
    "\n",
    "* **Termination**: Eventually, every **correct** (non-faulty) process must decide on a value (i.e., set its `d_i` to something other than `⊥`). The system cannot get stuck forever.\n",
    "* **Agreement**: No two correct processes decide on different values. If process `p_i` decides `v_a` and process `p_j` decides `v_b`, then it must be that `v_a = v_b`.\n",
    "* **Integrity**: If all correct processes propose the same value `v`, then any correct process that decides must decide on that value `v`. This prevents trivial solutions like \"always decide 0\".\n",
    "* **Weak Integrity (often used)**: A slightly different version states that the decided value must have been proposed by at least one of the processes. This ensures the outcome is not just made up.\n",
    "\n",
    "A process `p_i` is in the **Decided State** as soon as its decision variable `d_i` is no longer `⊥`.\n",
    "\n",
    "---\n",
    "\n",
    "## Synchronous Consensus Algorithm\n",
    "In a **synchronous** system, we assume that message delivery and processing happen in lock-step **rounds**. There's a known upper bound on how long a message takes to arrive. This assumption simplifies things greatly but is often unrealistic.\n",
    "\n",
    "**Goal**: To create an algorithm that is resilient to `f` crash failures and computes the minimum proposed value.\n",
    "\n",
    "### f-resilient (synchronous) Consensus Algorithm\n",
    "The algorithm operates in `f + 1` rounds. In each round, every process broadcasts the set of values it knows about so far, and then updates its set with the values it receives from others.\n",
    "\n",
    "```\n",
    "1 v = { value from application (call x) }\n",
    "2 B-multicast(v)\n",
    "3 for each round i ∈ 1 ... f + 1 do\n",
    "4 v' = v\n",
    "5 for each m received do\n",
    "6 update v = v ∪ m\n",
    "7 end\n",
    "8 B-multicast(v \\ v') // not needed in round f+1\n",
    "9 end\n",
    "10 Pick d as minimal value of v\n",
    "11 return d\n",
    "```\n",
    "\n",
    "* **Initialization**: Each process `p_i` starts with a set of values `V_i = {v_i}`, where `v_i` is its own initial proposal.\n",
    "* **Rounds**: For `k` from 1 to `f + 1`:\n",
    "    1.  Each process `p_i` broadcasts its current set of values `V_i` to all other processes.\n",
    "    2.  Each process `p_i` waits to receive messages from all other non-faulty processes. It updates its set `V_i` by taking the union of its current set and all the sets it received in this round.\n",
    "* **Decision**: After `f + 1` rounds, each process `p_i` decides on the minimum value in its final set `V_i`.\n",
    "\n",
    "#### Why does this work? (Proof Sketch)\n",
    "The proof for **Agreement** works by contradiction.\n",
    "* **Assume** two correct processes, `p_i` and `p_j`, decide on different minimum values, `x` and `y`, where `x < y`.\n",
    "* This means that at the end of round `f+1`, `p_j`'s set of values *did not contain* `x`.\n",
    "* For this to happen, the value `x` must have been \"hidden\" from `p_j` for all `f + 1` rounds. The only way to hide a value is if a process holding it crashes before sending it.\n",
    "* But we assume there are at most `f` faulty processes. In each round, at most one new process can fail and \"block\" the propagation of `x`. Over `f+1` rounds, even if a different process fails each round, the value `x` from a correct process would have had at least one round to propagate to everyone.\n",
    "* Therefore, it's a **contradiction** to think `p_j` never received `x`. Both `p_i` and `p_j` must have the same set of values from all correct processes and will thus decide on the same minimum.\n",
    "\n",
    "### Theorem\n",
    "A famous result in distributed computing states that any optimal, deterministic consensus algorithm that can tolerate `f` crash failures requires **at least `f + 1` rounds** of communication in the worst case.\n",
    "\n",
    "---\n",
    "\n",
    "## Byzantine Error\n",
    "What if processes don't just crash, but behave unpredictably or maliciously? This is a **Byzantine error**. A Byzantine node can lie, send conflicting messages to different peers, or collude with other faulty nodes. This models the most challenging failure scenario. The name comes from Lamport's famous paper, \"The Byzantine Generals Problem.\"\n",
    "\n",
    "### Examples\n",
    "This isn't just a theoretical or software problem; it can be caused by hardware faults.\n",
    "* **Single Event Upset (SEU)**: A cosmic ray or high-energy particle strikes a memory cell, flipping a bit from 0 to 1 (or vice-versa). This can corrupt data or instructions, causing the node to behave erratically.\n",
    "* **Single Event Latchup (SEL)**: A hardware error that can cause a short-circuit, leading to unpredictable behavior or permanent damage.\n",
    "\n",
    "These issues are critical in:\n",
    "* Aerospace, where radiation is higher.\n",
    "* Systems using non-**ECC (Error-Correcting Code) memory**.\n",
    "* High-reliability systems like **nuclear power plants** or **avionics**.\n",
    "\n",
    "---\n",
    "\n",
    "## Byzantine Consensus\n",
    "To solve consensus with Byzantine failures, we need a stronger integrity property.\n",
    "\n",
    "* **Byzantine Integrity**: If all **non-faulty** (i.e., correct) processes start with the same value `v`, then all non-faulty processes must decide on `v`. This ensures that a few Byzantine nodes cannot trick the honest majority into deciding on a wrong value when they already agree.\n",
    "\n",
    "**Goal**: Design an `f`-byzantine-resilient synchronous consensus algorithm.\n",
    "\n",
    "### The Bad News: Impossibility Result\n",
    "A groundbreaking result shows that no solution can exist if the number of faulty nodes `f` is too high relative to the total number of nodes `n`. Consensus is **impossible for `f ≥ n/3`**, or `n ≤ 3f`.\n",
    "\n",
    "### The Good News\n",
    "If `n > 3f`, solutions are possible. For example, to tolerate 1 Byzantine fault (`f=1`), you need at least 4 nodes in total (`n=4`). To tolerate 2 (`f=2`), you need at least 7 (`n=7`).\n",
    "\n",
    "### f-byzantine resilience?\n",
    "Let's see why `n=3, f=1` is impossible.\n",
    "Imagine a Commander (C) sending an order (\"attack\" or \"retreat\") to two Lieutenants (L1, L2). One of them is a traitor.\n",
    "\n",
    "* **Scenario**: The Commander is the traitor. C tells L1 to \"attack\" and L2 to \"retreat\". Now L1 and L2 have conflicting information. L1 tells L2 \"C told me to attack\", and L2 tells L1 \"C told me to retreat\". L1 knows one of them is a traitor, but it could be C or L2. L2 faces the same dilemma. They cannot agree.\n",
    "\n",
    "### Byzantine Non-Consensus larger n simulation\n",
    "This is a proof technique to show that if a solution existed for `n ≤ 3f`, it would lead to a contradiction.\n",
    "\n",
    "* **Practical Example (Proof by Reduction)**: Let's **assume** we have a magical algorithm that solves Byzantine consensus for `n=3` generals with `f=1` traitor. We will use this faulty assumption to solve an even simpler problem, which we know is truly impossible, thereby proving our initial assumption was wrong.\n",
    "* The \"truly impossible\" problem is the `n=2, f=1` scenario (one Commander, one traitor Lieutenant). The Lieutenant can never know if the Commander is lying or not.\n",
    "* **The Simulation**:\n",
    "    1.  The Commander (C) and Lieutenant (L) in the `n=2` problem will *simulate* the `n=3, f=1` algorithm.\n",
    "    2.  C will simulate being the Commander from the `n=3` world.\n",
    "    3.  L will simulate being *both* Lieutenant 1 and Lieutenant 2 from the `n=3` world.\n",
    "    4.  They run the magical `n=3` algorithm on these simulated roles.\n",
    "* **The Contradiction**: The algorithm is supposed to work even with one traitor. In this simulation, if the real Commander C is the traitor, then the simulated Commander is the traitor. If the real Lieutenant L is the traitor, then the simulated L1 and L2 are traitors. In either case, the number of simulated traitors is at most 1. The `n=3` algorithm should therefore work, allowing the real Commander and Lieutenant to reach consensus.\n",
    "* But we know consensus is impossible for `n=2, f=1`! Since our \"magical\" algorithm allowed us to solve an unsolvable problem, the magical algorithm itself cannot exist. This logic extends to show `n ≤ 3f` is impossible in general.\n",
    "\n",
    "---\n",
    "\n",
    "## Three Equivalent Problems\n",
    "These three problems are different formulations of the same core challenge and can be transformed into one another.\n",
    "\n",
    "1.  **Consensus**:\n",
    "    * **Goal**: All processes propose a value `v_i`; they must agree on a single one.\n",
    "    * **Properties**: Termination, Agreement, Integrity.\n",
    "\n",
    "2.  **Byzantine Generals**:\n",
    "    * **Goal**: A single Commander issues an order to `n-1` Lieutenants. They must all agree on the order received.\n",
    "    * **Properties**: Termination, Agreement, and a special **Integrity**: If the Commander is correct, all correct Lieutenants must decide on the Commander's proposed order. (Note: if the Commander is faulty, they just need to agree on *some* order).\n",
    "\n",
    "3.  **Interactive Consistency**:\n",
    "    * **Goal**: Every process `p_i` proposes a value `v_i`. All correct processes must agree on the *same vector* of values `V = (v_1, v_2, ..., v_n)`.\n",
    "    * **Properties**: Termination, Agreement (on the whole vector), and **Integrity**: If process `p_i` is correct, then the `i`-th component of the decided vector must be `v_i`.\n",
    "\n",
    "### Equivalence of the problems\n",
    "* **Byzantine Generals (BG) to Interactive Consistency (IC)**: To agree on a vector, simply run the BG algorithm `n` times. In the first run, `p_1` acts as Commander. In the second, `p_2` acts as Commander, and so on. The final vector is built from the outcomes of each run.\n",
    "* **Interactive Consistency (IC) to Consensus (C)**: First, run IC to get an agreed-upon vector of proposals. Then, each process independently applies a deterministic function (e.g., `min()`, `max()`, `majority()`) to that vector to compute a single final value. Since they all start with the same vector and apply the same function, they will arrive at the same consensus value.\n",
    "* **Consensus (C) to Byzantine Generals (BG)**: The Commander sends its value to all Lieutenants. Then, every process (including the Commander) initiates a Consensus round, proposing the value it received (or its own value, if it's the Commander). Because the Consensus algorithm can tolerate traitors, the honest nodes will agree on a single value, achieving the BG goal.\n",
    "\n",
    "---\n",
    "\n",
    "## Byzantine Generals Algorithm (f=1)\n",
    "This is a simple synchronous algorithm that solves the problem for `n=4, f=1`. It takes two rounds of communication.\n",
    "\n",
    "```\n",
    "// Executed by the Commander\n",
    "def Commander:\n",
    "v = value from application // e.g., \"attack\"\n",
    "B-multicast(v) to all Lieutenants // Round 1: Send the order\n",
    "// Executed by each Lieutenant\n",
    "def Lieutenant:\n",
    "  let v = value received from commander\n",
    "  let i = my unique process id\n",
    "  // Round 2: Relay the order you received to everyone else\n",
    "  B-multicast(i : v) to all other Lieutenants\n",
    "  // Wait to receive messages from the other n-2 Lieutenants\n",
    "  // Decide based on the majority vote of all orders received\n",
    "  let d = the majority vote of received answers. If there's a tie, use a default.\n",
    "  return d\n",
    "```\n",
    "\n",
    "**Why it works for `n=4, f=1`**:\n",
    "* **Case 1: Commander is honest**. All 3 Lieutenants receive the same correct order. They will all decide on that order.\n",
    "* **Case 2: A Lieutenant is the traitor**. The 2 honest Lieutenants and the Commander are honest. The 2 honest Lieutenants receive the correct order from the Commander. When they exchange messages, they will each have 2 votes for the correct order (one from the C, one from the other honest L) and 1 vote for whatever the traitor says. The majority vote will be the correct order.\n",
    "\n",
    "---\n",
    "\n",
    "## Fixing the Async Problem\n",
    "In a purely asynchronous system, the famous **FLP Impossibility Result** proves that there is no deterministic algorithm that can solve consensus while tolerating even a single crash failure. The core issue is the inability to distinguish a crashed node from a very slow one.\n",
    "\n",
    "So how do we build real systems?\n",
    "* **Use randomness**: If we allow algorithms to use random numbers, we can design protocols that are guaranteed to reach consensus with a probability of 1. They might not terminate on a specific run, but over infinite runs, they will.\n",
    "\n",
    "---\n",
    "\n",
    "## Paxos\n",
    "\n",
    "### What is it?\n",
    "**Paxos** is a family of protocols for solving consensus in an asynchronous network where processors may fail by crashing (it does not handle Byzantine failures). It was created by Leslie Lamport.\n",
    "\n",
    "* **Key features**:\n",
    "    * It does **not** rely on a fixed coordinator/leader.\n",
    "    * It works in an **asynchronous** system.\n",
    "    * It is resilient to up to `(n-1)/2` crash failures.\n",
    "    * It prioritizes **safety (Agreement)** over **liveness (Termination)**. This means it will never allow two nodes to decide differently, but it's not guaranteed to make progress and decide at all.\n",
    "\n",
    "\n",
    "\n",
    "### The Paxos Nodes\n",
    "Paxos operates by electing a temporary \"leader\" (called a **Proposer**) for a specific decision. Any node can try to become a proposer. Nodes that are not proposers act as **Acceptors**, voting on proposals.\n",
    "* A node can become a **Proposer** at any time.\n",
    "* All nodes are **Acceptors**.\n",
    "* Nodes that learn the final outcome are **Learners**. In practice, all nodes often play all three roles.\n",
    "\n",
    "### Steps\n",
    "Paxos works in two phases to decide on a single value.\n",
    "\n",
    "**Phase 1: Prepare/Promise (Electing a Leader)**\n",
    "1.  A **Proposer** decides it wants to lead. It picks a proposal number `n` that is unique and higher than any number it has used before. It sends a `Prepare(n)` message to a majority of Acceptors.\n",
    "2.  An **Acceptor** receives `Prepare(n)`.\n",
    "    * If `n` is higher than any proposal number it has promised to listen to before, it responds with a `Promise(n)` message. This is a promise to not accept any proposals with a number less than `n`.\n",
    "    * **Crucially**: If the Acceptor has *already accepted* a value `val_prev` from a previous proposal `n_prev`, it must include `(n_prev, val_prev)` in its `Promise` response.\n",
    "    * If `n` is not the highest it has seen, it ignores the message.\n",
    "\n",
    "**Phase 2: Accept/Accepted (Deciding on a Value)**\n",
    "3.  The **Proposer** waits for `Promise` responses. If it receives them from a **majority** of Acceptors, it is now the leader for proposal `n`. It then chooses a value `val` to propose.\n",
    "    * **The Rule**: If any of the `Promise` responses it received contained a previously accepted value, the Proposer **must** choose the value `val_prev` associated with the highest proposal number `n_prev` it saw. Otherwise, it is free to propose its own initial value.\n",
    "    * It then sends an `Accept(n, val)` message to a majority of Acceptors.\n",
    "4.  An **Acceptor** receives `Accept(n, val)`.\n",
    "    * If it has not made a newer promise (to a proposal number higher than `n`), it accepts the value and sends an `Accepted(n, val)` message to all nodes (who act as Learners).\n",
    "    * Once a Learner sees `Accepted` messages from a majority of nodes for the same value, that value is **decided**.\n",
    "\n",
    "---\n",
    "\n",
    "## The proof of paxos\n",
    "\n",
    "We will not go through the formal proof, as it involves a very large and detailed analysis of all possible message orderings and failure scenarios. The key safety property relies on the rule in Step 3: forcing a new leader to continue with a value that might have already been decided ensures that once a value is chosen, it can never be changed.\n",
    "\n",
    "However, Paxos can fail to terminate. This does not violate its safety guarantee, but it does violate the **Termination** requirement for consensus.\n",
    "\n",
    "* **Practical Example of Non-Termination (Dueling Proposers)**:\n",
    "    1.  Proposer P1 sends `Prepare(n=10)` and gets promises from a majority of Acceptors (A, B, C). P1 is now leader.\n",
    "    2.  Before P1 can send its `Accept` message, another Proposer P2 wakes up, chooses a higher number, and sends `Prepare(n=11)` to the same Acceptors.\n",
    "    3.  Acceptors A, B, and C see this higher proposal number. They respond to P2 with `Promise(n=11)`, and will now ignore any messages related to `n=10`.\n",
    "    4.  P1 finally sends its `Accept(n=10, value=\"X\")` message, but it is ignored by the majority because they've promised to listen to `n=11`. P1's proposal fails.\n",
    "    5.  Now P2 has a majority of promises and is the leader. But before it can send its `Accept` message, P1 realizes it failed, chooses an even higher number, and sends `Prepare(n=12)`.\n",
    "    6.  This cycle can repeat indefinitely, with the two proposers constantly preempting each other, and no value is ever decided. This is a \"livelock\" situation. In practice, randomized timeouts are used to make this scenario highly unlikely.\n",
    "\n",
    "---\n",
    "\n",
    "## Resources and alternatives\n",
    "* **Google TechTalk on Paxos**: A good video resource for understanding the algorithm in more detail.\n",
    "* **Raft Algorithm Illustration**: [https://raft.github.io/](https://raft.github.io/) provides an excellent interactive visualization of Raft, an alternative to Paxos designed for understandability.\n",
    "\n",
    "---\n",
    "\n",
    "## Heartbeat for Synchronized Systems\n",
    "A **heartbeat** is a common mechanism for failure detection in systems that are not fully asynchronous.\n",
    "* **How it works**:\n",
    "    1.  You guess a reasonable upper bound for message delay, `D`.\n",
    "    2.  Each process sends a \"beat\" message to others every `T` seconds.\n",
    "    3.  If a process hasn't received a beat from another process in the last `T + D` seconds, it **suspects** that the process has crashed.\n",
    "\n",
    "* **The Trade-off**:\n",
    "    * If `D` is **too small**, you get **inaccurate** detections. A slow but perfectly alive process might be declared dead.\n",
    "    * If `D` is **too large**, your detection is **incomplete**. A dead process might be considered alive for a long time (a \"zombie\").\n",
    "\n",
    "This shows we can only ever **suspect** a crash in a distributed system; we can never be 100% certain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948eb7f3",
   "metadata": {},
   "source": [
    "# LECTURE 23/09/2025\n",
    "\n",
    "## Mutual Exclusions\n",
    "\n",
    "What is a mutual exclusion (mutex)?\n",
    "Mutual exclusion algorithms ensures that one and only one process can access a\n",
    "shared resource at any given time.\n",
    "\n",
    "### Examples\n",
    "▶ Printing\n",
    "▶ Using Coffee Machine\n",
    "▶ Writing a file\n",
    "▶ Changing the stat of an actuator\n",
    "    ▶ Arm of a robot\n",
    "▶ Wireless Communication\n",
    "▶ Wired Communication\n",
    "\n",
    "---\n",
    "\n",
    "## System model\n",
    "What is a (computer science) process?\n",
    "A process p = (S, sι, M, →) in a set of processes p ∈ P has\n",
    "▶ a set of states S,\n",
    "▶ an initial state sι ∈ S,\n",
    "▶ a set of messages M\n",
    "    ▶ including the empty message ϵ ∈ M,\n",
    "▶ and a transition function →⊆ S × M 7 → S × 2P×M \n",
    "\n",
    "---\n",
    "\n",
    "## Type of communication\n",
    "▶ We can have mutex algorithms both in message-parsing and in shared memory\n",
    "paradigms.\n",
    "▶ Message parsing : Processes do not share variables, but may access\n",
    "recourses/states that are considered common.\n",
    "▶ Shared memory : more literal: processes can edit the same data.\n",
    "▶ We will start with message parsing.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "▶ Processes have Crash Failures\n",
    "    ▶ Stay dead\n",
    "▶ Direct Communication\n",
    "    ▶ Transparent routing\n",
    "    ▶ No forwarding\n",
    "▶ Reliable Communication\n",
    "    ▶ Synchronous\n",
    "        ▶ Delivery within fixed timeframe\n",
    "    ▶ Asynchronous\n",
    "        ▶ Delivery at some point\n",
    "        ▶ Underlying protocol handles re-transmission etc.\n",
    "    ▶ Partitions are fixed eventually\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements (Mutex Algorithms)\n",
    "\n",
    "1. Safety\n",
    "▶ at most one is given access\n",
    "2. Liveness\n",
    "▶ requests for access are (eventually) granted\n",
    "3. Ordering/Fairness\n",
    "▶ request A happened-before request B\n",
    "⇒ grant A before B.\n",
    "▶ If two processes request periodically, grant the requests in some \"fair\" manner.\n",
    "\n",
    "---\n",
    "\n",
    "## Properties (Mutex Algorithms)\n",
    "▶ Fault tolerance\n",
    "    ▶ What happens on crashes?\n",
    "▶ Performance\n",
    "    ▶ Message Complexity\n",
    "    ▶ Client Delay\n",
    "        ▶ Time from a request R to a grant of R\n",
    "    ▶ Synchronization Delay\n",
    "        ▶ Time from a release of R to a grant of the next request Q\n",
    "        ▶ Related to the throughput (rate the processes can access the critical section)\n",
    "    ▶ Bandwidth: proportional to the number of messages sent in each entry and exit operation\n",
    "\n",
    "---\n",
    "\n",
    "## Centralized Algorithm - With a Token\n",
    "The simplest possible case\n",
    "▶ Assume one external coordinator (a leader )\n",
    "▶ Coordinator/leader/server has ordered queue (the ring).\n",
    "▶ The reply constitutes a token signifying permission to enter the critical section.\n",
    "A process:\n",
    "▶ Asks coordinator for access\n",
    "▶ Waits\n",
    "▶ Gets token, computes, returns token\n",
    "The coordinator:\n",
    "▶ Gets a request, checks if anyone is using the resource (where is token?).\n",
    "▶ If resource is empty: replies by giving token\n",
    "▶ If not: does not reply, queues the request\n",
    "▶ When token is returned, coordinator checks for next in queue.\n",
    "\n",
    "### Properties (Centralized Algorithm)\n",
    "\n",
    "Requirements\n",
    "▶ Safe: Yes\n",
    "▶ Liveness: Yes\n",
    "▶ Ordering: No\n",
    "Properties\n",
    "▶ Client Delay\n",
    "▶ Entry: 2 (Request + Grant)\n",
    "▶ Exit: 1\n",
    "▶ Synchronization Delay\n",
    "▶ 2 (Release + Grant)\n",
    "▶ : Bandwidth: 3 messages to enter and\n",
    "leave a critical region: A request, a grant\n",
    "to enter and a release to exit\n",
    "Fault Tolerance\n",
    "\n",
    "---\n",
    "\n",
    "## Token Ring Algorithm (no leader)\n",
    "Idea\n",
    "▶ Send token around in a ring\n",
    "▶ Assumes ordering of processes\n",
    "▶ Forward token to “next” if not using mutex\n",
    "▶ Enter mutex if token is acquired\n",
    "\n",
    "### Properties\n",
    "Requirements\n",
    "▶ Safe: Yes\n",
    "▶ Liveness: Yes\n",
    "▶ Ordering: No (order by ring)\n",
    "Properties\n",
    "▶ Client Delay\n",
    "▶ Entry: n/2 avg, n − 1 worst case\n",
    "▶ Exit: 1\n",
    "▶ Synchronization Delay\n",
    "▶ n/2 avg, n − 1 worst case\n",
    "▶ Bandwidth:\n",
    "Fault Tolerance\n",
    "Deadlock if any process fails\n",
    "Can be recovered if crash can be detected reliably\n",
    "\n",
    "---\n",
    "\n",
    "## Ricart and Agrawala’s Algorithm\n",
    "Idea\n",
    "▶ Order Events!\n",
    "▶ Extension of shared priority queue (Lamport ‘78)\n",
    "▶ Basic algorithm\n",
    "▶ Request all for access\n",
    "▶ Execute CS when ”reply OK” permission is received from all other processes.\n",
    "Secret Ingredient\n",
    "Lamport Clocks!\n",
    "\n",
    "### Lamport Clocks (reminder)\n",
    "▶ Counter number of messages/events\n",
    "▶ Annotate messages with clock\n",
    "▶ increment local clock before send\n",
    "▶ “Correct” local clock on receive, then increment when sending\n",
    "▶ max(A, B) + 1\n",
    "\n",
    "### Algorithm\n",
    "A process:\n",
    "▶ Has its Lamport clock + links to all other nodes.\n",
    "▶ Updates its local Lamport clock on its events.\n",
    "▶ Timestamps messages with Lamport clock + ID.\n",
    "▶ Has three states: FREE, WANT, USE\n",
    "\n",
    "### On a request:\n",
    "▶ If all other processes is FREE, then all processes will reply immediately → the\n",
    "requester obtain entry (becomes USE).\n",
    "▶ If process is in USE then waits and then replies.\n",
    "▶ On more than 1 request: Respond in order of timestamps (of requests): Break ties\n",
    "with ID.\n",
    "\n",
    "### Properties (Ricart and Agrawala)\n",
    "Requirements\n",
    "▶ Safe: Yes\n",
    "▶ Liveness: Yes\n",
    "▶ Ordering: Yes\n",
    "Properties\n",
    "▶ Client Delay\n",
    "▶ Entry: 1 (multicast) + 1\n",
    "▶ Exit: 1 (multicast)\n",
    "▶ Synchronization Delay\n",
    "▶ 1\n",
    "▶ Bandwidth:\n",
    "▶ Entry:\n",
    "▶ n − 1 + n − 1\n",
    "▶ Exit: up to n − 1, included into Entry\n",
    "Fault Tolerance\n",
    "Deadlock if any process fails\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95896e72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be0e3f02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "395dea2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b3d905",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c775234f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eabe455b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
