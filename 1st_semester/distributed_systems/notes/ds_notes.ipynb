{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2983dd2",
   "metadata": {},
   "source": [
    "## **COURSE OVERVIEW**\n",
    "\n",
    "## **INTRO**\n",
    "### 1. Introduction\n",
    "## **Theory & Algorithms**\n",
    "### 1. Models of DS\n",
    "### 2. Time in DS\n",
    "### 3. Multicast\n",
    "### 4. Consensus\n",
    "### 5. Distributed Mutual Exclusion\n",
    "## **Application**\n",
    "### 7. Distributed Storage\n",
    "### 8. Distributed Computing\n",
    "### 9. Blockchains\n",
    "### 10. Peer-to-Peer Networking\n",
    "### 11. Internet of Things and Routing\n",
    "### 12. Distributed Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27860a15",
   "metadata": {},
   "source": [
    "# Lecture 08/09/2025\n",
    "\n",
    "## What is a distributed systems?\n",
    "A distributed system is one where hardware and software components in/on networked computers communicate and coordinate their activity only by passing messages.\n",
    "\n",
    "## CONCURRENCY\n",
    "\n",
    "Concurrency is the ability of a system to execute multiple tasks or processes simultaneously or at overlapping times, improving efficiency.\n",
    "However it can cause some problems such as:\n",
    "- Deadlocks and livelocks: these are conditions in which the processes do not make progress due to their circumstances.\n",
    "- Non-determinism: occurs when the output or behavior of a concurrent program differs for the same input, depending on the precise, unpredictable timing of events, such as thread interleaving or resource access.\n",
    "\n",
    "Other issues could rise from the absence of shared state that will eventually lead to:\n",
    "- Pass messages to synchronize: for example if 2 people have a shared resource and both are trying to access it at the same time an error could occur for one member of the party.\n",
    "- May not agree on time: \n",
    "\n",
    "Everything can fail in distributed systems:\n",
    "- Devices\n",
    "- Integrity of data\n",
    "- Network\n",
    "    - Security\n",
    "    - Man-in-the-Middle (MITM): attack occurs when an attacker intercepts and potentially alters communication between two legitimate parties, unbeknownst to them.\n",
    "    - Zibantine failure: is a condition of a system, particularly a distributed computing system, where a fault occurs such that different symptoms are presented to different observers, including imperfect information on whether a system component has failed.\n",
    "\n",
    "Distributed systems are used for domain, redundancy, and performance.\n",
    "\n",
    "---\n",
    "## Domain\n",
    "A domain is a specific area of knowledge or activity that a distributed system is designed to address.\n",
    "Some examples of domains are:\n",
    "- The internet\n",
    "- Wireless Mesh Networks\n",
    "- Industrail systems\n",
    "- Ledgers (bitcoin, ethereum)\n",
    "However these domains can encounter some limits that can be physical and logical.\n",
    "- Physical limits: are constraints imposed by the physical properties of the system's components or environment, such as hardware limitations, network bandwidth, latency, and geographical distribution.   \n",
    "- Logical limits: defined by its bounded context. This is a core concept from Domain-Driven Design (DDD). The logical limit of a domain in a distributed system is defined by its bounded context. This is a core concept from Domain-Driven Design (DDD), a software development approach that focuses on aligning software design with the business domain. A bounded context is an explicit boundary within a distributed system where a specific domain model and its language (ubiquitous language) are consistent and applicable.\n",
    "---\n",
    "## Redundancy\n",
    "A system with redundacy means that it has duplicate ocmponents, processes or data.\n",
    "Given these specifications the system will result:\n",
    "- Robust: more resilient to failure\n",
    "- Available: as in system availability which is measured in uptime.\n",
    "\n",
    "A system with redundancy can:\n",
    "- offer 99.9% uptime or \"five nines\": means it's designed to be operational and available 99.9% of the time, with a small amount of planned or unplanned downtime.\n",
    "- be a backup: in an active-passive configuration, the redundant server acts as a hot or cold backup. The primary server handles all the workload, while the backup server waits on standby. Given the duplication of information and components if one fails the other automatically takes over.\n",
    "- be a database: In an active-active configuration, all redundant servers are considered active and work together simultaneously. They are not simply waiting as a backup. Incoming requests are distributed among all the servers using a load balancer.\n",
    "- used in the banking sector: any downtime can lead to significant financial losses.\n",
    "\n",
    "---\n",
    "## Performance\n",
    "To ensure a performant system we need:\n",
    "- Economics\n",
    "- Scalability\n",
    "Here we talk about different topics such as:\n",
    "- Video streaming: requires a lot of procesing power\n",
    "- Cloud computing: Offers on-demand, scalable resources, eliminating the need for upfront hardware investment.\n",
    "- Supercomputers: Excel at massive, specialized calculations but are very expensive and not scalable for general-purpose use.\n",
    "- Many inexpensive vs many expensive specialized: Distributing workloads across many inexpensive machines is often more economical and scalable than using a few expensive, specialized ones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b169279",
   "metadata": {},
   "source": [
    "# Lecture 09/09/2025\n",
    "\n",
    "# Models of distributed systems\n",
    "\n",
    "## Aspects of models\n",
    "\n",
    "Why do we build distributed systems?\n",
    "\n",
    "- **Inherent distribution**: By definition, distributed systems span multiple computers, often connected through networks such as telecommunications systems.\n",
    "- **Reliability**: Even if one node fails, the system as a whole can continue functioning, avoiding single points of failure.\n",
    "- **Performance**: Workloads can be shared among multiple machines, and data can be accessed from geographically closer nodes to reduce latency.\n",
    "- **Scalability for large problems**: Some datasets and computations are simply too large to fit into a single machine, requiring distributed processing.\n",
    "\n",
    "### Modelling the process ‚Äì API Style\n",
    "\n",
    "A distributed system can be described in terms of modules that exchange **events** through well-defined interfaces:\n",
    "\n",
    "- **Event representation**:  \n",
    "  \\{Event\\_type | Attributes, ‚Ä¶\\}  \n",
    "  or  \n",
    "  \\{Process\\_ID | Event\\_type | Attributes, ‚Ä¶\\}\n",
    "\n",
    "- **Module behavior**:  \n",
    "  Each module reacts to incoming events and produces outputs according to specified rules:\n",
    "upon event {condition | Event | attributes} such that condition holds do\n",
    "perform some action\n",
    "\n",
    "\n",
    "Multiple modules together (one per process or subsystem) should collectively satisfy desired **global properties** (e.g., safety, liveness).\n",
    "\n",
    "### What we want/will make\n",
    "\n",
    "We aim to:\n",
    "- Design APIs for modules and prove that their composition satisfies global system properties.\n",
    "- Implement modules that guarantee **local properties**.\n",
    "- Use pseudocode and mathematics to formally demonstrate when such guarantees are possible‚Äîor prove impossibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Failures\n",
    "\n",
    "Failures are inevitable in distributed systems. They can arise due to hardware breakdowns, software bugs, network disruptions, or even human mistakes. Designing robust systems requires understanding different types of failures and strategies to mitigate them.\n",
    "\n",
    "### Types of failures\n",
    "\n",
    "1. **Crash-stop**: A process halts and all other processes can reliably detect the failure. *Easiest to handle.*\n",
    "2. **Crash-silent**: A process halts but failures cannot be detected reliably.\n",
    "3. **Crash-noisy**: Failures may be detected, but only with eventual accuracy (false positives or delays are possible).\n",
    "4. **Crash-recovery**: Processes may fail and later recover, rejoining the system. Requires care to avoid state inconsistencies.\n",
    "5. **Crash-arbitrary (Byzantine failures)**: Processes behave arbitrarily or maliciously, deviating from the protocol. *Hardest to handle.*\n",
    "6. **Randomized behavior**: Processes make decisions probabilistically. Correctness is argued via probability theory rather than strict guarantees.\n",
    "\n",
    "---\n",
    "\n",
    "## Communication\n",
    "\n",
    "Is communication always required? In distributed systems, yes‚Äîbut it can be realized in different ways:\n",
    "\n",
    "- **Message passing**:\n",
    "1. Types of links and their potential failures.\n",
    "2. Network topology (commonly assumed fully connected).\n",
    "3. Routing algorithms for multi-hop communication.\n",
    "4. Broadcast and multicast primitives.\n",
    "\n",
    "- **Shared memory**:\n",
    "1. Which process can read or write to which location?\n",
    "2. How do we guarantee reading the *freshest* value? (Consistency models)\n",
    "\n",
    "### On types of links\n",
    "\n",
    "A **link** is a module implementing send/receive operations with certain properties.\n",
    "\n",
    "- **TCP/IP**: Enables reliable communication between a pair of nodes (or none).\n",
    "- **SSH**: Adds protection against corruption, interception, and tampering.\n",
    "\n",
    "**Network reliability models**:\n",
    "1. **Perfect links**: Reliable delivery, no duplication, no spurious messages.\n",
    "2. **Fair-loss links**: Messages may be lost occasionally, but infinitely many attempts guarantee eventual delivery; finite duplication possible.\n",
    "3. **Stubborn links**: Messages are retransmitted until delivery is guaranteed but still no creation (this model is built upon the fair-loss).\n",
    "4. **Logged-perfect links**: Perfect delivery with persistent logs for auditing/recovery.\n",
    "5. **Authenticated links**: Reliable delivery, no duplication, and sender authenticity.\n",
    "\n",
    "### Can networks fail?\n",
    "\n",
    "While TCP/IP and lower-level protocols often give us the illusion of **perfect links** and **fail-stop crashes**, failures still happen.\n",
    "\n",
    "- **Network partitions**: Occur when many links fail simultaneously, dividing the system into disconnected components. This is rare but catastrophic.\n",
    "\n",
    "### Crashes vs Failures\n",
    "\n",
    "Having discussed both **network** and **process** failures, it is important to distinguish between the two levels:\n",
    "\n",
    "- A **process can crash** (e.g., by crashing, halting, or misbehaving).  \n",
    "- A **system fails** when the combination of process crashes and communication assumptions no longer allows correct operation.\n",
    "\n",
    "For the remainder of our discussion, we usually assume **perfect links** (thanks to TCP/IP and lower-level reliability mechanisms). This means that:\n",
    "- Messages are delivered reliably,\n",
    "- No duplicates are created,\n",
    "- No spurious (phantom) messages appear.\n",
    "\n",
    "Under this assumption, we can define **system failure models** in terms of process behavior:\n",
    "\n",
    "- **Fail-stop system**: Processes may experience crash-stop failures, but links are perfect.  \n",
    "- More complex models (e.g., crash-recovery, Byzantine failures) are defined similarly, always considering both the **process failure type** and the **communication assumptions**.\n",
    "\n",
    "In short, a system failure model = (process failure model) + (assumed link properties).\n",
    "\n",
    "---\n",
    "\n",
    "## Timing\n",
    "\n",
    "Timing plays a central role in distributed systems, especially when considering **synchronization** and **failure detection**.\n",
    "\n",
    "- Systems may be **synchronous** (bounded delays) or **asynchronous** (no timing guarantees).\n",
    "- Links are still modeled as modules with send/receive properties.\n",
    "\n",
    "### Synchronous vs. Asynchronous Systems\n",
    "\n",
    "Distributed systems can be broadly classified according to their **timing assumptions**:\n",
    "\n",
    "1. **Asynchronous systems**:\n",
    "   - No bounds on message transmission delays.\n",
    "   - No assumptions about process execution speeds (relative speeds may differ arbitrarily).\n",
    "   - Failure detection is unreliable, since a slow process cannot be distinguished from a failed one.\n",
    "   - Coordination and ordering rely on **logical clocks** (e.g., Lamport clocks, vector clocks), rather than real time.\n",
    "\n",
    "2. **Synchronous systems**:\n",
    "   - Bounds exist on message transmission delays and process execution speeds.\n",
    "   - **Timed failure detection** is possible: if a message or heartbeat is not received within a known bound, a failure can be suspected reliably.\n",
    "   - Transit delays can be measured and incorporated into algorithms.\n",
    "   - Coordination can be based on **real-time clocks** rather than purely logical clocks.\n",
    "   - Performance is often analyzed in terms of **worst-case bounds**, since timing assumptions provide guarantees.\n",
    "   - Processes may maintain **synchronized clocks** (to some degree of precision), enabling algorithms such as consensus and coordinated actions.\n",
    "\n",
    "**Key question**: *Can processes in an asynchronous system with fair-loss links reach agreement (e.g., on coordinated attack time)?*\n",
    "\n",
    "### Proof via contradiction (Two Generals Problem)\n",
    "\n",
    "1. Assume a protocol exists where a fixed sequence of messages guarantees agreement.\n",
    "2. Consider the last message in this sequence that is successfully delivered.\n",
    "3. If this message is lost, the receiving general decides **not** to attack.\n",
    "4. But the sender cannot distinguish whether the message was delivered or lost, so must behave deterministically and decide the same action in both cases.\n",
    "5. This creates a contradiction: one general attacks, the other does not.  \n",
    " $\\Rightarrow$ Perfect agreement is impossible under these assumptions.\n",
    "\n",
    "### Which crash/link/timing assumptions implement distributed systems?\n",
    "\n",
    "A **failure detector** can be modeled as just another module that provides (possibly imperfect) information about which processes are alive. Different combinations of timing assumptions and failure detectors allow different guarantees in distributed systems.  \n",
    "\n",
    "### Example\n",
    "\n",
    "![image](../images/Screenshot%202025-09-09%20at%2009.56.39.png)\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "This algorithm describes a **Perfect Failure Detector** for distributed systems using a heartbeat mechanism.\n",
    "\n",
    "In short, here's what it does:\n",
    "\n",
    "1.  **Sends Heartbeats:** Periodically, on a **timeout**, every process sends a `HEARTBEATREQUEST` message to all other processes in the system.\n",
    "2.  **Waits for Replies:** It assumes no one is alive and waits for `HEARTBEATREPLY` messages. When a process receives a reply, it marks the sender as `alive`.\n",
    "3.  **Detects Failures:** At the next timeout, any process that has not sent a reply is considered to have **crashed**. The algorithm then triggers a `Crash` event for that process.\n",
    "\n",
    "Because it assumes **perfect communication links** (messages are never lost), this method guarantees that a non-responsive process has truly failed, making the failure detection \"perfect.\"\n",
    "\n",
    "### Network latency and bandwith\n",
    "\n",
    "When discussing communication performance, two key metrics matter:\n",
    "\n",
    "- **Latency**: The time it takes for a single message (or bit) to travel from sender to receiver.  \n",
    "- **Bandwidth**: The rate at which data can be transmitted, usually measured in bits per second (bps) or bytes per second (B/s).\n",
    "\n",
    "#### Physical Link\n",
    "Sometimes, surprisingly ‚Äúlow-tech‚Äù physical methods can provide high bandwidth, even if latency is poor:\n",
    "- **Hard drives in a van**  \n",
    "- Messengers carrying storage devices  \n",
    "- Smoke signals (extreme latency, minimal bandwidth)  \n",
    "- Radio signals or laser communication\n",
    "\n",
    "#### Network Links\n",
    "More conventional digital communication technologies include:\n",
    "- DSL (Digital Subscriber Line)  \n",
    "- Cellular data (e.g., 3G, 4G, 5G)  \n",
    "- Wi-Fi (various standards)  \n",
    "- Ethernet/fiber cables  \n",
    "- Satellite links  \n",
    "\n",
    "#### Latency examples\n",
    "1. Hard drives transported by van: $\\approx$ 1 day latency  \n",
    "2. Intra-continent fiber-optic cable: $\\approx$ 100 ms latency  \n",
    "\n",
    "#### Bandwidth examples\n",
    "1. Hard drives in a van: $\\frac{50 \\, \\text{TB}}{1 \\, \\text{day}}$ = **very high bandwidth** despite huge latency  \n",
    "2. 3G cellular network: $\\approx 1 \\, \\text{Mbit/s}$ bandwidth  \n",
    "\n",
    "---\n",
    "\n",
    "## Performance\n",
    "\n",
    "### Performance measures\n",
    "\n",
    "- **SLI (Service Level Indicator)**: What aspect of the system do we measure?  \n",
    "Examples: bandwidth, latency, fault tolerance, uptime, failure detection time.\n",
    "- **SLO (Service Level Objective)**: What target values do we aim for?  \n",
    "Example: latency < 200ms.\n",
    "- **SLA (Service Level Agreement)**: An SLO backed with contractual consequences.  \n",
    "Example: \"99% uptime, otherwise partial refund.\"\n",
    "\n",
    "Why should we study these?\n",
    "- Measuring means we can improve\n",
    "- Spend time improving when it is needed.\n",
    "- Reliability is kind of the point with distributed systems.\n",
    "\n",
    "### Reading SLAs\n",
    "\n",
    "When evaluating claims like *‚ÄúThis solution offers 99% uptime‚Äù*, consider:\n",
    "\n",
    "- **Sampling frequency**: How often is system availability checked?\n",
    "- **Responsibility scope**: Does the SLA cover only server uptime, or also account for client/network failures?\n",
    "- **Time interval**: Does 99% apply per day, per month, or per year?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c259a",
   "metadata": {},
   "source": [
    "# LECTURE 15/09/2025\n",
    "\n",
    "## The Challenge of Time\n",
    "\n",
    "In distributed systems, we often contrast **synchronous** and **asynchronous** computation. A synchronous system has known, bounded delays for message delivery and process execution. An asynchronous system has no such guarantees. Most real-world systems are asynchronous, which makes coordination difficult. Without certain timing guarantees, some problems are impossible to solve deterministically, a classic example being the **Two Generals' Problem**, which illustrates the impossibility of reaching a consensus over an unreliable channel.\n",
    "\n",
    "### Reasons for Asynchrony\n",
    "Asynchrony isn't an abstract problem; it arises from concrete issues with the physical components of a system: the network and the nodes themselves.\n",
    "\n",
    "#### Network unpredictability:\n",
    "* **Physical failures:** Cables can be damaged (famously by sharks or cut by construction) requiring traffic to be rerouted. ü¶à\n",
    "* **Message loss:** Packets can be dropped, requiring retransmission protocols (like TCP) to resend data.\n",
    "* **Congestion:** High traffic can lead to queues and variable delays (latency).\n",
    "* **Re-configuration:** The network topology itself may change, causing temporary disruptions.\n",
    "\n",
    "#### Node unpredictability:\n",
    "* **OS scheduling:** The operating system's scheduler can preempt a process at any time to run another one.\n",
    "* **Garbage collection (GC):** In managed languages (like Java or Go), a \"stop-the-world\" GC pause can halt an application for milliseconds or even seconds.\n",
    "* **Hardware faults:** Nodes can crash, reboot, or suffer from other hardware-related issues.\n",
    "\n",
    "But what if a system were \"perfect\"? Imagine no network loss and perfectly functioning nodes. Could asynchrony still occur? **Yes**. The non-deterministic nature of process scheduling is a fundamental source of asynchrony. A real-world example is the **2012 Knight Capital Group glitch**, where a software deployment error led to an algorithm running haywire. The system's components were working \"correctly,\" but the timing and interaction between processes led to a catastrophic failure, costing the company $440 million in 45 minutes.\n",
    "\n",
    "---\n",
    "\n",
    "## How Do Distributed Systems Use Time?\n",
    "\n",
    "Systems need to measure time for many fundamental operations. Think about how you would implement these on a single computer; in a distributed system, this becomes much harder.\n",
    "\n",
    "1.  **Scheduling and Timeouts:** To run a task for a specific duration or to give up on an operation if a response isn't received within a certain window.\n",
    "2.  **Failure Detection:** Using **heartbeats** (periodic \"I'm alive\" messages) to detect if a node has crashed. If a heartbeat isn't received within a timeout period, the node is presumed dead.\n",
    "3.  **Event Timestamping:** Recording the time an event occurred, which is critical in databases for transaction ordering and data versioning (e.g., using Multi-Version Concurrency Control or MVCC).\n",
    "4.  **Performance Measurement:** Logging and statistics gathering to measure latency, throughput, and other performance metrics.\n",
    "5.  **Data Expiration:** Caching systems use Time-To-Live (TTL) values to expire old data. DNS records and security certificates also have expiration times.\n",
    "6.  **Causal Ordering:** Most importantly, to determine the **order of events** across different nodes to maintain consistency and causality.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Clocks\n",
    "\n",
    "In distributed systems, we primarily talk about two types of clocks. From a practical standpoint, a clock is simply something we can query to get a timestamp.\n",
    "\n",
    "* **Physical Clocks:** These measure the passage of real-world time in units like seconds. They are based on physical phenomena, like the oscillation of a crystal.\n",
    "* **Logical Clocks:** These don't track real time. Instead, they count events (e.g., the number of requests processed) to determine the logical order of operations.\n",
    "\n",
    "---\n",
    "\n",
    "## Physical Clocks: The Quartz Crystal\n",
    "\n",
    "Most computers use quartz clocks. Here's how they work:\n",
    "\n",
    "* A thin slice of quartz crystal is precisely cut to control its oscillation frequency when an electric voltage is applied (the **piezoelectric effect**).\n",
    "* When you boot your computer, it queries a **Real-Time Clock (RTC)**‚Äîa small, battery-powered circuit on the motherboard‚Äîwhich has been continuously counting these oscillations.\n",
    "* By counting the cycles, the computer can calculate the elapsed time.\n",
    "\n",
    "However, these clocks aren't perfect:\n",
    "* **Manufacturing variations:** No two crystals are identical.\n",
    "* **Temperature sensitivity:** Frequency changes with temperature.\n",
    "* This imperfection leads to **clock drift**. We measure this in **parts per million (ppm)**. A drift of 1 ppm means the clock is off by one microsecond per second, which adds up to about **32 seconds per year**. A typical computer clock might have a drift of around 50 ppm.\n",
    "\n",
    "Better, but more expensive, alternatives include:\n",
    "* **Atomic clocks:** Extremely precise but very expensive.\n",
    "* **GPS:** Satellites contain atomic clocks. A GPS receiver can use signals from multiple satellites to calculate a very precise time.\n",
    "* **Network Time Protocol (NTP):** Ask another, more accurate server for the time.\n",
    "\n",
    "---\n",
    "\n",
    "## Time Standards and Representations\n",
    "\n",
    "To agree on time, we need standards.\n",
    "\n",
    "* **Solar Time (UT1):** Based on the Earth's rotation. A day is the time between the sun reaching its highest point in the sky on two consecutive days. This is not perfectly stable.\n",
    "* **International Atomic Time (TAI):** Based on the oscillations of a caesium-133 atom. One second is defined as exactly 9,192,631,770 oscillations. TAI is extremely stable.\n",
    "* **Coordinated Universal Time (UTC):** The global standard we all use. It's a compromise: it ticks at the same rate as TAI but is kept within 0.9 seconds of Solar Time (UT1) by adding **leap seconds**.\n",
    "\n",
    "### Leap Seconds\n",
    "To keep UTC aligned with the Earth's wobbly rotation, a second is occasionally added. This happens on June 30 or December 31.\n",
    "* **Positive leap second:** The time `23:59:59` is followed by `23:59:60`, and then `00:00:00`.\n",
    "* **Negative leap second:** `23:59:58` would be followed directly by `00:00:00`. (This has never happened).\n",
    "Leap seconds are a notorious source of bugs in computer systems.\n",
    "\n",
    "### Common Representations\n",
    "* **Unix time:** The number of seconds that have elapsed since `00:00:00 UTC` on 1 January 1970 (the \"epoch\"). Importantly, it **ignores leap seconds**; a day with a leap second is still counted as having 86,400 seconds.\n",
    "* **ISO 8601:** A standard format for representing dates and times, e.g., `2025-09-15T14:30:00Z` (where `Z` indicates UTC).\n",
    "\n",
    "---\n",
    "\n",
    "## Network Time Protocol (NTP)\n",
    "\n",
    "Since computer clocks drift, they need to be periodically corrected. NTP is the most common protocol for this. A client synchronizes its clock with a more accurate time server.\n",
    "\n",
    "The main protocols are **NTP** and the more precise **PTP** (Precision Time Protocol).\n",
    "On Ubuntu/Linux, you can check the time synchronization service with: `systemctl status systemd-timesyncd`.\n",
    "\n",
    "### NTP Synchronization Logic\n",
    "Let's analyze the message exchange between a client and a server.\n",
    "\n",
    "```\n",
    "\\--------t1-------------t4------------\\> NTP CLIENT\n",
    "           \\           /\n",
    "            \\         /\n",
    "\\------------t2-----t3----------------\\> NTP SERVER\n",
    "\n",
    "```\n",
    "\n",
    "* $T_1$: Client sends a request.\n",
    "* $T_2$: Server receives the request.\n",
    "* $T_3$: Server sends a response.\n",
    "* $T_4$: Client receives the response.\n",
    "\n",
    "The client can now calculate two important values:\n",
    "1.  **Round-trip delay ($\\delta$):** This is the total time the messages spent on the network, excluding the server's processing time.\n",
    "    $$\\delta = (T_4 - T_1) - (T_3 - T_2)$$\n",
    "2.  **Clock offset/skew ($\\theta$):** This is the client's best guess of the difference between its clock and the server's clock. Assuming the network delay is symmetric (i.e., the trip to the server takes as long as the trip back), the client calculates its offset as the difference between its local time ($T_4$) and what it thinks the server's time should be ($T_3$ plus half the round-trip delay).\n",
    "    $$\\theta = (T_3 + \\frac{\\delta}{2}) - T_4$$\n",
    "\n",
    "Based on the calculated offset $\\theta$, the client's clock is adjusted:\n",
    "* If $|\\theta| < 125ms$: **Slew** the clock. The clock is gradually sped up or slowed down until it's correct. This avoids sudden time jumps.\n",
    "* If $125ms \\le |\\theta| < 1000s$: **Jump** the clock. The time is set immediately. This can cause issues for applications sensitive to time reversals.\n",
    "* If $|\\theta| \\ge 1000s$: **Ignore**. The offset is too large and is likely an error, so the update is ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## Clock Types Revisited: Monotonic vs. Time-of-Day\n",
    "\n",
    "This brings us to two important types of clocks available in most programming environments.\n",
    "\n",
    "* **Time-of-day Clock:**\n",
    "    * Measures time since a fixed point in the past (e.g., the Unix epoch).\n",
    "    * **Not monotonic:** It can jump forwards or backwards due to NTP adjustments or leap seconds.\n",
    "    * Useful for timestamping events that need to be compared across different nodes.\n",
    "\n",
    "* **Monotonic Clock:**\n",
    "    * Measures time since an arbitrary point in the past (e.g., system boot).\n",
    "    * **Guaranteed to move forward** and is not affected by NTP jumps.\n",
    "    * Perfect for measuring elapsed time (e.g., timeouts) on a single node. You cannot use its value to compare timestamps between different nodes.\n",
    "\n",
    "Relying on physical clocks alone is insufficient for ordering events correctly in a distributed system due to clock skew and network latency.\n",
    "\n",
    "---\n",
    "\n",
    "## The Happens-Before Relation\n",
    "\n",
    "To reason about causality without perfect physical clocks, we use a logical concept called the **happens-before** relation, denoted by $\\rightarrow$. An event is an atomic operation on a single node.\n",
    "\n",
    "We say event **a happens before event b** ($a \\rightarrow b$) if one of the following is true:\n",
    "1.  `a` and `b` happen on the same node, and `a` occurs before `b`.\n",
    "2.  `a` is the sending of a message, and `b` is the receipt of that same message.\n",
    "3.  There exists some event `c` such that $a \\rightarrow c$ and $c \\rightarrow b$ (transitivity).\n",
    "\n",
    "This relation defines a **partial order**. It's possible that neither $a \\rightarrow b$ nor $b \\rightarrow a$. In this case, we say `a` and `b` are **concurrent**, written as $a || b$. This means we cannot determine their causal order from the information we have.\n",
    "\n",
    "This notion of causality is inspired by physics:\n",
    "* Information cannot travel faster than the speed of light. If two events in spacetime are too far apart to influence each other, they are not causally related.\n",
    "* In distributed systems, we replace the speed of light with the speed of messages. If no chain of messages connects event `a` to event `b`, then `a` cannot have caused `b`.\n",
    "\n",
    "---\n",
    "\n",
    "## Safety and Liveness\n",
    "\n",
    "When designing distributed algorithms, we want them to satisfy certain properties across all possible executions. These properties usually fall into two categories:\n",
    "\n",
    "* **Safety:** *Nothing bad ever happens.*\n",
    "    * A safety property, once violated, can never be undone. For example, \"a database will never return incorrect data.\" If it does so even once, the property is broken forever.\n",
    "* **Liveness:** *Something good eventually happens.*\n",
    "    * A liveness property can always be satisfied in the future. For example, \"every request will eventually receive a response.\" Even if a request is waiting, there's always the possibility it will be answered later.\n",
    "\n",
    "### Formal Definitions\n",
    "* **Safety:** A property is a safety property if for any execution where it is violated, there is a finite prefix of that execution after which the violation is guaranteed and unavoidable.\n",
    "* **Liveness:** A property is a liveness property if for any finite (partial) execution, there is at least one possible continuation of that execution where the property is satisfied.\n",
    "\n",
    "### Examples\n",
    "Consider a \"perfect link\" communication channel:\n",
    "* **Safety Property:** A process only receives messages that were actually sent. (Prevents the \"bad thing\" of phantom messages).\n",
    "* **Liveness Property:** If a correct process sends a message to another correct process, the destination eventually receives it. (Ensures the \"good thing\" of message delivery eventually happens).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599cc37",
   "metadata": {},
   "source": [
    "# LECTURE 16/09/2025\n",
    "\n",
    "## Multicast\n",
    "\n",
    "A multicast is a one-to-many communication where a single process sends a message to a specific group, and all members of that group receive it.\n",
    "\n",
    "### What is it?\n",
    "\n",
    "**Examples:**\n",
    "* **Systems needing redundancy:** Algorithms with failover or replication, such as in Databases, DNS, or Banks.\n",
    "* **One-to-many streaming:** Live TV/Radio broadcasts.\n",
    "* **Many-to-many collaboration:** Skype, Teams, TikTok, and Massively Multiplayer Online games (MMOs).\n",
    "\n",
    "**Disclaimer for this lecture:**\n",
    "* We assume groups are **closed and static** (no members joining or leaving).\n",
    "* We will not be discussing multiple overlapping groups.\n",
    "* We will **not assume any special hardware support** for multicast.\n",
    "* **Good news:** All algorithms shown work in both synchronous and asynchronous networks.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "**Assuming:**\n",
    "* We have **reliable 1-to-1 communication** (like TCP) as a building block.\n",
    "* The sending process might crash.\n",
    "* There is no default message ordering.\n",
    "\n",
    "**Guarantees we want:**\n",
    "* If a message is sent, it is **delivered exactly once**.\n",
    "* Messages are eventually delivered to all **non-crashed (correct) processes**.\n",
    "* The system is **fault-tolerant**; if one node fails, the rest can continue.\n",
    "\n",
    "### General broadcast structure\n",
    "\n",
    "We introduce a \"broadcast algorithm\" layer that sits between the application and the network.\n",
    "\n",
    "* **Node 1** doesn't send directly to the network; it tells the **broadcast algorithm** to broadcast a message.\n",
    "* The **broadcast algorithm** handles the logic of sending, re-transmitting, and ordering messages over the network.\n",
    "* The **broadcast algorithm** on the receiving end then **delivers** the message to Node 2.\n",
    "\n",
    "---\n",
    "\n",
    "## Problems - IP Multicast\n",
    "\n",
    "Standard IP Multicast often uses UDP, which offers no guarantees.\n",
    "\n",
    "* **No re-transmission:** Lost packets are gone forever.\n",
    "* **No reception guaranteed:** Messages might never arrive.\n",
    "* **No ordering:** Messages can be delivered in an arbitrary order.\n",
    "\n",
    "We need to build smarter algorithms to solve these problems.\n",
    "\n",
    "### Implementing reliable broadcast algorithms\n",
    "\n",
    "Different algorithms provide different ordering guarantees.\n",
    "\n",
    "* **FIFO broadcast:** If a process sends `m1` before `m2`, they are delivered in that order. Preserves order from a single sender.\n",
    "* **Causal broadcast:** If `broadcast(m1)` *happens-before* `broadcast(m2)`, then `m1` is delivered before `m2` everywhere. Preserves causality across different senders.\n",
    "* **Total order broadcast:** If one node delivers `m1` before `m2`, then *all* nodes must deliver `m1` before `m2`. Everyone agrees on a single, global delivery order.\n",
    "* **FIFO-total order broadcast:** A combination of both FIFO and total order guarantees.\n",
    "\n",
    "---\n",
    "\n",
    "## Hierarchy\n",
    "\n",
    "We can think of these broadcast types as layers, each adding stronger guarantees.\n",
    "\n",
    "* **Best-effort broadcast** is the unreliable base layer. We add re-transmission to get...\n",
    "* **Reliable broadcast**, which guarantees delivery but not order. From there, we can add...\n",
    "* **FIFO broadcast**, which doesn't re-order messages from the same sender. Then...\n",
    "* **Causal broadcast**, which doesn't re-order messages related by the happens-before rule. Finally...\n",
    "* **Total order broadcast**, which ensures all processes deliver messages in the exact same sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## Reliable Multicast\n",
    "\n",
    "### Properties\n",
    "\n",
    "A reliable multicast protocol must have these three properties:\n",
    "\n",
    "* **Integrity:** Messages are delivered at most once (no duplicates).\n",
    "* **Validity:** If a correct process sends a message, it is eventually delivered.\n",
    "* **Agreement:** If a correct process delivers a message, all other correct processes also deliver it.\n",
    "\n",
    "A naive implementation where everyone forwards to everyone else is inefficient ($O(N^2)$ messages). A better approach is **Gossip**, where each node forwards a message to a few random peers. This is far more scalable and works with high probability.\n",
    "\n",
    "---\n",
    "\n",
    "## Ordered Multicast\n",
    "\n",
    "### Details and implications\n",
    "\n",
    "1.  **FIFO and Causal ordering are partial orderings.** They don't specify the order for concurrent multicasts (those not linked by the happens-before relation).\n",
    "2.  **Reliable totally ordered multicast** is often called **atomic multicast**. It's a powerful tool for building consistent distributed systems.\n",
    "3.  **Ordering does not imply reliability.** A protocol could guarantee total order but still fail to deliver a message to a correct process, breaking the \"Agreement\" property.\n",
    "\n",
    "---\n",
    "\n",
    "## FIFO Broadcast\n",
    "\n",
    "Reliable multicast that respects sender order is a FIFO broadcast. This is typically implemented by having the sender add a sequence number to each message. Receivers only deliver messages from a specific sender in the order of their sequence numbers.\n",
    "\n",
    "---\n",
    "\n",
    "## Totally Ordered Multicast\n",
    "\n",
    "This is complex because everyone must agree on a single, global message order.\n",
    "\n",
    "### Totally Ordered Multicast (Sequencer)\n",
    "\n",
    "We elect a single process to act as a **leader** or **sequencer**.\n",
    "\n",
    "1.  Processes send their messages to the sequencer.\n",
    "2.  The sequencer assigns a global, sequential number to each message and broadcasts it to the group.\n",
    "3.  All processes deliver messages in the order dictated by the sequencer.\n",
    "\n",
    "* **Problems:** The sequencer is a performance bottleneck and a single point of failure.\n",
    "\n",
    "### Totally Ordered Multicast (ISIS)\n",
    "\n",
    "A decentralized approach where processes negotiate the order.\n",
    "\n",
    "1.  Process `p` broadcasts a message `m` with a proposed ID.\n",
    "2.  Every other process `q` responds to `p` with its own proposed ID (typically the highest it has seen + 1).\n",
    "3.  Process `p` collects all proposals, picks the largest one as the final ID, and broadcasts this final ID to the group.\n",
    "\n",
    "* **The Trick:** Each process tracks the \"largest proposed ID\" and the \"largest agreed-upon ID\" to ensure it never delivers a message out of order.\n",
    "* **Tradeoff:** This is more robust than a sequencer but requires more communication rounds (3 rounds vs. the sequencer's 2).\n",
    "\n",
    "### Is it really ordered?\n",
    "\n",
    "Let's say process A sends `m` and `n`, and the protocol assigns the final timestamps `1` to `m` and `2` to `n`. A will deliver `m` before `n`. Could process B deliver `n` before `m`?\n",
    "\n",
    "* No. B receives the same final, agreed-upon timestamps of `1` for `m` and `2` for `n` via multicast. It cannot invent different ones.\n",
    "* What if B proposed a timestamp of `3` for `m`? Then the final, agreed-upon timestamp for `m` would have to be at least `3`, but we know it's `1`. This is a contradiction.\n",
    "* What if B wants to deliver `n` (with final timestamp `2`) before it has even heard of `m`? This is not possible, because B would have to participate in the proposal round for `m`. In that round, it would propose a timestamp larger than `2`, leading to `m`'s final timestamp being greater than `2`, which contradicts the fact that it's `1`.\n",
    "\n",
    "The protocol forces all nodes to converge on the same sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## Totaly order broadcast via Lamport clocks\n",
    "\n",
    "We can achieve total order by giving each message a logical timestamp.\n",
    "\n",
    "* **Idea:** Attach a Lamport timestamp to all messages and deliver them in timestamp order.\n",
    "* **Problem:** If I receive a message with timestamp 5, how do I know a message with timestamp 4 won't arrive later?\n",
    "* **Solution:** Use FIFO links. A process can only deliver the message with timestamp 5 after it has received a message with a timestamp *greater than 5* from **every other process**. This confirms no earlier messages are still in transit.\n",
    "\n",
    "---\n",
    "\n",
    "## Causal broadcast via lamport clock\n",
    "\n",
    "Physical clock timestamps may not respect causality. The solution is **Logical Clocks**. They are designed to capture the happens-before relation (`e1` ‚áí `e2` implies `T(e1) < T(e2)`).\n",
    "\n",
    "We will look at two types:\n",
    "1.  Lamport Clocks\n",
    "2.  Vector Clocks\n",
    "\n",
    "---\n",
    "\n",
    "## Lamport clocks Algorithm\n",
    "\n",
    "Each process maintains a single integer counter.\n",
    "\n",
    "* Each process initializes a local clock `t` to 0.\n",
    "* Before any event, a process increments its clock: `t = t + 1`.\n",
    "* When sending a message `m`, it sends the tuple `(t, m)`.\n",
    "* When receiving `(t_msg, m)`, a process updates its clock `t = max(t, t_msg)` and then increments it for the receive event.\n",
    "\n",
    "### Properties\n",
    "\n",
    "* If `a` happens-before `b` (`a` ‚áí `b`), then `L(a) < L(b)`.\n",
    "* However, if `L(a) < L(b)`, it does **not** mean `a` ‚áí `b`. They could be concurrent.\n",
    "\n",
    "---\n",
    "\n",
    "## Vector Clocks\n",
    "\n",
    "Each process `pi` maintains a vector `Vi` of size `N` (number of processes).\n",
    "\n",
    "* Initially, `Vi[j] = 0` for all `j`.\n",
    "* Before an event at `pi`, it increments its own clock entry: `Vi[i] = Vi[i] + 1`.\n",
    "* When sending a message, it attaches its entire vector `V`.\n",
    "* On receiving a message with vector `V'`, the process updates its local vector by taking the element-wise maximum: `Vi[j] = max(Vi[j], V'[j])` for all `j`.\n",
    "\n",
    "**Comparison Rules:**\n",
    "* `V = W` if `V[j] = W[j]` for all `j`.\n",
    "* `V ‚â§ W` if `V[j] ‚â§ W[j]` for all `j`.\n",
    "* `V < W` if `V ‚â§ W` and `V ‚â† W`.\n",
    "\n",
    "### Vector Clocks, as used for CO Multicast\n",
    "\n",
    "To ensure causal order, when process `Pj` receives a message `m` from `Pi` (with vector `Vm`), it delays delivery of `m` until **both** conditions are met:\n",
    "\n",
    "1.  `Vm[i] = Vj[i] + 1`\n",
    "    * This ensures `m` is the very next message `Pj` expected from `Pi`.\n",
    "2.  `Vm[k] ‚â§ Vj[k]` for all `k != i`\n",
    "    * This ensures `Pj` has already delivered all messages that `Pi` had seen before it sent `m`.\n",
    "\n",
    "![image](../images/Screenshot%202025-09-16%20at%2012.41.19.png)\n",
    "\n",
    "As we can see in vector two we need to move the point of V2 = (1,1,0) ahead of (1,0,0) due to the order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b71429",
   "metadata": {},
   "source": [
    "# LECTURE 22/09/2025\n",
    "\n",
    "## Consensus\n",
    "In distributed computing, **consensus** is the fundamental challenge of getting a group of independent processes (or nodes) to **agree on a single value**. This agreed-upon value is final. Think of it as a committee that must vote on and finalize one decision, and once made, it cannot be changed.\n",
    "\n",
    "This is formally equivalent to **total order broadcast**, where processes must agree on the *sequence* of messages to deliver. If they can agree on the first message, then the second, then the third, and so on, they are effectively solving consensus for each message slot in the order.\n",
    "\n",
    "### Practical Examples\n",
    "* **Multicast & Bank Accounts**: Imagine you have $100 in an account replicated across multiple servers. If you deposit $50 and simultaneously withdraw $30, all servers must agree on the order of operations. Do they process the deposit first (balance becomes $120) or the withdrawal first (balance becomes $120)? They must reach a consensus to ensure the final balance is consistent everywhere.\n",
    "* **Redundancy**:\n",
    "    * **Space and Aeronautics**: The flight control computers on a spacecraft or modern airplane must agree on sensor readings and control actions. If one computer thinks the plane should pitch up and another thinks it should pitch down, they must reach a consensus to avoid a catastrophic failure.\n",
    "* **Replication**:\n",
    "    * **Distributed File Systems**: When you write to a file stored on Google Drive or Dropbox, multiple replicas of that data are updated. Consensus ensures all replicas agree on the latest version of the file.\n",
    "    * **Ledger Technology (e.g., Blockchain)**: A blockchain is essentially a chain of consensus decisions. Miners or validators around the world must agree on which block of new transactions is the next one to be added to the chain.\n",
    "\n",
    "### Common algorithms\n",
    "* **Paxos**: A classic and highly influential algorithm for reaching consensus on a single value in an asynchronous system where nodes can crash. **Multi-Paxos** extends this to agree on a sequence of values, effectively creating a total order broadcast.\n",
    "* **Raft, Viewstamped Replication, Zab**: These are more modern algorithms designed to be more understandable and easier to implement than Paxos. They solve total order broadcast by default, often by first electing a stable leader to coordinate decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## System model\n",
    "In distributed systems, we must define the \"rules of the game\" under which an algorithm operates. This is the **system model**, and it typically specifies:\n",
    "* **Network Behavior**: Are messages delivered reliably? Can they be delayed indefinitely (**asynchronous**) or is there a known maximum delay (**synchronous**)?\n",
    "* **Node Behavior**: How can processes fail? Can they simply stop (**crash-fail**) or can they behave maliciously and lie (**Byzantine**)?\n",
    "* **Timing Assumptions**: Do processes have access to synchronized clocks?\n",
    "\n",
    "The choice of system model drastically affects what problems are solvable.\n",
    "\n",
    "---\n",
    "\n",
    "## Reliable consensus vs failures summary\n",
    "Achieving consensus becomes progressively harder as the system becomes less reliable.\n",
    "\n",
    "* **No Failures (Easy Case)**: If no process ever fails, consensus is trivial.\n",
    "    1.  Every process broadcasts its proposed value to all others.\n",
    "    2.  Each process waits until it has received a value from every other process.\n",
    "    3.  Each process applies a simple function (like choosing the minimum value, or the first one received) to its collection of received values. Since everyone has the same set of values, they will all decide on the same outcome.\n",
    "\n",
    "* **With Crash Failures**: If processes can crash, the simple approach fails. A process might wait forever for a message from a crashed node.\n",
    "    * **The core problem**: How do you distinguish a process that is just very **slow** from one that is **dead**? This ambiguity is a central challenge in asynchronous systems.\n",
    "    * **Solution**: You need a **failure detector** mechanism to handle crashed nodes, but these are often imperfect.\n",
    "\n",
    "* **With Lies (Byzantine Failures)**: This is the hardest case. A faulty process can lie, sending value `A` to one node and value `B` to another.\n",
    "    * **The trust problem**: If you receive conflicting information, how do you know who is telling the truth?\n",
    "    * **Impact**: To tolerate these malicious failures, you need more nodes in total. Intuitively, you need enough honest nodes to \"outvote\" the liars. This significantly decreases the number of faulty nodes a system can withstand compared to simple crash failures.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements for consensus\n",
    "For a set of processes `p_i` proposing values, we define a set of formal properties that any correct consensus algorithm must satisfy. Each process has a decision variable `d_i`, initially set to `‚ä•` (undecided).\n",
    "\n",
    "* **Termination**: Eventually, every **correct** (non-faulty) process must decide on a value (i.e., set its `d_i` to something other than `‚ä•`). The system cannot get stuck forever.\n",
    "* **Agreement**: No two correct processes decide on different values. If process `p_i` decides `v_a` and process `p_j` decides `v_b`, then it must be that `v_a = v_b`.\n",
    "* **Integrity**: If all correct processes propose the same value `v`, then any correct process that decides must decide on that value `v`. This prevents trivial solutions like \"always decide 0\".\n",
    "* **Weak Integrity (often used)**: A slightly different version states that the decided value must have been proposed by at least one of the processes. This ensures the outcome is not just made up.\n",
    "\n",
    "A process `p_i` is in the **Decided State** as soon as its decision variable `d_i` is no longer `‚ä•`.\n",
    "\n",
    "---\n",
    "\n",
    "## Synchronous Consensus Algorithm\n",
    "In a **synchronous** system, we assume that message delivery and processing happen in lock-step **rounds**. There's a known upper bound on how long a message takes to arrive. This assumption simplifies things greatly but is often unrealistic.\n",
    "\n",
    "**Goal**: To create an algorithm that is resilient to `f` crash failures and computes the minimum proposed value.\n",
    "\n",
    "### f-resilient (synchronous) Consensus Algorithm\n",
    "The algorithm operates in `f + 1` rounds. In each round, every process broadcasts the set of values it knows about so far, and then updates its set with the values it receives from others.\n",
    "\n",
    "```\n",
    "1 v = { value from application (call x) }\n",
    "2 B-multicast(v)\n",
    "3 for each round i ‚àà 1 ... f + 1 do\n",
    "4 v' = v\n",
    "5 for each m received do\n",
    "6 update v = v ‚à™ m\n",
    "7 end\n",
    "8 B-multicast(v \\ v') // not needed in round f+1\n",
    "9 end\n",
    "10 Pick d as minimal value of v\n",
    "11 return d\n",
    "```\n",
    "\n",
    "* **Initialization**: Each process `p_i` starts with a set of values `V_i = {v_i}`, where `v_i` is its own initial proposal.\n",
    "* **Rounds**: For `k` from 1 to `f + 1`:\n",
    "    1.  Each process `p_i` broadcasts its current set of values `V_i` to all other processes.\n",
    "    2.  Each process `p_i` waits to receive messages from all other non-faulty processes. It updates its set `V_i` by taking the union of its current set and all the sets it received in this round.\n",
    "* **Decision**: After `f + 1` rounds, each process `p_i` decides on the minimum value in its final set `V_i`.\n",
    "\n",
    "#### Why does this work? (Proof Sketch)\n",
    "The proof for **Agreement** works by contradiction.\n",
    "* **Assume** two correct processes, `p_i` and `p_j`, decide on different minimum values, `x` and `y`, where `x < y`.\n",
    "* This means that at the end of round `f+1`, `p_j`'s set of values *did not contain* `x`.\n",
    "* For this to happen, the value `x` must have been \"hidden\" from `p_j` for all `f + 1` rounds. The only way to hide a value is if a process holding it crashes before sending it.\n",
    "* But we assume there are at most `f` faulty processes. In each round, at most one new process can fail and \"block\" the propagation of `x`. Over `f+1` rounds, even if a different process fails each round, the value `x` from a correct process would have had at least one round to propagate to everyone.\n",
    "* Therefore, it's a **contradiction** to think `p_j` never received `x`. Both `p_i` and `p_j` must have the same set of values from all correct processes and will thus decide on the same minimum.\n",
    "\n",
    "### Theorem\n",
    "A famous result in distributed computing states that any optimal, deterministic consensus algorithm that can tolerate `f` crash failures requires **at least `f + 1` rounds** of communication in the worst case.\n",
    "\n",
    "---\n",
    "\n",
    "## Byzantine Error\n",
    "What if processes don't just crash, but behave unpredictably or maliciously? This is a **Byzantine error**. A Byzantine node can lie, send conflicting messages to different peers, or collude with other faulty nodes. This models the most challenging failure scenario. The name comes from Lamport's famous paper, \"The Byzantine Generals Problem.\"\n",
    "\n",
    "### Examples\n",
    "This isn't just a theoretical or software problem; it can be caused by hardware faults.\n",
    "* **Single Event Upset (SEU)**: A cosmic ray or high-energy particle strikes a memory cell, flipping a bit from 0 to 1 (or vice-versa). This can corrupt data or instructions, causing the node to behave erratically.\n",
    "* **Single Event Latchup (SEL)**: A hardware error that can cause a short-circuit, leading to unpredictable behavior or permanent damage.\n",
    "\n",
    "These issues are critical in:\n",
    "* Aerospace, where radiation is higher.\n",
    "* Systems using non-**ECC (Error-Correcting Code) memory**.\n",
    "* High-reliability systems like **nuclear power plants** or **avionics**.\n",
    "\n",
    "---\n",
    "\n",
    "## Byzantine Consensus\n",
    "To solve consensus with Byzantine failures, we need a stronger integrity property.\n",
    "\n",
    "* **Byzantine Integrity**: If all **non-faulty** (i.e., correct) processes start with the same value `v`, then all non-faulty processes must decide on `v`. This ensures that a few Byzantine nodes cannot trick the honest majority into deciding on a wrong value when they already agree.\n",
    "\n",
    "**Goal**: Design an `f`-byzantine-resilient synchronous consensus algorithm.\n",
    "\n",
    "### The Bad News: Impossibility Result\n",
    "A groundbreaking result shows that no solution can exist if the number of faulty nodes `f` is too high relative to the total number of nodes `n`. Consensus is **impossible for `f ‚â• n/3`**, or `n ‚â§ 3f`.\n",
    "\n",
    "### The Good News\n",
    "If `n > 3f`, solutions are possible. For example, to tolerate 1 Byzantine fault (`f=1`), you need at least 4 nodes in total (`n=4`). To tolerate 2 (`f=2`), you need at least 7 (`n=7`).\n",
    "\n",
    "### f-byzantine resilience?\n",
    "Let's see why `n=3, f=1` is impossible.\n",
    "Imagine a Commander (C) sending an order (\"attack\" or \"retreat\") to two Lieutenants (L1, L2). One of them is a traitor.\n",
    "\n",
    "* **Scenario**: The Commander is the traitor. C tells L1 to \"attack\" and L2 to \"retreat\". Now L1 and L2 have conflicting information. L1 tells L2 \"C told me to attack\", and L2 tells L1 \"C told me to retreat\". L1 knows one of them is a traitor, but it could be C or L2. L2 faces the same dilemma. They cannot agree.\n",
    "\n",
    "### Byzantine Non-Consensus larger n simulation\n",
    "This is a proof technique to show that if a solution existed for `n ‚â§ 3f`, it would lead to a contradiction.\n",
    "\n",
    "* **Practical Example (Proof by Reduction)**: Let's **assume** we have a magical algorithm that solves Byzantine consensus for `n=3` generals with `f=1` traitor. We will use this faulty assumption to solve an even simpler problem, which we know is truly impossible, thereby proving our initial assumption was wrong.\n",
    "* The \"truly impossible\" problem is the `n=2, f=1` scenario (one Commander, one traitor Lieutenant). The Lieutenant can never know if the Commander is lying or not.\n",
    "* **The Simulation**:\n",
    "    1.  The Commander (C) and Lieutenant (L) in the `n=2` problem will *simulate* the `n=3, f=1` algorithm.\n",
    "    2.  C will simulate being the Commander from the `n=3` world.\n",
    "    3.  L will simulate being *both* Lieutenant 1 and Lieutenant 2 from the `n=3` world.\n",
    "    4.  They run the magical `n=3` algorithm on these simulated roles.\n",
    "* **The Contradiction**: The algorithm is supposed to work even with one traitor. In this simulation, if the real Commander C is the traitor, then the simulated Commander is the traitor. If the real Lieutenant L is the traitor, then the simulated L1 and L2 are traitors. In either case, the number of simulated traitors is at most 1. The `n=3` algorithm should therefore work, allowing the real Commander and Lieutenant to reach consensus.\n",
    "* But we know consensus is impossible for `n=2, f=1`! Since our \"magical\" algorithm allowed us to solve an unsolvable problem, the magical algorithm itself cannot exist. This logic extends to show `n ‚â§ 3f` is impossible in general.\n",
    "\n",
    "---\n",
    "\n",
    "## Three Equivalent Problems\n",
    "These three problems are different formulations of the same core challenge and can be transformed into one another.\n",
    "\n",
    "1.  **Consensus**:\n",
    "    * **Goal**: All processes propose a value `v_i`; they must agree on a single one.\n",
    "    * **Properties**: Termination, Agreement, Integrity.\n",
    "\n",
    "2.  **Byzantine Generals**:\n",
    "    * **Goal**: A single Commander issues an order to `n-1` Lieutenants. They must all agree on the order received.\n",
    "    * **Properties**: Termination, Agreement, and a special **Integrity**: If the Commander is correct, all correct Lieutenants must decide on the Commander's proposed order. (Note: if the Commander is faulty, they just need to agree on *some* order).\n",
    "\n",
    "3.  **Interactive Consistency**:\n",
    "    * **Goal**: Every process `p_i` proposes a value `v_i`. All correct processes must agree on the *same vector* of values `V = (v_1, v_2, ..., v_n)`.\n",
    "    * **Properties**: Termination, Agreement (on the whole vector), and **Integrity**: If process `p_i` is correct, then the `i`-th component of the decided vector must be `v_i`.\n",
    "\n",
    "### Equivalence of the problems\n",
    "* **Byzantine Generals (BG) to Interactive Consistency (IC)**: To agree on a vector, simply run the BG algorithm `n` times. In the first run, `p_1` acts as Commander. In the second, `p_2` acts as Commander, and so on. The final vector is built from the outcomes of each run.\n",
    "* **Interactive Consistency (IC) to Consensus (C)**: First, run IC to get an agreed-upon vector of proposals. Then, each process independently applies a deterministic function (e.g., `min()`, `max()`, `majority()`) to that vector to compute a single final value. Since they all start with the same vector and apply the same function, they will arrive at the same consensus value.\n",
    "* **Consensus (C) to Byzantine Generals (BG)**: The Commander sends its value to all Lieutenants. Then, every process (including the Commander) initiates a Consensus round, proposing the value it received (or its own value, if it's the Commander). Because the Consensus algorithm can tolerate traitors, the honest nodes will agree on a single value, achieving the BG goal.\n",
    "\n",
    "---\n",
    "\n",
    "## Byzantine Generals Algorithm (f=1)\n",
    "This is a simple synchronous algorithm that solves the problem for `n=4, f=1`. It takes two rounds of communication.\n",
    "\n",
    "```\n",
    "// Executed by the Commander\n",
    "def Commander:\n",
    "v = value from application // e.g., \"attack\"\n",
    "B-multicast(v) to all Lieutenants // Round 1: Send the order\n",
    "// Executed by each Lieutenant\n",
    "def Lieutenant:\n",
    "  let v = value received from commander\n",
    "  let i = my unique process id\n",
    "  // Round 2: Relay the order you received to everyone else\n",
    "  B-multicast(i : v) to all other Lieutenants\n",
    "  // Wait to receive messages from the other n-2 Lieutenants\n",
    "  // Decide based on the majority vote of all orders received\n",
    "  let d = the majority vote of received answers. If there's a tie, use a default.\n",
    "  return d\n",
    "```\n",
    "\n",
    "**Why it works for `n=4, f=1`**:\n",
    "* **Case 1: Commander is honest**. All 3 Lieutenants receive the same correct order. They will all decide on that order.\n",
    "* **Case 2: A Lieutenant is the traitor**. The 2 honest Lieutenants and the Commander are honest. The 2 honest Lieutenants receive the correct order from the Commander. When they exchange messages, they will each have 2 votes for the correct order (one from the C, one from the other honest L) and 1 vote for whatever the traitor says. The majority vote will be the correct order.\n",
    "\n",
    "---\n",
    "\n",
    "## Fixing the Async Problem\n",
    "In a purely asynchronous system, the famous **FLP Impossibility Result** proves that there is no deterministic algorithm that can solve consensus while tolerating even a single crash failure. The core issue is the inability to distinguish a crashed node from a very slow one.\n",
    "\n",
    "So how do we build real systems?\n",
    "* **Use randomness**: If we allow algorithms to use random numbers, we can design protocols that are guaranteed to reach consensus with a probability of 1. They might not terminate on a specific run, but over infinite runs, they will.\n",
    "\n",
    "---\n",
    "\n",
    "## Paxos\n",
    "\n",
    "### What is it?\n",
    "**Paxos** is a family of protocols for solving consensus in an asynchronous network where processors may fail by crashing (it does not handle Byzantine failures). It was created by Leslie Lamport.\n",
    "\n",
    "* **Key features**:\n",
    "    * It does **not** rely on a fixed coordinator/leader.\n",
    "    * It works in an **asynchronous** system.\n",
    "    * It is resilient to up to `(n-1)/2` crash failures.\n",
    "    * It prioritizes **safety (Agreement)** over **liveness (Termination)**. This means it will never allow two nodes to decide differently, but it's not guaranteed to make progress and decide at all.\n",
    "\n",
    "\n",
    "\n",
    "### The Paxos Nodes\n",
    "Paxos operates by electing a temporary \"leader\" (called a **Proposer**) for a specific decision. Any node can try to become a proposer. Nodes that are not proposers act as **Acceptors**, voting on proposals.\n",
    "* A node can become a **Proposer** at any time.\n",
    "* All nodes are **Acceptors**.\n",
    "* Nodes that learn the final outcome are **Learners**. In practice, all nodes often play all three roles.\n",
    "\n",
    "### Steps\n",
    "Paxos works in two phases to decide on a single value.\n",
    "\n",
    "**Phase 1: Prepare/Promise (Electing a Leader)**\n",
    "1.  A **Proposer** decides it wants to lead. It picks a proposal number `n` that is unique and higher than any number it has used before. It sends a `Prepare(n)` message to a majority of Acceptors.\n",
    "2.  An **Acceptor** receives `Prepare(n)`.\n",
    "    * If `n` is higher than any proposal number it has promised to listen to before, it responds with a `Promise(n)` message. This is a promise to not accept any proposals with a number less than `n`.\n",
    "    * **Crucially**: If the Acceptor has *already accepted* a value `val_prev` from a previous proposal `n_prev`, it must include `(n_prev, val_prev)` in its `Promise` response.\n",
    "    * If `n` is not the highest it has seen, it ignores the message.\n",
    "\n",
    "**Phase 2: Accept/Accepted (Deciding on a Value)**\n",
    "3.  The **Proposer** waits for `Promise` responses. If it receives them from a **majority** of Acceptors, it is now the leader for proposal `n`. It then chooses a value `val` to propose.\n",
    "    * **The Rule**: If any of the `Promise` responses it received contained a previously accepted value, the Proposer **must** choose the value `val_prev` associated with the highest proposal number `n_prev` it saw. Otherwise, it is free to propose its own initial value.\n",
    "    * It then sends an `Accept(n, val)` message to a majority of Acceptors.\n",
    "4.  An **Acceptor** receives `Accept(n, val)`.\n",
    "    * If it has not made a newer promise (to a proposal number higher than `n`), it accepts the value and sends an `Accepted(n, val)` message to all nodes (who act as Learners).\n",
    "    * Once a Learner sees `Accepted` messages from a majority of nodes for the same value, that value is **decided**.\n",
    "\n",
    "---\n",
    "\n",
    "## The proof of paxos\n",
    "\n",
    "We will not go through the formal proof, as it involves a very large and detailed analysis of all possible message orderings and failure scenarios. The key safety property relies on the rule in Step 3: forcing a new leader to continue with a value that might have already been decided ensures that once a value is chosen, it can never be changed.\n",
    "\n",
    "However, Paxos can fail to terminate. This does not violate its safety guarantee, but it does violate the **Termination** requirement for consensus.\n",
    "\n",
    "* **Practical Example of Non-Termination (Dueling Proposers)**:\n",
    "    1.  Proposer P1 sends `Prepare(n=10)` and gets promises from a majority of Acceptors (A, B, C). P1 is now leader.\n",
    "    2.  Before P1 can send its `Accept` message, another Proposer P2 wakes up, chooses a higher number, and sends `Prepare(n=11)` to the same Acceptors.\n",
    "    3.  Acceptors A, B, and C see this higher proposal number. They respond to P2 with `Promise(n=11)`, and will now ignore any messages related to `n=10`.\n",
    "    4.  P1 finally sends its `Accept(n=10, value=\"X\")` message, but it is ignored by the majority because they've promised to listen to `n=11`. P1's proposal fails.\n",
    "    5.  Now P2 has a majority of promises and is the leader. But before it can send its `Accept` message, P1 realizes it failed, chooses an even higher number, and sends `Prepare(n=12)`.\n",
    "    6.  This cycle can repeat indefinitely, with the two proposers constantly preempting each other, and no value is ever decided. This is a \"livelock\" situation. In practice, randomized timeouts are used to make this scenario highly unlikely.\n",
    "\n",
    "---\n",
    "\n",
    "## Resources and alternatives\n",
    "* **Google TechTalk on Paxos**: A good video resource for understanding the algorithm in more detail.\n",
    "* **Raft Algorithm Illustration**: [https://raft.github.io/](https://raft.github.io/) provides an excellent interactive visualization of Raft, an alternative to Paxos designed for understandability.\n",
    "\n",
    "---\n",
    "\n",
    "## Heartbeat for Synchronized Systems\n",
    "A **heartbeat** is a common mechanism for failure detection in systems that are not fully asynchronous.\n",
    "* **How it works**:\n",
    "    1.  You guess a reasonable upper bound for message delay, `D`.\n",
    "    2.  Each process sends a \"beat\" message to others every `T` seconds.\n",
    "    3.  If a process hasn't received a beat from another process in the last `T + D` seconds, it **suspects** that the process has crashed.\n",
    "\n",
    "* **The Trade-off**:\n",
    "    * If `D` is **too small**, you get **inaccurate** detections. A slow but perfectly alive process might be declared dead.\n",
    "    * If `D` is **too large**, your detection is **incomplete**. A dead process might be considered alive for a long time (a \"zombie\").\n",
    "\n",
    "This shows we can only ever **suspect** a crash in a distributed system; we can never be 100% certain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948eb7f3",
   "metadata": {},
   "source": [
    "# LECTURE 23/09/2025\n",
    "\n",
    "## Mutual Exclusions\n",
    "\n",
    "What is a mutual exclusion (mutex)? \n",
    "A mutual exclusion (mutex) is a synchronization primitive that prevents multiple processes from concurrently accessing a **critical section** or a **shared resource**. It ensures that when one process is executing code within the critical section, no other process can enter it, guaranteeing exclusive access.\n",
    "\n",
    "### Examples\n",
    "‚ñ∂ **Printing:** A print spooler must grant access to the printer to only one document at a time to prevent the pages of different documents from being interleaved.\n",
    "‚ñ∂ **Using Coffee Machine:** Only one person can brew a coffee at a time. The machine is a shared resource, and the process of making coffee is the critical section.\n",
    "‚ñ∂ **Writing a file:** If two processes write to the same file simultaneously, the data can become corrupted. A mutex ensures one process finishes its write operation before another can begin.\n",
    "‚ñ∂ **Changing the state of an actuator:** An actuator, like a robotic arm, can only receive one command at a time to move to a specific position. Conflicting commands could cause damage or unpredictable behavior.\n",
    "‚ñ∂ **Wireless/Wired Communication:** Processes need exclusive access to the communication channel (e.g., a specific frequency or Ethernet cable) to transmit a packet of data without causing collisions and data corruption.\n",
    "\n",
    "---\n",
    "\n",
    "## System model\n",
    "What is a (computer science) process?\n",
    "A process is an instance of a running computer program. In distributed systems, we can formally model a process `p` as a state machine. The tuple `p = (S, s‚ÇÄ, M, ‚Üí)` consists of:\n",
    "‚ñ∂ a set of states **S**, representing all possible conditions the process can be in.\n",
    "‚ñ∂ an initial state **s‚ÇÄ ‚àà S**, where the process begins.\n",
    "‚ñ∂ a set of messages **M** that it can send or receive, including the empty message `œµ` for internal state changes.\n",
    "‚ñ∂ and a transition function **‚Üí**, which defines how a process changes its state `s ‚àà S` upon receiving a message `m ‚àà M`, and what messages it sends to other processes as a result.\n",
    "\n",
    "---\n",
    "\n",
    "## Type of communication\n",
    "Mutex algorithms can be designed for different underlying system architectures.\n",
    "‚ñ∂ **Message Passing:** Processes are independent and communicate by sending and receiving messages over a network. They do not share memory. Exclusive access is coordinated through explicit communication.\n",
    "    * **Example:** A client-server application where clients send requests to a server to access a database.\n",
    "‚ñ∂ **Shared Memory:** Processes have access to a common area of memory. They can communicate and synchronize by reading and writing to shared variables. This is more common in multi-threaded applications on a single machine.\n",
    "    * **Example:** Two threads updating a shared counter variable. They need a mutex to ensure the update operation (`read-increment-write`) is atomic.\n",
    "‚ñ∂ We will start with message passing.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "‚ñ∂ **Process Failures:** Processes can fail by **crashing**. Once a process crashes, it stops executing and does not recover or send any more messages (it stays dead).\n",
    "‚ñ∂ **Direct Communication:** Processes can send messages directly to any other process. We don't need to worry about routing or message forwarding.\n",
    "‚ñ∂ **Reliable Communication:** The communication channels are reliable, meaning messages are not lost, corrupted, or duplicated.\n",
    "    ‚ñ∂ **Synchronous:** There's a known upper bound on the time it takes for a message to be delivered. This makes it easier to detect failures.\n",
    "    ‚ñ∂ **Asynchronous:** A message will eventually be delivered, but there's no guarantee on how long it will take. The underlying protocol handles re-transmissions for reliability.\n",
    "‚ñ∂ **Network Partitions:** We assume that if the network splits (partitions), it will eventually heal, and processes will be able to communicate again.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements (Mutex Algorithms)\n",
    "\n",
    "1.  **Safety (Correctness):** At most one process can be in its critical section at any given time. This is the fundamental \"mutual exclusion\" property. Violating this leads to race conditions and corrupted data.\n",
    "2.  **Liveness (Progress):** Every request to enter the critical section is eventually granted. This ensures the system does not grind to a halt. It prevents **deadlock** (where processes are stuck waiting for each other) and **starvation** (where a process is indefinitely denied access).\n",
    "3.  **Ordering/Fairness:** If one request to enter the critical section happens-before another, access should be granted in that same order. This is typically based on logical clocks (like Lamport timestamps) and ensures fairness, preventing a process that requested first from being overtaken by later requests.\n",
    "\n",
    "---\n",
    "\n",
    "## Properties (Mutex Algorithms)\n",
    "When evaluating different mutex algorithms, we consider these key metrics:\n",
    "‚ñ∂ **Fault tolerance:** How does the algorithm behave if one or more processes crash? Can the system continue to function, or does it deadlock?\n",
    "‚ñ∂ **Performance:**\n",
    "    ‚ñ∂ **Message Complexity:** The number of messages required per entry into the critical section. This is a primary measure of network overhead.\n",
    "    ‚ñ∂ **Client Delay:** The time a process has to wait from the moment it requests entry into the critical section until it is granted access.\n",
    "    ‚ñ∂ **Synchronization Delay:** The time between one process exiting the critical section and the next process being granted entry. This is a good measure of system throughput.\n",
    "    ‚ñ∂ **Bandwidth:** The total amount of data transmitted, which is proportional to the number and size of messages sent for each critical section entry and exit.\n",
    "\n",
    "---\n",
    "\n",
    "## Centralized Algorithm - With a Token\n",
    "This is the most straightforward approach, mimicking a real-world queue manager. \n",
    "‚ñ∂ A single, designated **coordinator** process manages access to the resource.\n",
    "‚ñ∂ The coordinator holds a **token**. A process can only enter the critical section if it possesses this token.\n",
    "‚ñ∂ The coordinator maintains a FIFO (First-In, First-Out) queue of requests.\n",
    "\n",
    "A process that wants to enter the critical section:\n",
    "1.  Sends a `REQUEST` message to the coordinator.\n",
    "2.  Waits until it receives a `GRANT` message (the token) from the coordinator.\n",
    "3.  Enters the critical section, does its work, and exits.\n",
    "4.  Sends a `RELEASE` message (returns the token) to the coordinator.\n",
    "\n",
    "The coordinator's logic:\n",
    "1.  When it receives a `REQUEST`, it checks if the token is available.\n",
    "2.  If the token is available, it sends `GRANT` to the requesting process.\n",
    "3.  If the token is in use, it adds the request to its queue.\n",
    "4.  When it receives a `RELEASE`, it takes the next process from the queue (if any) and sends it the `GRANT` message.\n",
    "\n",
    "### Properties (Centralized Algorithm)\n",
    "\n",
    "**Requirements**\n",
    "‚ñ∂ **Safety:** Yes. The coordinator only gives the token to one process at a time.\n",
    "‚ñ∂ **Liveness:** Yes. As long as the coordinator doesn't crash, every request is queued and will eventually be served.\n",
    "‚ñ∂ **Ordering:** Yes, FIFO ordering is provided by the coordinator's queue.\n",
    "\n",
    "**Properties**\n",
    "‚ñ∂ **Message Complexity:** 3 messages per CS entry (`REQUEST`, `GRANT`, `RELEASE`).\n",
    "‚ñ∂ **Client Delay:** 1 round trip time (`REQUEST` + `GRANT`).\n",
    "‚ñ∂ **Synchronization Delay:** 1 round trip time (`RELEASE` + `GRANT` to the next process).\n",
    "‚ñ∂ **Fault Tolerance:** Poor. If the **coordinator crashes**, the entire system halts. This is a single point of failure. If a process crashes while holding the token, the system also starves unless the coordinator has a timeout mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "## Token Ring Algorithm (no leader)\n",
    "This algorithm organizes processes into a logical ring, where each process knows its successor. üíç\n",
    "‚ñ∂ A single **token** circulates continuously around the ring.\n",
    "‚ñ∂ A process `pi` waits for the token to arrive from its predecessor.\n",
    "‚ñ∂ When `pi` receives the token:\n",
    "    * If `pi` needs to enter the critical section, it holds the token, enters the CS, and upon exit, passes the token to its successor.\n",
    "    * If `pi` does not need to enter the critical section, it immediately passes the token to its successor.\n",
    "\n",
    "### Properties\n",
    "**Requirements**\n",
    "‚ñ∂ **Safety:** Yes. Only the process holding the token can enter the critical section.\n",
    "‚ñ∂ **Liveness:** Yes. The token continuously circulates, so every process will eventually get a chance to enter.\n",
    "‚ñ∂ **Ordering:** No. Access order is determined by the process's position in the ring, not by the time of the request.\n",
    "\n",
    "**Properties**\n",
    "‚ñ∂ **Client Delay:** Varies. Best case is 0 (token arrives just as it's needed). Worst case is the time it takes for the token to make a full circle, which involves `n` messages.\n",
    "‚ñ∂ **Message Complexity:** Between 1 (if every process wants to enter) and infinity (if no process wants to enter, the token just circulates endlessly).\n",
    "‚ñ∂ **Synchronization Delay:** Between 1 and `n` message hops.\n",
    "‚ñ∂ **Fault Tolerance:** Poor. If a process crashes, the ring is broken, and the token can be lost. If the token itself is lost (e.g., due to a message drop in an unreliable network), the system deadlocks. Detecting these failures is complex.\n",
    "\n",
    "---\n",
    "\n",
    "## Ricart and Agrawala‚Äôs Algorithm\n",
    "This is a fully distributed algorithm where processes achieve consensus to grant access. It uses timestamps to create a total ordering of requests. üïí\n",
    "‚ñ∂ **Core Idea:** A process that wants to enter the critical section must get permission from every other process. \"He who asks first, gets served first.\"\n",
    "‚ñ∂ **Secret Ingredient:** **Lamport Clocks** are used to assign a unique, ordered timestamp (`ts`, `pid`) to every `REQUEST` message. This breaks ties and ensures all processes agree on the request order.\n",
    "\n",
    "### Lamport Clocks (reminder)\n",
    "‚ñ∂ Each process `pi` maintains a local logical clock `Ci`.\n",
    "‚ñ∂ Before sending a message, `pi` increments `Ci`. The message is timestamped with `Ci`.\n",
    "‚ñ∂ When a process `pj` receives a message with timestamp `T`, it updates its own clock: `Cj = max(Cj, T) + 1`. This ensures causality is captured.\n",
    "\n",
    "### Algorithm\n",
    "When a process `pi` wants to enter the critical section:\n",
    "1.  It sets its state to `WANT`.\n",
    "2.  It creates a request with the current Lamport timestamp `(ts, i)` and multicasts a `REQUEST` message to all other processes.\n",
    "3.  It waits for a `REPLY` from every other process. Once all replies are received, it enters the critical section (state = `USE`).\n",
    "\n",
    "When a process `pj` receives a `REQUEST(ts, i)` from `pi`:\n",
    "1.  If `pj` is in state `USE` (in the CS), it defers its reply.\n",
    "2.  If `pj` is in state `WANT`, it compares the timestamp of its own request `(ts', j)` with the incoming request `(ts, i)`. If `(ts, i)` is smaller (happened earlier), it sends a `REPLY`. Otherwise, it defers the reply.\n",
    "3.  If `pj` is in state `FREE`, it sends a `REPLY` immediately.\n",
    "\n",
    "After exiting the critical section, `pi` changes its state to `FREE` and sends a `REPLY` to all deferred requests.\n",
    "\n",
    "### Properties (Ricart and Agrawala)\n",
    "**Requirements**\n",
    "‚ñ∂ **Safety:** Yes. A process `pi` can only enter if it has received a `REPLY` from all others. Another process `pj` will not send its `REPLY` to `pi` if `pj` has an earlier request or is already in the CS.\n",
    "‚ñ∂ **Liveness:** Yes. No deadlock because request timestamps provide a total ordering.\n",
    "‚ñ∂ **Ordering:** Yes, by Lamport timestamp.\n",
    "\n",
    "**Properties**\n",
    "‚ñ∂ **Message Complexity:** `2(n-1)` messages per CS entry. This consists of `n-1` `REQUEST` messages and `n-1` `REPLY` messages.\n",
    "‚ñ∂ **Synchronization Delay:** One message propagation time.\n",
    "‚ñ∂ **Fault Tolerance:** Poor. If any process crashes, it will not send its `REPLY`, causing all other requesting processes to block forever.\n",
    "\n",
    "---\n",
    "\n",
    "## Maekawa's Algorithm\n",
    "This algorithm optimizes Ricart & Agrawala by requiring a process to get permission from only a subset of other processes, called a **voting set** or **quorum**. üó≥Ô∏è\n",
    "‚ñ∂ **Core Idea:** Instead of asking everyone, ask a cleverly chosen subset of processes. The subsets are designed to overlap, ensuring that only one process can get a majority \"vote\" at a time.\n",
    "\n",
    "### Voting Set\n",
    "A voting set `Vi` for a process `pi` must satisfy two conditions:\n",
    "1.  `pi ‚àà Vi` (A process is always in its own voting set).\n",
    "2.  `‚àÄ i, j : Vi ‚à© Vj ‚â† ‚àÖ` (Any two voting sets must have at least one common member).\n",
    "\n",
    "This intersection property is key to safety: the common member acts as an arbiter who will only grant permission to one of the two competing processes at a time. To minimize message complexity, we want the size of the voting sets, `K = |Vi|`, to be as small as possible. This is achieved when `K ‚âà ‚àön`, leading to `|Vi| ‚âà ‚àön`.\n",
    "\n",
    "A common way to construct these sets is to arrange the `n` processes in a `‚àön x ‚àön` grid. The voting set for a process `pi` then consists of all processes in the same row and column as `pi`.\n",
    "\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "Similar to Ricart & Agrawala, but communication is limited to the voting set `Vi`.\n",
    "1.  **Request:** To enter the CS, `pi` sends `REQUEST` to every process in its voting set `Vi`.\n",
    "2.  **Wait:** `pi` waits for a `GRANT` message from every process in `Vi`.\n",
    "3.  **Enter CS:** Once all grants are received, `pi` enters the CS.\n",
    "4.  **Release:** After exiting, `pi` sends a `RELEASE` to every process in `Vi`.\n",
    "\n",
    "Each process `pj` will only send a `GRANT` to one request at a time. It keeps other requests queued.\n",
    "\n",
    "### Properties (Maekawa's Algorithm)\n",
    "\n",
    "**Requirements**\n",
    "‚ñ∂ **Safety:** Yes, guaranteed by the non-empty intersection of voting sets.\n",
    "‚ñ∂ **Liveness:** No. This algorithm is prone to deadlock. For example, `p1` might be waiting for a grant from `p2`, while `p2` is waiting for a grant from `p3`, who is waiting for a grant from `p1`.\n",
    "‚ñ∂ **Ordering:** No, not inherently.\n",
    "\n",
    "**Properties**\n",
    "‚ñ∂ **Message Complexity:** `3‚àön` (`‚àön` REQUEST, `‚àön` GRANT, `‚àön` RELEASE). This is a significant improvement over `2(n-1)`.\n",
    "‚ñ∂ **Synchronization Delay:** 2 message propagation times (`RELEASE` then `GRANT`).\n",
    "‚ñ∂ **Fault Tolerance:** Poor. If a process in a voting set crashes, any process that needs its vote will starve.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview of Mutex Algorithms\n",
    "| Algorithm | Messages per Entry | Synchronization Delay | Key Problem(s) |\n",
    "| :--- | :---: | :---: | :--- |\n",
    "| **Centralized** | 3 | 2 message delays | Coordinator crash (single point of failure) |\n",
    "| **Token Ring** | 1 to ‚àû | 1 to `n` delays | Lost token, process crash breaks the ring |\n",
    "| **Ricart & Agrawala** | `2(n-1)` | 1 message delay | Crash of any process blocks the system |\n",
    "| **Maekawa** | `3‚àön` | 2 message delays | Prone to deadlock, crash in voting set |\n",
    "\n",
    "---\n",
    "\n",
    "## Mutual exclusion on shared memory\n",
    "In this model, processes run on the same machine (e.g., as threads) and can read/write to common memory locations. Synchronization is achieved by manipulating shared variables. Operations like `read` and `write` to a single memory location are assumed to be **atomic**.\n",
    "\n",
    "A classic example for two processes is **Peterson's Algorithm**.\n",
    "‚ñ∂ **Shared Variables:**\n",
    "    ‚ñ∂ `flags: array[2] of boolean` (initialized to `false`)\n",
    "    ‚ñ∂ `turn: integer`\n",
    "‚ñ∂ **Entry Protocol for Process `i` (where `j` is the other process):**\n",
    "    ```\n",
    "    1: flags[i] = true;         // I want to enter\n",
    "    2: turn = j;                // I yield to the other process\n",
    "    3. while (flags[j] && turn == j) {\n",
    "    4:   // busy-wait\n",
    "    5: }\n",
    "    ```\n",
    "‚ñ∂ **Exit Protocol for Process `i`:**\n",
    "    ```\n",
    "    6: flags[i] = false;        // I am done\n",
    "    ```\n",
    "**How it works:** Process `i` signals its intent by raising its flag. It then gives the other process `j` priority by setting `turn = j`. It will only enter the critical section if `j` does not want to enter (`flags[j]` is false) OR if it's `j`'s turn but `j` has yielded (`turn != j`, i.e., `turn == i`).\n",
    "\n",
    "### Requirements\n",
    "‚ñ∂ **Safe:** Yes.\n",
    "‚ñ∂ **Liveness:** Yes. A process is only blocked if the other is in the CS. No deadlock.\n",
    "‚ñ∂ **Ordering:** Not strictly, but it is starvation-free (fair).\n",
    "\n",
    "### Fault Tolerance\n",
    "Performance is measured in memory accesses, not messages. However, these algorithms depend heavily on the memory consistency model provided by the hardware. They work correctly under **Sequential Consistency**.\n",
    "\n",
    "---\n",
    "\n",
    "## Dekker's algorithm\n",
    "Dekker's algorithm solves mutual exclusion for **two processes** using shared memory. It uses flags to show intent and a turn variable to break ties, ensuring one process eventually gets priority.\n",
    "\n",
    "### ## How It Works\n",
    "\n",
    "* **Variables (Shared):**\n",
    "    * `flags[2]`: A boolean array, initially `false`. `flags[i] = true` means process `i` wants to enter.\n",
    "    * `turn`: An integer (`0` or `1`) indicating whose turn it is if both want to enter.\n",
    "\n",
    "* **Protocol (Process `i`):**\n",
    "    1.  **Enter:** Set `flags[i] = true`. While the other process (`j`) also has its flag up, politely wait if it's their `turn`.\n",
    "    2.  **Critical Section:** Once inside, perform the critical work.\n",
    "    3.  **Exit:** Give the `turn` to process `j` and set `flags[i] = false`.\n",
    "\n",
    "### ## Properties\n",
    "\n",
    "* **Safety:** Yes (guarantees mutual exclusion).\n",
    "* **Liveness:** Yes (no deadlock or starvation).\n",
    "* **Ordering:** No (but it is fair).\n",
    "* **Fault Tolerance:** None. If a process fails in the critical section, the other process will be blocked forever.\n",
    "\n",
    "---\n",
    "\n",
    "## Sequential consistency: implementation\n",
    "This model assumes:\n",
    "‚ñ∂ **Atomic writes:** A write operation appears to happen instantaneously to all processes.\n",
    "‚ñ∂ **Read-from-memory:** A read operation gets the value from the last completed write.\n",
    "\n",
    "![img](../images/Screenshot%202025-09-23%20at%2011.07.48.png)\n",
    "\n",
    "Sequential consistency is a model that defines how these concurrent operations behave. It ensures that all processes see the same single, interleaved order of all read and write operations, as if the operations were executed one after another on a single processor.\n",
    "\n",
    "---\n",
    "\n",
    "## Sequential Consistency: formally\n",
    "**Sequential Consistency (SC)** is a memory model where the result of any execution is the same as if all processes' operations were executed in some single sequential order, and the operations of each individual process appear in this sequence in the order specified by its program.\n",
    "\n",
    "Essentially, you can imagine all operations from all processes being put into a single timeline (interleaved), but the local order for each process is preserved.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Weak Memory Models\n",
    "Modern CPUs often reorder memory operations for performance, leading to memory models that are \"weaker\" than SC.\n",
    "**Rule of thumb:** Any behavior not allowed by Sequential Consistency is the result of a weak memory model.\n",
    "\n",
    "\n",
    "![img](../images/Screenshot%202025-09-23%20at%2011.07.03.png)\n",
    "The key feature is the addition of a private store buffer for each process. When a process like P1‚Äã executes a write instruction (e.g., x = 1), the data is not sent directly to the main shared memory. Instead, it's placed in its local store buffer first and this creates a delay.\n",
    "\n",
    "### An example: total store order (TSO)\n",
    "TSO is a common weak model where each process has a **write buffer**.\n",
    "‚ñ∂ **Non-atomic writes:** A write is first placed in a local buffer and committed to main memory later.\n",
    "‚ñ∂ **Read locally or from memory:** A process can read its own writes from its buffer before they are visible to others.\n",
    "\n",
    "This means a process `p1` might execute a write, but another process `p2` won't see that new value until it's flushed from `p1`'s buffer to main memory.\n",
    "\n",
    "![img](../images/Screenshot%202025-09-23%20at%2011.07.03.png)\n",
    "\n",
    "Process P1‚Äã's writes (x:=1, x:=2) are queued in its private store buffer, while main memory still holds the old value x=0.\n",
    "\n",
    "Due to store forwarding (\"read-your-own-writes\"), P1‚Äã reads its own latest update for x (which is 2) directly from this buffer. In contrast, it reads other variables like y from main memory. This mechanism can produce results that would be impossible under a stricter Sequential Consistency model.\n",
    "\n",
    "\n",
    "### Peterson's Algorithm under TSO\n",
    "\n",
    "**Main issue:** The processor might reorder independent instructions. In Peterson's algorithm:\n",
    "`flags[i] = true;`\n",
    "`turn = j;`\n",
    "A TSO model could reorder these writes. Even worse, it might reorder the write to `flags[i]` with the read of `flags[j]` in the `while` loop. If both processes read the old `false` value for the other's flag before their own `true` value becomes visible to the other, they could both enter the critical section, violating safety!\n",
    "\n",
    "**How to fix this?**\n",
    "We need to insert **memory fences** (or barriers). A fence is an instruction that forces the CPU to commit all pending writes to memory and/or wait for all reads to complete before proceeding. This enforces the intended order at critical points in the algorithm, restoring correctness at a slight performance cost.\n",
    "```markdown\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95896e72",
   "metadata": {},
   "source": [
    "# LECTURE 06/10/2025\n",
    "\n",
    "## Replication and Election\n",
    "\n",
    "Replication involves maintaining copies of data on multiple machines (replicas) to achieve specific system goals. This lecture also covers how a group of processes can agree on a single leader.\n",
    "\n",
    "---\n",
    "\n",
    "## Goals of replication\n",
    "\n",
    "Replication is a fundamental technique in distributed systems for creating robust and scalable services.\n",
    "\n",
    "‚ñ∂ **Fault Tolerance**: The system continues to operate correctly even if some of its replicas fail. This is ideally **transparent** to the user, who doesn't notice the failure. It can tolerate both node and network failures.\n",
    "\n",
    "‚ñ∂ **High Availability**: By having multiple copies, the service can remain accessible and responsive most of the time. If one replica is down, requests can be served by another, minimizing downtime.\n",
    "\n",
    "‚ñ∂ **Performance**: Replication can improve performance by placing data closer to users (reducing latency) and by distributing the workload across multiple machines, overcoming the scaling limits of a single server (**vertical scaling**).\n",
    "\n",
    "**Caching** is a common form of replication. Examples include:\n",
    "‚ñ∂ Your browser caching website assets locally.\n",
    "\n",
    "‚ñ∂ Netflix prefetching video segments to a server near you.\n",
    "\n",
    "‚ñ∂ The DNS system replicating domain name records across the world.\n",
    "\n",
    "---\n",
    "\n",
    "## Problems\n",
    "\n",
    "While powerful, replication introduces significant challenges:\n",
    "\n",
    "‚ñ∂ **Consistency**: How do we ensure that all replicas have the same data, or at least a consistent view of it? If a client writes to one replica, how and when do other replicas see that change? This is the core problem of consensus.\n",
    "\n",
    "‚ñ∂ **Overhead**: Keeping replicas in sync requires communication. This adds network traffic and processing overhead.\n",
    "\n",
    "‚ñ∂ **Failure Handling**: How does the system detect that a replica has crashed? Once detected, how does it manage failover to another replica and reintegrate the failed replica once it recovers?\n",
    "\n",
    "---\n",
    "\n",
    "## CAP theorem\n",
    "\n",
    "The CAP theorem, also known as Brewer's theorem, presents a fundamental trade-off in distributed system design.\n",
    "\n",
    "‚ñ∂ **Consistency**: Every read operation receives the most recent write or an error. All nodes have the same data at the same time. Think of it like a bank account balance that is identical no matter which ATM you use.\n",
    "\n",
    "‚ñ∂ **Availability**: Every request receives a (non-error) response, without the guarantee that it contains the most recent write. Your bank account is always accessible, even if the balance shown is slightly out of date.\n",
    "\n",
    "‚ñ∂ **Partition Tolerance**: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes. A network failure between two data centers will not cause the entire service to fail.\n",
    "\n",
    "### Theorem\n",
    "\n",
    "It is **impossible** for a distributed system to simultaneously provide all three guarantees: **Consistency**, **Availability**, and **Partition Tolerance**.\n",
    "\n",
    "In the presence of a network partition (a realistic scenario in any distributed system), you must choose between consistency and availability.\n",
    "\n",
    "* To maintain **Consistency**, you must sacrifice Availability (e.g., stop accepting writes or reads until the partition heals).\n",
    "* To maintain **Availability**, you must sacrifice Consistency (e.g., allow operations on both sides of the partition, which may lead to conflicting data that needs to be reconciled later).\n",
    "\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "The choice between C, A, and P depends entirely on the application's requirements.\n",
    "\n",
    "‚ñ∂ **CP Systems (Consistency & Partition Tolerance)**: These systems choose consistency over availability during a partition. They are essential where data accuracy is non-negotiable.\n",
    "    * **Financial Sector**: Banking systems cannot tolerate inconsistent account balances.\n",
    "    * **Scientific Computing**: Large-scale simulations (e.g., weather forecasting) require a consistent state.\n",
    "\n",
    "‚ñ∂ **AP Systems (Availability & Partition Tolerance)**: These systems choose availability, accepting that data might be temporarily inconsistent across replicas.\n",
    "    * **Social Networks**: It's more important for users to be able to post content than for every other user to see it instantly. Eventual consistency is acceptable.\n",
    "    * **Search Engines**: An index might be slightly out of date, but the search service must always be available.\n",
    "\n",
    "‚ñ∂ **CA Systems (Consistency & Availability)**: These systems cannot tolerate partitions. This model is effectively limited to single-node systems (like a traditional single-server database), as any system with a network is subject to partitioning.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "For the following replication models, we assume:\n",
    "\n",
    "‚ñ∂ **Asynchronous System**: There are no bounds on message delivery time or process execution speed.\n",
    "\n",
    "‚ñ∂ **Reliable Communication**: Messages are eventually delivered, but not necessarily in order.\n",
    "\n",
    "‚ñ∂ **Crash-fail Model**: Processes fail by crashing (stopping completely) and do not send malicious or incorrect data (i.e., no Byzantine failures unless stated otherwise).\n",
    "\n",
    "‚ñ∂ **Atomic Operations**: Operations are all-or-nothing.\n",
    "\n",
    "‚ñ∂ **Deterministic Objects**: Objects behave as \"state machines.\" Their output depends solely on the sequence of operations applied, not on random chance, timers, or external events.\n",
    "    * *Notation*: `o.m(v)` means applying the modifier `m` with value `v` to object `o`. Example: `myAccount.deposit(1000)`.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The ideal replicated system should meet these criteria:\n",
    "\n",
    "‚ñ∂ **Transparency**: The user interacts with the service as if it were a single, non-replicated entity.\n",
    "\n",
    "‚ñ∂ **Consistency**: The state of replicated objects remains consistent across all replicas.\n",
    "\n",
    "The ultimate goal is a system that is **indistinguishable from a single, correct, and highly available copy**.\n",
    "\n",
    "---\n",
    "\n",
    "## Operations\n",
    "\n",
    "A generalized workflow for a replicated operation involves five phases:\n",
    "\n",
    "1.  **Request**: A client sends a request to a frontend or directly to a replica.\n",
    "2.  **Coordination**: The replicas decide whether the operation can be applied immediately or must wait. They agree on a definitive order for operations.\n",
    "3.  **Execution**: Each replica executes the operation.\n",
    "4.  **Agreement**: The replicas communicate to reach a consensus on the outcome of the operation.\n",
    "5.  **Response**: One or more replicas send a response back to the client.\n",
    "\n",
    "---\n",
    "\n",
    "## Fault tolerance\n",
    "\n",
    "The primary goal here is to build an ***f*-resilient** system, meaning it can tolerate the failure of up to *f* replicas without any service interruption (**downtime**) or impact on the user (**transparency**).\n",
    "\n",
    "---\n",
    "\n",
    "## Consistency models\n",
    "\n",
    "Consistency models are rules that define what guarantees a system provides regarding the order and visibility of operations.\n",
    "\n",
    "‚ñ∂ **Strong Consistency**: Guarantees that once a write completes, any subsequent read from any replica will see that new value (or a newer one). This is the most intuitive model but often the hardest to implement efficiently.\n",
    "    * *Inconsistency Example*: A client writes `x=1` to replica A. Replica A crashes before it can inform replica B. Another client then reads from replica B and gets the old value `x=0`.\n",
    "\n",
    "‚ñ∂ **Weak Consistency**: Offers fewer guarantees. It doesn't ensure that subsequent reads will see the latest write. Systems prioritize availability and performance, accepting that replicas may be out of sync for a period.\n",
    "\n",
    "‚ñ∂ **Eventual Consistency**: A specific form of weak consistency. It guarantees that if no new updates are made, all replicas will *eventually* converge to the same value. It's a common model for highly available systems, but its drawback is that clients may temporarily read stale data. Typically, conflicts are resolved with a \"last write wins\" policy.\n",
    "\n",
    "### Desired Temporal Consistencies\n",
    "\n",
    "These are common client-centric consistency guarantees:\n",
    "\n",
    "‚ñ∂ **Read-your-writes**: If a process writes a value, a subsequent read by that *same process* will always see that value or a newer one.\n",
    "\n",
    "‚ñ∂ **Monotonic reads**: If a process reads a value, any subsequent read by that *same process* will see the same value or a newer one. It never sees an older value.\n",
    "\n",
    "‚ñ∂ **Causal consistency**: If operation A *happened-before* operation B (e.g., a user posts an answer to a question), then every process sees A before it sees B. Unrelated operations can be seen in different orders.\n",
    "\n",
    "---\n",
    "\n",
    "## Linearizability (Lamport)\n",
    "\n",
    "Linearizability is a **strong consistency** model. It provides the illusion that there is only a single copy of the data and that all operations on it are **atomic**.\n",
    "\n",
    "An execution history is **linearizable** if:\n",
    "1.  All operations can be reordered into a sequential history that is correct according to the object's specification (e.g., a queue's operations).\n",
    "2.  This sequential order **respects the real-time order** of non-overlapping operations. If operation A finishes before operation B begins, then A must appear before B in the sequential history.\n",
    "\n",
    "### Implementation & Drawbacks\n",
    "\n",
    "A common theoretical implementation approach highlights its difficulty:\n",
    "‚ñ∂ Synchronize hardware clocks across all machines.\n",
    "\n",
    "‚ñ∂ Guess a maximal network delay, `D`.\n",
    "\n",
    "‚ñ∂ When a request arrives, place it in a hold-back queue and wait for time `D` to pass to ensure no earlier message is still in transit.\n",
    "\n",
    "‚ñ∂ Process operations from the sorted queue.\n",
    "\n",
    "This is impractical because:\n",
    "‚ñ∂ **No perfect clock synchronization** algorithm exists for distributed systems.\n",
    "\n",
    "‚ñ∂ In an asynchronous system, there is **no guaranteed upper bound** on network delay `D`.\n",
    "\n",
    "---\n",
    "\n",
    "## Sequential Consistency (Lamport)\n",
    "\n",
    "Sequential consistency is a slightly weaker, but more practical, strong consistency model than linearizability.\n",
    "\n",
    "An execution history is **sequentially consistent** if:\n",
    "1.  The result of any execution is the same as if all operations were executed in *some* sequential order.\n",
    "2.  The operations of each individual process appear in this sequence in the order specified by its program (**program order**).\n",
    "\n",
    "The key difference from linearizability is that sequential consistency **does not have to respect real-time order** across different processes. An operation that finished earlier in real-time might appear later in the sequential history, as long as the program order for each process is maintained.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Replication Architectures for Fault Tolerance\n",
    "\n",
    "‚ñ∂ **Read-only replication**: Used for static or immutable content like files on a CDN. Simple to manage as data doesn't change.\n",
    "\n",
    "‚ñ∂ **Passive replication (Primary-Secondary)**: A primary replica handles all writes and propagates them to passive secondary replicas. Provides high consistency. Used in systems where data integrity is critical, like traditional databases.\n",
    "\n",
    "‚ñ∂ **Active Replication (State Machine Replication)**: All replicas are peers and process every request. Requires a total order for all incoming requests to ensure replicas remain in sync. Offers fast failover and can handle Byzantine failures.\n",
    "\n",
    "---\n",
    "\n",
    "## Passive Replication\n",
    "\n",
    "Also known as the Primary-Backup model.\n",
    "\n",
    "1.  **Request**: Client sends a write request to the **primary** replica.\n",
    "2.  **Coordination**: The primary decides the order of operations.\n",
    "3.  **Execution**: The primary executes the operation and updates its state.\n",
    "4.  **Agreement**: The primary sends the update to all **backup** replicas. It waits for acknowledgements (ACKs) from them.\n",
    "5.  **Response**: Once acknowledged by backups, the primary replies to the client.\n",
    "\n",
    "* **Pros**: Conceptually simple and provides **linearizability** (if reads also go through the primary).\n",
    "* **Cons**: The primary is a performance bottleneck and a single point of failure. **Failover** (electing a new primary) can be slow and complex. It does not tolerate a Byzantine primary.\n",
    "* **Optimization**: To improve read performance, reads can be offloaded to backups, but this sacrifices strong consistency for eventual consistency.\n",
    "\n",
    "---\n",
    "\n",
    "## Active Replication\n",
    "\n",
    "All replicas are equal and process requests concurrently.\n",
    "\n",
    "1.  **Request**: A frontend multicasts the client's request to all replicas using a **totally-ordered, reliable multicast**.\n",
    "2.  **Coordination**: The total-order protocol delivers requests to all replicas in the same, deterministic sequence.\n",
    "3.  **Execution**: Each replica executes the request as it is delivered. Since they all start in the same state and execute the same operations in the same order, they remain in sync.\n",
    "4.  **Agreement**: Implicit in the ordered multicast; no separate agreement phase is needed.\n",
    "5.  **Response**: Any replica can respond to the client. For Byzantine fault tolerance, the client may wait for `f+1` or `(n/2)+1` identical responses.\n",
    "\n",
    "* **Pros**: **Fast failover** (just remove the crashed replica from the group). Excellent for load distribution on reads. Can handle Byzantine failures. Provides **sequential consistency**.\n",
    "* **Cons**: The required **total-order multicast** is complex and expensive to implement, especially in asynchronous systems where it's theoretically impossible.\n",
    "\n",
    "---\n",
    "\n",
    "## Availability\n",
    "\n",
    "This section shifts focus from consistency-first fault tolerance to systems where **availability is the primary goal**. We are willing to relax consistency guarantees for higher uptime and faster responses. This is the philosophy behind most large-scale web services (e.g., social media, e-commerce).\n",
    "\n",
    "---\n",
    "\n",
    "## Gossip Architecture\n",
    "\n",
    "Gossip (or Epidemic) protocols are a popular method for building highly available systems with **eventual consistency**. Replicas periodically exchange information with random peers to spread updates throughout the system, much like a rumor spreads through a crowd.\n",
    "\n",
    "### Operations\n",
    "\n",
    "* **Reads**: Can be served by any replica, but might return stale data.\n",
    "* **Writes (Updates)**: Sent to one or more replicas and then propagated via gossip.\n",
    "\n",
    "### Relaxed Consistency\n",
    "\n",
    "* Clients may read outdated data.\n",
    "* However, guarantees like **causal consistency** are often provided to ensure a sensible user experience (e.g., you always see your own updates). Vector clocks are the key mechanism for this.\n",
    "\n",
    "### The Idea: Vector Clocks Everywhere\n",
    "\n",
    "* Each replica manager `R_i` maintains a **vector clock** that tracks the latest update it has seen from every other replica.\n",
    "* When a frontend `F_j` sends an update, it tags it with its own vector clock (`prev`), representing the state of the world it last knew.\n",
    "* A replica manager uses these timestamps to order incoming updates, delay updates from the \"future\" (whose prerequisites haven't been seen yet), and avoid applying duplicate updates.\n",
    "\n",
    "### Phases\n",
    "\n",
    "1.  **Request**: A frontend sends the client's request (tagged with its timestamp) to one or more replica managers.\n",
    "2.  **Coordination**: The replica manager queues the request until its timestamp's dependencies are met (i.e., all causally preceding updates have been applied).\n",
    "3.  **Execution**: The operation is applied in the correct causal order.\n",
    "4.  **Agreement**: Updates are spread lazily to other replicas through periodic **gossip** messages. If a replica detects a gap in its history, it can request the missing data.\n",
    "5.  **Response**:\n",
    "    * **Writes** can be acknowledged immediately for low latency.\n",
    "    * **Reads** may need to wait until the replica's state is at least as new as the timestamp provided by the client's frontend.\n",
    "\n",
    "### Frontend View\n",
    "\n",
    "The frontend `F` maintains a vector timestamp `prev` summarizing the latest updates it is aware of.\n",
    "1.  On a client operation `o`, it sends the pair `(o, prev)` to a replica `R_i`.\n",
    "2.  It waits for a response from `R_i`, which will include an updated timestamp.\n",
    "3.  It merges the new timestamp with its own `prev`. This `prev` state can also be updated via gossip from other frontends.\n",
    "\n",
    "### Replica Manager View\n",
    "\n",
    "#### Read Logic\n",
    "A replica manager has a current value `v` with its associated timestamp `vts`.\n",
    "1.  It receives a read request `(o, prev)` from a frontend.\n",
    "2.  If `prev <= vts` (the replica's data is at least as new as what the frontend has seen), it can **return `(v, vts)` instantly**.\n",
    "3.  Otherwise, the replica must wait for gossip to deliver the necessary updates until the condition `prev <= vts` is met.\n",
    "\n",
    "#### Write Logic\n",
    "1.  Receives a write request `(v, id, prev)` from a frontend.\n",
    "2.  Assigns a new, unique timestamp to the update based on the `prev` timestamp and its own local clock.\n",
    "3.  Stores the update in a stable log and acknowledges the write to the frontend immediately.\n",
    "4.  The update is later applied locally once all its causal dependencies are met.\n",
    "5.  The update is propagated to other replicas during gossip rounds.\n",
    "\n",
    "---\n",
    "\n",
    "## Leader election\n",
    "\n",
    "Leader election is a procedure to dynamically choose one process to act as a coordinator or leader. This is critical in many algorithms, such as passive replication, where a new primary must be chosen if the old one fails.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Let `P` be the set of processes. The leader for process `p_i` is `L(p_i)`.\n",
    "\n",
    "1.  **Safety**: For any process `p_i`, either it has no leader (`L(p_i) = ‚ä•`) or its leader `L(p_i)` is the non-crashed process with the single largest identifier.\n",
    "2.  **Liveness**: Eventually, every non-crashed process `p_i` has a leader (`L(p_i) != ‚ä•`).\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* Processes have unique identifiers (IDs).\n",
    "* Processes can crash, but they stay dead.\n",
    "* The system can reliably detect crashes (often via timeout, which requires synchrony assumptions).\n",
    "\n",
    "---\n",
    "\n",
    "## Chang and Roberts Algorithm\n",
    "\n",
    "A simple and efficient election algorithm for systems where processes are arranged in a logical **ring**.\n",
    "\n",
    "### Idea\n",
    "‚ñ∂ A token containing a process ID is passed around the ring.\n",
    "\n",
    "‚ñ∂ When a process notices the leader has failed, it starts an election by sending an `election` message with its own ID to its neighbor.\n",
    "\n",
    "‚ñ∂ When a process receives an `election` message:\n",
    "    * If the ID in the message is **higher** than its own, it forwards the message unchanged.\n",
    "    * If the ID is **lower** and the process isn't already participating, it replaces the ID with its own and forwards it.\n",
    "    * If it receives its **own ID** back, it declares itself the winner and sends an `elected` message around the ring to announce the new leader.\n",
    "\n",
    "### Properties\n",
    "‚ñ∂ **Safe and Live** (if no failures occur during the election).\n",
    "\n",
    "‚ñ∂ Message Complexity: `3N - 1` messages in the worst case for one election.\n",
    "\n",
    "‚ñ∂ **Vulnerable to crashes**: If a process in the ring fails during the election, the token can be lost. Requires a reliable failure detection and ring-management layer to handle this.\n",
    "\n",
    "---\n",
    "\n",
    "## Bully Algorithm\n",
    "\n",
    "An election algorithm for synchronous systems where any process can communicate with any other.\n",
    "\n",
    "### Idea\n",
    "‚ñ∂ The process with the **highest ID** always wins. It \"bullies\" lower-ID processes into submission.\n",
    "\n",
    "‚ñ∂ When a process `P` detects the leader has failed, it starts an election:\n",
    "    1.  `P` sends an `ELECTION` message to all processes with a **higher ID**.\n",
    "    2.  `P` waits for a response.\n",
    "        * If it gets **no response** within a timeout, it assumes all higher-ID processes are down. `P` declares itself the leader and sends a `COORDINATOR` message to all other processes.\n",
    "        * If it **receives an `ANSWER`** from a higher-ID process, it gives up. Its job is done, as it knows a higher-ID process is taking over the election.\n",
    "\n",
    "‚ñ∂ When a process receives an `ELECTION` message from a lower-ID process, it responds with an `ANSWER` and starts its own election (if it's not already running one).\n",
    "\n",
    "### Properties\n",
    "‚ñ∂ **Safe & Live**, but relies heavily on:\n",
    "    * A **synchronous system** where timeouts can reliably detect crashes.\n",
    "    * Reliable failure detection.\n",
    "\n",
    "‚ñ∂ **Message Complexity**:\n",
    "    * Best-case: `N-2` messages (the second-highest ID process starts the election).\n",
    "    * Worst-case: `O(N^2)` messages (the lowest-ID process starts the election, triggering a cascade).\n",
    "\n",
    "‚ñ∂ **Safety is broken** if the timeout is too short (a slow process is wrongly assumed dead) or if the system is actually asynchronous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e3f02",
   "metadata": {},
   "source": [
    "# LECTURE 07/10/2025\n",
    "\n",
    "# LECTURE: Clustered Storage\n",
    "\n",
    "This lecture explores Google's clustered storage infrastructure, focusing on the Google File System (GFS), the Chubby lock service, and the Bigtable database system. It details their architecture, design choices, and performance characteristics, which are tailored for Google's large-scale data processing workloads.\n",
    "\n",
    "***\n",
    "\n",
    "## Google's Infrastructure and Workload\n",
    "\n",
    "Google's services, like its search engine, Gmail, and YouTube, run on a massive, distributed infrastructure designed to handle two main types of workloads:\n",
    "\n",
    "* **Offline Batch Jobs**: These involve processing petabytes of data with large, sequential reads and writes. Short outages are generally acceptable for tasks like web indexing and log processing.\n",
    "* **Online Applications**: These manage smaller datasets (terabytes) with frequent, small reads and writes. Low latency and high availability are critical, as any downtime directly impacts users of services like web search and Google Docs.\n",
    "\n",
    "This infrastructure is built on clusters of commodity hardware, where component failures are considered normal events. Google's philosophy is to achieve fault tolerance and high throughput through software techniques like replication and parallelism rather than relying on expensive, highly reliable hardware.\n",
    "\n",
    "### Key Infrastructure Components\n",
    "\n",
    "Google's infrastructure consists of several core components for data storage, coordination, and computation:\n",
    "\n",
    "* **GFS (Google File System)**: A distributed file system for storing massive datasets.\n",
    "* **Chubby**: A distributed lock service for coordination and master election.\n",
    "* **Bigtable**: A sparse, distributed, multi-dimensional sorted map built on top of GFS.\n",
    "* **MapReduce**: A framework for large-scale parallel data processing.\n",
    "\n",
    "***\n",
    "\n",
    "## Google File System (GFS)\n",
    "\n",
    "GFS is a distributed filesystem designed specifically for Google's application workload and infrastructure.\n",
    "\n",
    "### GFS Design Assumptions\n",
    "\n",
    "GFS is built on several key assumptions about its use case:\n",
    "\n",
    "* **Component failures are normal**: The system is built from thousands of commodity machines and must constantly monitor itself and recover from failures transparently.\n",
    "* **Files are huge**: Multi-gigabyte files are common, so I/O operations and block sizes are optimized for large datasets.\n",
    "* **Workloads are specific**: The primary operations are **large sequential reads** and **record appends** (concurrent appends to the same file). High sustained throughput is more important than low latency.\n",
    "* **Non-POSIX API**: GFS provides a specialized API that includes operations like `snapshot` and `record append` to better serve its applications.\n",
    "\n",
    "### GFS Architecture\n",
    "\n",
    "GFS uses a single-master architecture to simplify design and allow for optimal data placement.\n",
    "\n",
    "* **GFS Master**: A single server that stores all file system metadata, including the namespace, access control information, and the mapping from files to chunks. It also manages chunk replication and placement. The master does not handle file data directly; it only provides metadata to clients.\n",
    "* **Chunkservers**: These servers store data on their local disks as fixed-size **chunks** (64 MB). They read or write chunk data based on instructions from clients or the master.\n",
    "* **GFS Client**: A library linked into applications. It communicates with the master for metadata and then interacts directly with chunkservers for data I/O, minimizing the master‚Äôs involvement.\n",
    "\n",
    "### Write Operation and Replication\n",
    "\n",
    "GFS uses a **passive replication** model to ensure data consistency and fault tolerance. For each chunk, the master designates one replica as the **primary** and others as **secondaries**.\n",
    "\n",
    "The write process follows these steps:\n",
    "\n",
    "1. The client asks the master for the primary and secondary replica locations for a specific chunk.\n",
    "2. The master provides this information to the client.\n",
    "3. The client pushes the data to all replicas, typically in a chained fashion for network efficiency.\n",
    "4. Once all replicas acknowledge receiving the data, the client sends a write request to the primary replica.\n",
    "5. The primary determines a serial order for mutations and forwards the write request to all secondaries.\n",
    "6. The secondaries reply to the primary after completing the operation.\n",
    "7. Finally, the primary replies to the client. If errors occur, the file region may be left in an inconsistent state.\n",
    "\n",
    "### GFS Consistency Model\n",
    "\n",
    "GFS has a **relaxed consistency model** that prioritizes performance for its specific workload.\n",
    "\n",
    "* A file region is \"**defined**\" if all clients see the same data and the writes in their entirety. A region is \"**consistent**\" if all clients see the same data, but it might be a mix of writes from different clients.\n",
    "* **Successful serial writes** leave a region in a defined state.\n",
    "* **Successful concurrent writes** leave the region consistent but undefined.\n",
    "* **Record appends** are guaranteed to be written atomically at least once.\n",
    "\n",
    "### Master Fault Tolerance\n",
    "\n",
    "To mitigate the single-point-of-failure risk, the master's state is replicated.\n",
    "\n",
    "* All metadata changes are written to an **operations log** stored on multiple machines.\n",
    "* **Shadow masters** provide read-only access to the filesystem, even if the primary master is down.\n",
    "* If the master fails, an external mechanism selects a new master to take over.\n",
    "\n",
    "***\n",
    "\n",
    "## Chubby: A Distributed Lock Service\n",
    "\n",
    "Chubby is a coarse-grained lock service designed to provide coordination and reliable storage for loosely coupled distributed systems.\n",
    "\n",
    "### Purpose and Use Cases\n",
    "\n",
    "Chubby's primary role is to allow clients to synchronize their activities, often for tasks that take hours or days. Key uses include:\n",
    "\n",
    "* **Master Election**: GFS and Bigtable use Chubby to elect a single active master from a pool of potential servers.\n",
    "* **Name Service**: It provides a well-known location for clients to discover the location of other services.\n",
    "* **Reliable Storage**: It offers a reliable, though low-volume, file system for storing small amounts of critical metadata.\n",
    "\n",
    "### Chubby Architecture\n",
    "\n",
    "A Chubby deployment, known as a **cell**, typically consists of five replicated servers (replicas) per data center.\n",
    "\n",
    "* The replicas use the **Paxos consensus algorithm** to elect a **master**.\n",
    "* All read and write requests are handled by the master, which ensures strict consistency. Updates are propagated to a quorum of replicas before being acknowledged.\n",
    "* Clients interact with Chubby through a library that caches file data and handles communication with the master.\n",
    "* **Sessions** are maintained between the client and the master using leases. If a client fails to renew its lease, the session expires, and any locks it holds are released.\n",
    "\n",
    "***\n",
    "\n",
    "## Bigtable: A High-Performance Storage System\n",
    "\n",
    "Bigtable is a distributed storage system for managing structured data at a very large scale. It is not a relational database but rather a sparse, persistent, multi-dimensional sorted map.\n",
    "\n",
    "### Bigtable Data Model\n",
    "\n",
    "The fundamental mapping in Bigtable is:\n",
    "`(row:string, column:string, time:int64) -> string`\n",
    "\n",
    "* **Rows**: Data is indexed by a **row key**. Rows are sorted lexicographically, allowing for efficient scans over consecutive rows. This is often exploited by structuring row keys to group related data together (e.g., reversing domain names). All operations on a single row are **atomic**.\n",
    "* **Columns**: Columns are grouped into **column families**, which are the basic unit of access control and storage. A column is identified by its family and a **qualifier** (e.g., `anchor:cnnsi.com`).\n",
    "* **Timestamps**: Each cell can contain multiple versions of data, indexed by a 64-bit timestamp.\n",
    "\n",
    "### Bigtable Architecture\n",
    "\n",
    "Bigtable is built on top of GFS and Chubby and consists of three main components:\n",
    "\n",
    "* **Client Library**: Linked into every client application, it handles communication with the rest of the system.\n",
    "* **Master Server**: Responsible for metadata tasks like assigning tablets to tablet servers, load balancing, and garbage collection of GFS files. The master is largely stateless, storing all its metadata in Chubby, which simplifies recovery.\n",
    "* **Tablet Servers**: Each tablet server manages a set of **tablets** (10‚Äì1000 per server). A tablet is a contiguous range of rows from a table. Tablet servers handle all read/write requests for the tablets they serve and split tablets that grow too large.\n",
    "\n",
    "### Tablet Location and Serving\n",
    "\n",
    "* To find the server responsible for a particular row, clients traverse a three-level **B+-tree** hierarchy stored in Bigtable itself. The location of the root tablet is stored in a Chubby file.\n",
    "* When a tablet server receives a write, it writes the update to a commit log in GFS and then stores it in an in-memory sorted map called a **memtable**.\n",
    "* Reads are served from a merged view of the memtable and a set of immutable files on GFS called **SSTables**.\n",
    "* Periodically, the memtable is converted into a new SSTable (**minor compaction**), and existing SSTables are merged to reduce clutter and reclaim space (**major compaction**).\n",
    "\n",
    "### Performance\n",
    "\n",
    "Bigtable's performance scales well with the number of tablet servers.\n",
    "\n",
    "* **Random reads** are relatively slow because they can't take advantage of locality on GFS. However, reads from in-memory locality groups are much faster.\n",
    "* **Sequential reads and scans** are very fast because they read entire data blocks from GFS, maximizing data utilization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dea2f",
   "metadata": {},
   "source": [
    "# LECTURE 20/10/2025\n",
    "\n",
    "# PEER TO PEER NETWORKS\n",
    "\n",
    "## Overlay Networks and Peer to Peer Networks\n",
    "\n",
    "**Overlay Networks** are virtual networks built over an underlying network. In overlay networks, nodes are connected through logical links rather than physical links. These networks allow flexible routing, improved scalability, and support for new services without modifying the underlying physical network.\n",
    "\n",
    "Overlay networks adds a layer to the stack to provide something that the underlying network does not have without changing it (e.g. service like multimedia content distribution, routing protocol, etc...)\n",
    "\n",
    "Is it possible to build everything on the application layer? No, while an Application Layer Overlay Network provides valuable application-specific intelligence and routing, it still fundamentally relies on the lower layers (especially IP and TCP/UDP) for the actual transmission, addressing, and physical delivery of data across the network.\n",
    "\n",
    "### Types of overlay:\n",
    "\n",
    "| For what | Type | Description |\n",
    "|:------------:|:------------:|:-------------:|\n",
    "| Application needs | Distributed hash tables | Decentralized key mapping service in a large network |\n",
    "|| P2P file sharing | Constructing addressing and routing mechanism to support cooperative discovery and use of files |\n",
    "|| Content distribution networks | Replication, caching and placement strategies |\n",
    "| Network Style | wireless ad hoc networks | Provides customized routing protocols |\n",
    "|| Distruption-tolerant networks | Networks designed to operate in hostile environments |\n",
    "| Additional features | Multicast | Provides multicast services where multicast routers are not available |\n",
    "|| Resilience | Improved robustness and availability |\n",
    "|| Security | Enhanced security over underling IP network, including VPN |\n",
    "\n",
    "### Limitations of client/server paradigm\n",
    "Simple architecture:\n",
    "client -- request --> [Server]\n",
    "\n",
    "However this architecture has some issues uch as scalability (more computing power if there are more users) and reliability (network depends on server)\n",
    "\n",
    "### Limitations of P2P\n",
    "\n",
    "System is based on the direct communication between peers, it is decentralized, it survives extreme network changes, model is highly scalable and benefits from consumer technology.\n",
    "\n",
    "However it has some issues: on/off behaviour, need to join, need to discover other peers, misunderstanding communication rules when implementing peers, prevent free riding, incentivise participation and reciprocation.\n",
    "\n",
    "This networks needs protocols for: finding peers, finding what services a peer provides, obtain status from a peer, invoke a service on a peer, create/join/leave peer groups, create data connection, relaying messages. Basically any kind of communication between peers and network.\n",
    "\n",
    "A P2P system consists of autonomous entities (peers) able to auto organize and share a set of distributed resources in a computer network. \n",
    "\n",
    "A peer is a node, it possesses a unique id, it belongs to either one or multiple groups and each one of them can communicate with other peers.\n",
    "\n",
    "### Types of P2P applications categories\n",
    "| Type | Description |\n",
    "|-----|-----|\n",
    "| Distributed computing | Decomposition of larger problem into smaller paraller problems |\n",
    "| File sharing | Efficient search across WAN |\n",
    "| Collaborative applications | Update mechanism to provide consistency in multi-user requirement |\n",
    "\n",
    "### Common primitives in a P2P file sharing system\n",
    "\n",
    "- join: participate in network, discover at leas one existing peer and register itself in the network.\n",
    "- publish: advertise my file, for example napster a centralized server stored mappings, in gnutella broadcasts a query hit, in a DHT-based system inserts a record into the DHT. (distributed hash table).\n",
    "- search: find file/service, flooding (broadcast queries to all neighbors) or indexing/dht lookup or hybrid approaches.\n",
    "- fetch: retrieve file/use service, establish connection with one or more peers and download the file.\n",
    "\n",
    "---\n",
    "\n",
    "## Centralized Peer to Peer Network\n",
    "\n",
    "How did it start?\n",
    "It started with Napster which was an American proprietary peer-to-peer file sharing application primarily associated with digital audio file distribution.\n",
    "The kkey idea was to share content, storange and bandwith of individual users.\n",
    "\n",
    "### Challenges of Napster\n",
    "Main:\n",
    "- Find where a file is stored\n",
    "\n",
    "Other:\n",
    "- How to scale it up in order to support countless machines\n",
    "- How to make the system dynamic in which each machine can come and go\n",
    "\n",
    "### Solutions\n",
    "\n",
    "A centralized index system maintains a mapping between files (e.g., songs) and the machines currently storing them. To locate a file, a user queries the index system, which returns the machine hosting the requested file (preferably the nearest or least) loaded one. The file can then be retrieved via FTP. \n",
    "\n",
    "Advantages:\n",
    "- Simple and easy to implement.\n",
    "- Supports sophisticated search functionalities.\n",
    "\n",
    "Disadvantages:\n",
    "- Single point of failure reduces robustness.\n",
    "- Centralized bottleneck limits scalability.\n",
    "\n",
    "### Search operation\n",
    "In this system, the client sends search keywords to the server, which looks up matching files in its index and returns a list of hosts represented as `<ip_address, portnum>` pairs. The client then pings each host to measure transfer rates and selects the optimal one to download the file from.\n",
    "\n",
    "### Issues with search operation\n",
    "This approach faces several issues: the centralized server can become a source of congestion and represents a single point of failure. Additionally, communication lacks security since messages and passwords are sent in plaintext. Napster was also held legally responsible for users‚Äô copyright violations, leading to claims of indirect infringement.\n",
    "\n",
    "---\n",
    "\n",
    "## Unstructured Peer to Peer Network\n",
    "\n",
    "The first ever unstructured and decentralized P2P network was Gnutella, founded in 2000. In this system, peers form an overlay network. When a client joins, it connects to a few known nodes, which become its neighbors. Unlike centralized systems, there is no need to publish files. For searching, a node sends a query to its neighbors, who forward it recursively until a match is found, at which point a reply is sent back to the requester. Once the desired file is located, it is fetched directly from the peer hosting it.\n",
    "\n",
    "### Search operation in GNutella\n",
    "\n",
    "**Advantages:**\n",
    "- Fully decentralized with no central point of failure.  \n",
    "- Highly robust against node failures.  \n",
    "\n",
    "**Disadvantages:**\n",
    "- Poor scalability, as deterministic searches may require contacting many peers.  \n",
    "- Network can become flooded with excessive query traffic.  \n",
    "- Each request must include a Time-To-Live (TTL) limit to prevent uncontrolled flooding.  \n",
    "\n",
    "### Avoiding excessive traffic\n",
    "\n",
    "In Gnutella, queries are forwarded to all neighbors except the one they were received from, and each query, identified by a DescriptorID, is forwarded only once. Peers maintain a list of recently seen messages to avoid duplicates, dropping any repeated queries with the same DescriptorID and payload type. Responses (QueryHits) are routed back only along the path from which the original query arrived, and any QueryHit without a corresponding query is discarded.\n",
    "\n",
    "### Download operation in GNutella\n",
    "\n",
    "The requester selects the \"best\" responder from the QueryHits and fetches the file directly using HTTP:\n",
    "```\n",
    "GET /get/<File Index>/<File Name>/HTTP/1.0\\r\\n\n",
    "Connection: Keep-Alive\\r\\n\n",
    "Range: bytes=0-\\r\\n\n",
    "User-Agent: Gnutella\\r\\n\n",
    "\\r\\n\n",
    "```\n",
    "\n",
    "HTTP is used because it is widely supported and accepted by firewalls, and the `Range` field allows partial file transfers.\n",
    "\n",
    "### Comparing Napster and GNutella\n",
    "\n",
    "|                | Napster                          | Gnutella                              |\n",
    "|----------------|---------------------------------|----------------------------------------|\n",
    "| **Pros**       | - Simple                        | - Simple                               |\n",
    "|                | - Search scope is O(1)          | - Fully decentralized                  |\n",
    "|                |                                 | - Search cost distributed              |\n",
    "| **Cons**       | - Server maintains O(N) state   | - Search scope is O(N)                 |\n",
    "|                | - Server performance bottleneck | - Search scope is O(N)                 |\n",
    "|                | - Single point of failure       | - Large number of freeloaders          |\n",
    "\n",
    "\n",
    "### New GNutella protocol\n",
    "\n",
    "The FastTrack protocol, initially implemented in Kazaa, KazaaLite, and Grokster, and later adapted in Gnutella, improves on traditional P2P networks by designating some peers as supernodes (ultrapeers). These supernodes leverage healthier, more reliable participants and maintain a Napster-like directory of files.  \n",
    "\n",
    "**Smart Query Flooding:**\n",
    "- **Join:** Client contacts a supernode on startup; may become a supernode itself.  \n",
    "- **Publish:** Client sends its list of shared files to its supernode.  \n",
    "- **Search:** Queries are sent to the supernode, which floods them only among other supernodes.  \n",
    "- **Fetch:** Files are downloaded directly from peers and can be fetched simultaneously from multiple sources.  \n",
    "\n",
    "\n",
    "### FastTrack\n",
    "In FastTrack, each supernode maintains a directory of nearby peers, storing entries like `<filename, peer pointer>`, similar to Napster servers. Supernode membership is dynamic, and any peer can become a supernode if it earns enough reputation, which depends on factors like connection uptime and total uploads. More advanced reputation schemes, sometimes based on economic incentives, have also been developed to manage supernode selection and reliability.\n",
    "\n",
    "**Pros:**\n",
    "- Balances search overhead with storage requirements.  \n",
    "- Accounts for node heterogeneity, including bandwidth and computational resources.  \n",
    "\n",
    "**Cons:**\n",
    "- No strict guarantees on search scope or search time.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Structured Peer to Peer Network\n",
    "\n",
    "Structured P2P networks organize peers and resources using a predefined structure, often based on distributed hash tables (DHTs), to enable efficient and deterministic data location. Unlike unstructured networks, where searches rely on flooding, structured networks provide guaranteed lookup performance. Prominent examples include **Chord**, **Pastry**, and **Tapestry**, each using different routing and identifier schemes to map keys to nodes and facilitate scalable, fault-tolerant file retrieval.\n",
    "\n",
    "### API based on unique GUID associated to data\n",
    "\n",
    "Structured P2P networks often provide an API based on a unique **GUID** associated with each data object. The basic operations include storing, retrieving, and deleting data, with replicas maintained at all nodes responsible for the GUID. An alternate approach treats the GUID as a reference to an object, supporting messaging and access control.  \n",
    "\n",
    "**API:**\n",
    "\n",
    "```text\n",
    "put(GUID, data)         # Store data; replicated at responsible nodes\n",
    "remove(GUID)            # Delete all references and data associated with GUID\n",
    "value = get(GUID)       # Retrieve data from one of the responsible nodes\n",
    "\n",
    "publish(GUID)           # Make object accessible via its GUID\n",
    "unpublish(GUID)         # Make object inaccessible\n",
    "sendToObj(msg, GUID, n) # Send a message to n replicas of the object (e.g., request to download)\n",
    "```\n",
    "\n",
    "### Robustness in this system\n",
    "\n",
    "To maintain stability, each node periodically sends a `stabilize()` message to its successor. When a node receives this message, it checks whether the sender is a better predecessor and updates its predecessor if needed, then responds with its own predecessor via `notify()`. Upon receiving `notify()`, the sender updates its successor if the notified node falls between itself and its current successor; otherwise, no change is made. This process ensures the network remains consistent and resilient to node joins or failures.\n",
    "\n",
    "### Achieving Efficiency: finger tables\n",
    "\n",
    "Each node maintains a finger table to speed up lookups. The i-th entry of a node with ID `n` points to the first node whose ID is ‚â• `(n + 2^(i-1)) mod 2^m`, where `m` is the number of bits in the ID space. These fingers allow queries to ‚Äújump‚Äù exponentially closer to the target GUID rather than traversing one successor at a time, reducing lookup time from O(N) to O(log N) hops.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison between Napster, Gnutella and Chord\n",
    "| Network   | Memory Lookup      | Latency      | #Messages for a Lookup |\n",
    "|-----------|-----------------|-------------|----------------------|\n",
    "| Napster   | O(1) (O(N) @server) | O(1)       | O(1)                 |\n",
    "| Gnutella  | O(N)             | O(N)        | O(N)                 |\n",
    "| Chord     | O(log N)         | O(log N)    | O(log N)             |\n",
    "\n",
    "---\n",
    "\n",
    "## Pastry Overview\n",
    "\n",
    "Pastry assigns IDs to nodes using a virtual ring, similar to Chord. Each node maintains a **leaf set**, which includes its immediate successors and predecessors, and a **routing table** based on prefix matching. For example, a node with ID `01110100101` keeps neighbor peers for each prefix `*`, `0*`, `01*`, `011*`, ‚Ä¶ up to `0111010010*`. When routing to a target ID, such as `01110111001`, the node forwards the message to the neighbor whose ID shares the **longest prefix match** with the target, progressively moving the message closer to its destination.\n",
    "\n",
    "### Pastry Locality\n",
    "\n",
    "Pastry incorporates network locality into its routing. For each prefix (e.g., `011*`), the node selects the neighbor with the **shortest round-trip time** among all peers matching that prefix. Shorter prefixes have many candidates spread across the network, so early routing hops tend to be physically shorter, while later hops toward longer prefixes generally cover larger distances. Nodes can also monitor network traffic to discover better neighbors, ensuring more efficient routing based on proximity.\n",
    "\n",
    "#### Pastry Prefix Routing Example\n",
    "\n",
    "Node IDs (hex, simplified for illustration):\n",
    "\n",
    "- Source: 65a1fc\n",
    "- Target: d46a1c\n",
    "\n",
    "Routing table for 65a1fc (showing first 4 prefix rows):\n",
    "\n",
    "| Row | Prefix | Example Neighbor |\n",
    "|-----|--------|----------------|\n",
    "| 1   | 6      | 6xxxxxx        |\n",
    "| 2   | 65     | 65xxxxx        |\n",
    "| 3   | 65a    | 65axxxx        |\n",
    "| 4   | 65a1   | 65a1xxx        |\n",
    "\n",
    "**Routing Steps:**\n",
    "\n",
    "1. Current node: 65a1fc  \n",
    "   - Longest shared prefix with target d46a1c = 0 (no match)  \n",
    "   - Forward to neighbor in **row 1** that starts with a different first hex digit closer to `d` ‚Üí e.g., neighbor `dxxxxxx`\n",
    "\n",
    "2. Next node: `dxxxxxx`  \n",
    "   - Prefix match with target: 1 hex digit (`d`)  \n",
    "   - Forward to neighbor in **row 2 or 3** with longer matching prefix ‚Üí `d4xxxxx`\n",
    "\n",
    "3. Next node: `d4xxxxx`  \n",
    "   - Prefix match: 2 hex digits (`d4`)  \n",
    "   - Forward to neighbor in **row 3 or 4** ‚Üí `d46xxxx`\n",
    "\n",
    "4. Next node: `d46xxxx`  \n",
    "   - Prefix match: 3 hex digits (`d46`)  \n",
    "   - Forward to neighbor in **row 4** ‚Üí target `d46a1c`  \n",
    "\n",
    "**Key Idea:**  \n",
    "- At each hop, the node forwards to a neighbor with the **longest matching prefix** with the target.  \n",
    "- This reduces the number of hops to roughly **log_base_16(N)** for N nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## Tapestry\n",
    "\n",
    "Tapestry uses the same **prefix-based routing** as Pastry but adds more flexibility through its **DOLR (Distributed Object Location and Routing) interface**, which includes operations like `publish(GUID)`, `unpublish(GUID)`, and `sendToObj(msg, GUID, [n])`. The key advantage is that applications can place replicas of objects **closer to frequent users**, reducing latency, minimizing network load, and improving tolerance to network and host failures.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3d905",
   "metadata": {},
   "source": [
    "# LECTURE 21/10/2025\n",
    "\n",
    "# Internet of Things\n",
    "\n",
    "Before IoT it was WSN (Wirless Sensor Networks), it was a key topic around 2005 with the following life:\n",
    "\n",
    "Embedded Systems -> Wireless Sensor Networks -> Cyber Physical Systems -> Internet of Things\n",
    "\n",
    "Nowadays the industry if focused in IoT which is similar to CPS, WSN and ES.\n",
    "\n",
    "These technologies are more or less the same but there are some differences:\n",
    "\n",
    "WSN is ES with a wireless interface\n",
    "\n",
    "CPS is WSN with actuators\n",
    "\n",
    "IoT is CPS with IP\n",
    "\n",
    "## Challenges: resource constraints\n",
    "\n",
    "- Power (energy)\n",
    "- Bandwith\n",
    "- Memory, CPU\n",
    "\n",
    "To overcome the power consumption issue a Dynamic power management (DPM) was implemented:\n",
    "\n",
    "example: STRONGARM SA1100\n",
    "\n",
    "THe would be 3 stages: RUN, IDLE (SW routing may stio the CPU not in use while monitoring interrupts), SLEEP (shutdown of onchip activity)\n",
    "\n",
    "Each state consumes a different amount of power.\n",
    "\n",
    "### Sample Applications of IoT and Dynamic Power Management\n",
    "\n",
    "#### 1. Flower Auction\n",
    "- IoT sensors manage **temperature, humidity, and lighting** in flower storage areas.  \n",
    "- **Dynamic Power Management (DPM)** optimizes energy use based on auction activity and sensor data.  \n",
    "- Results in **improved product quality**, **lower operational costs**, and **energy efficiency**.\n",
    "\n",
    "#### 2. PermaSense Case\n",
    "- Long-term **environmental monitoring** system deployed in the Swiss Alps.  \n",
    "- Uses **low-power sensors** and **adaptive duty-cycling** to study permafrost and climate change.  \n",
    "- Integrates **solar energy harvesting** and **event-driven wake-ups** for year-round operation.  \n",
    "- Demonstrates **reliable, autonomous IoT systems** under extreme conditions.\n",
    "\n",
    "##### 3. Basic System Architecture\n",
    "### Core Components\n",
    "- **Sensor Nodes** ‚Äì sense data and manage local power states.  \n",
    "- **Dynamic Power Manager (DPM)** ‚Äì controls active/sleep modes based on workload.  \n",
    "- **Communication Network** ‚Äì low-power wireless (LoRa, ZigBee, BLE).  \n",
    "- **Edge/Cloud Layer** ‚Äì performs analytics and optimizes energy use.  \n",
    "- **User Interface** ‚Äì provides monitoring and control dashboards.\n",
    "\n",
    "### Key Idea\n",
    "A **distributed, hierarchical IoT architecture** where each node intelligently manages its power while cooperating for system-wide energy optimization.\n",
    "\n",
    "\n",
    "### Low Duty Cycle (50ms/30min)\n",
    "\n",
    "This slide illustrates a communication protocol designed for extreme power saving, operating with a very low duty cycle of 50 milliseconds of activity every 30 minutes. To maintain timing despite long periods of sleep mode, the system relies on internal clock synchronization rather than direct network alignment. The protocol manages time accuracy by calculating node wake-up times based on other clocks and providing local compensation for crystal drift and temperature impact. Finally, the duty cycle is adjustable, allowing the system to schedule more frequent activity bursts during critical times, such as day/night transitions\n",
    "\n",
    "---\n",
    "\n",
    "## Mainstream protocolos / MAC layers\n",
    "\n",
    "### Standardized WSN protocols\n",
    "\n",
    "THe traditional WiFi + IP is too resource intensive for WSN.\n",
    "\n",
    "802.15.1 (bluetooth: cable replacement):\n",
    "\n",
    "It is classic, there is one PAN coordinator that can have up to 7 slaves with a bandwith of 1 Mb/s, through BLE it can have unlimited connections but a max speed of 125 kb/s\n",
    "\n",
    "It is based on a bluetooth mesh network\n",
    "\n",
    "### Personal area networks\n",
    "\n",
    "802.15.4 are protocol designed for wireless personal area networks (like a smart home, cars, remote engineering) and is used for monitoring and control, it is also easy to install but it lacks mobility and has advantages such as low power but with a downside of low transmission rates and low range.\n",
    "\n",
    "802.15.4 protocol defines physical and MAC layer\n",
    "\n",
    "### IEEE 802.15.4 MAC Overview\n",
    "\n",
    "THe network of a mac protocol is composed by different nodes that have different funcionalities/roles:\n",
    "\n",
    "- FFD (full function device): any topologu, network coordinator capable, talks to any device.\n",
    "- RFD (reduced function device): star topology only, network coordinator incapable, talks only to the coordinator, very simple implementation.\n",
    "\n",
    "Basically one of the FFD can become a master while the other are slaves, if a network assumes a different form other than a star every node is FFD, otherwise if a network results very complex there will be clustered stars where there are multiple stars inside the network and only in those forms there are RFDs.\n",
    "\n",
    "**CSMA-CA**: stands for carrier-sense multiple access collision avoidance, basically happens when there are 2 nodes trying to communicate with the same node in the same instance (i think, redo this chapter with carl), basically after the receivers sends a clear to send signal it the other node should back off if carrier is occupied (since only the other node receives the ACK), in wifi it uses RTS/CTS (idk what these stand for).\n",
    "\n",
    "**IEEE 802.15.4 Optional Supeframe Structure**: a period of time of 15ms * 2^n (where n stards for amount of nodes i guess and it is between 0 and 14 included), these period begins and ends with a short window of time of network beacon in thich the coordinator transmits a signal containing network information, frame structure and notification of pending node messages, after the beacon there is an extended period at the beginning which is reserved in case the beacon requires more time, then a contention access period in which ca be accessed by any node using CSMA-CA, then Contention Free Period which is a reserved time for nodes requiring guaranteed bandwidth (n=0) and at the very end the small window of network beacon mentioned earlier.\n",
    "\n",
    "### MAC Modes\n",
    "\n",
    "Starting from the base 802.15.4 MAC protocol it branches into various different modes and each implements different features.\n",
    "\n",
    "\n",
    "### Zigbee\n",
    "This is a low power wireless mesh network standard targeted at battery powered devices in wireless control and monitoring applications.\n",
    "\n",
    "- Built on top of the IEEE 802.15.4 standard, adding a Network layer.\n",
    "- Created and maintained by the Zigbee Alliance.\n",
    "- Supports dynamic node joining with assigned 16-bit addresses.\n",
    "- Enables tree, star, and mesh network topologies.\n",
    "- Uses a distance-vector algorithm for route discovery.\n",
    "- Provides 128-bit AES encryption for security.\n",
    "- Includes an application layer framework for building applications.\n",
    "\n",
    "### 6LoWPAN\n",
    "\n",
    "6LoWPAN (IPv6 over Low-Power Wireless Personal Area Networks) is an IETF standard that enables efficient transmission of IPv6 packets over IEEE 802.15.4 low-power radio networks. Although IPv6 already exists, its default header size and packet structure are too large for constrained IoT devices, so 6LoWPAN introduces an adaptation layer to make IPv6 practical in low-power environments.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Why not just use standard IP?**\n",
    "  - Developers are already familiar with IP and its tools.\n",
    "  - But IPv6 headers (40 bytes) are too large for low-power radios with very small frame sizes.\n",
    "\n",
    "- **Addressing limitations**\n",
    "  - Traditional ZigBee uses 16-bit addresses, which is insufficient for large-scale IoT deployments.\n",
    "  - 6LoWPAN uses IPv6 addresses, enabling global addressing and interoperability.\n",
    "\n",
    "- **IETF Standard**\n",
    "  - Developed by the IETF as: **6LoWPAN = IPv6 over IEEE 802.15.4**.\n",
    "\n",
    "- **Header Compression**\n",
    "  - IEEE 802.15.4 devices have **64-bit MAC addresses**.\n",
    "  - IPv6 interface identifiers also use **64 bits**, so these can be derived directly from the MAC.\n",
    "  - This eliminates the need to repeat the address in the IPv6 header.\n",
    "  - Result: a compressed 6LoWPAN header as small as **4 bytes** for simple point-to-point or star networks.\n",
    "\n",
    "- **Optional Headers**\n",
    "  - **Fragmentation Header** ‚Üí Used when packets exceed the tiny 802.15.4 frame size.\n",
    "  - **Mesh Header** ‚Üí Adds routing information for multi-hop mesh networks.\n",
    "\n",
    "**In Short**\n",
    "6LoWPAN makes IPv6 feasible on extremely constrained IoT devices by compressing headers, supporting fragmentation, and enabling mesh networking, all while preserving the familiar IPv6 protocol model.\n",
    "\n",
    "### LoRa (Long Range) Summary\n",
    "\n",
    "LoRa is a long-range, low-power, and low-throughput wireless communication technology designed for IoT devices that need extended range and long battery life. It separates responsibilities between the physical layer (LoRa) and the MAC layer (LoRaWAN), enabling flexible deployments and adaptive performance.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Characteristics**\n",
    "  - Designed for **long-range**, **low-power**, and **low-data-rate** communication.\n",
    "  - Ideal for battery-powered IoT sensors that transmit small, infrequent messages.\n",
    "\n",
    "- **Layer Structure**\n",
    "  - **LoRa (PHY Layer)** ‚Üí A proprietary modulation technique developed by *Semtech*.\n",
    "  - **LoRaWAN (MAC Layer)** ‚Üí An open specification defined by the **LoRa Alliance** for networking, security, device classes, and gateway operation.\n",
    "\n",
    "- **Range**\n",
    "  - Typically **2‚Äì5 km** in urban/obstructed environments.\n",
    "  - Can exceed **15 km** in rural or open areas.\n",
    "\n",
    "- **Data Throughput**\n",
    "  - Ranges from **0.3 kbps to 50 kbps**, depending on configuration.\n",
    "\n",
    "- **Adaptive Spreading Factor (SF)**\n",
    "  - LoRaWAN dynamically adjusts the **Spreading Factor** of the LoRa PHY.\n",
    "  - Higher SF ‚Üí **longer range** but **lower data rate**.\n",
    "  - Lower SF ‚Üí **higher data rate** but **reduced range**.\n",
    "  - This trade-off helps optimize **network capacity**, **device lifetime**, and **reliability**.\n",
    "\n",
    "**Summary**\n",
    "LoRa and LoRaWAN enable ultra-long-range, energy-efficient IoT communication by combining Semtech‚Äôs PHY technology with an open, adaptive MAC layer that balances range, throughput, and power consumption.\n",
    "\n",
    "### SigFox\n",
    "\n",
    "SigFox is an ultra-narrowband, long-range, low-power communication technology designed for extremely small and infrequent data transmissions. It operates on a global network deployed and maintained by SigFox through partnerships with local telecom operators, enabling wide coverage without requiring users to install or manage their own infrastructure.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Range**\n",
    "  - Typically **a few kilometers** in dense urban environments.\n",
    "  - Up to **40 km** in rural areas when using directional antennas.\n",
    "  - Signals can even **penetrate underground**, making it suitable for buried sensors.\n",
    "\n",
    "- **Network Deployment**\n",
    "  - SigFox builds and manages the antenna network globally.\n",
    "  - Works with **local telecom partners** for deployment and maintenance.\n",
    "  - Device owners simply **subscribe** to the service (around **1 euro per device per year**).\n",
    "\n",
    "### Uplink Capabilities (Device ‚Üí Network)\n",
    "- Up to **140 messages per day**.\n",
    "- Maximum **6 messages per hour**.\n",
    "- Each uplink message can contain **up to 12 bytes** of payload.\n",
    "\n",
    "### Downlink Capabilities (Network ‚Üí Device)\n",
    "- Up to **4 downlink messages per day**.\n",
    "- Each downlink message carries **up to 8 bytes** of payload.\n",
    "\n",
    "### Best Use Cases\n",
    "SigFox is ideal for applications that:\n",
    "- Send **very small** and **infrequent** bursts of data.\n",
    "- Require **minimal downlink communication**.\n",
    "- Need **wide coverage** and **very long battery life**.\n",
    "\n",
    "Examples include:\n",
    "- Utility meters  \n",
    "- Environmental sensors  \n",
    "- Alarms and alert systems  \n",
    "\n",
    "### Narrowband Internet of Things (NB-IoT)\n",
    "\n",
    "Narrowband Internet of Things (NB-IoT) is a secure, reliable, and efficient Low-Power Wide-Area (LPWA) technology standardized by **3GPP**. It operates on **licensed cellular spectrum**, ensuring high reliability and predictable network performance. NB-IoT is designed to support massive numbers of low-power devices with excellent coverage, long battery life, and low deployment cost.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Low Power Consumption**\n",
    "  - Optimized for long battery life (often exceeding 10 years) through power-saving modes and efficient signaling.\n",
    "\n",
    "- **Low Device & Connectivity Cost**\n",
    "  - Minimal hardware complexity and lightweight communication protocols reduce both module costs and subscription fees.\n",
    "\n",
    "- **Massive Scalability**\n",
    "  - Supports **tens of thousands of devices per base station**, making it ideal for large-scale IoT deployments.\n",
    "\n",
    "- **Long Range**\n",
    "  - Approximately **5 km** in dense urban areas.\n",
    "  - Up to **50 km** in rural or open environments.\n",
    "\n",
    "- **Excellent Signal Penetration**\n",
    "  - Strong indoor coverage, capable of reaching:\n",
    "    - Elevators\n",
    "    - Basements\n",
    "    - Underground parking facilities\n",
    "\n",
    "**Summary**\n",
    "NB-IoT delivers secure, energy-efficient, and large-scale IoT connectivity using licensed cellular networks, making it suitable for applications like smart meters, tracking devices, industrial sensors, and other deployments requiring deep indoor coverage and high reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## IoT Routing\n",
    "\n",
    "Routing is the process of selecting the best path and sending data packets across networks, from a source to a destination. Its job is to provide low power wireless links and create a network so that the data package traverses multiple links to reach the destination.\n",
    "\n",
    "### Address Assignment\n",
    "\n",
    "ZigBee uses a **distributed address assignment scheme** to allocate network addresses to devices in a structured and scalable way. Instead of relying on a centralized server, each parent node calculates address ranges for its children based on global network parameters defined by the ZigBee coordinator.\n",
    "\n",
    "**Coordinator-Defined Network Parameters**\n",
    "\n",
    "The ZigBee coordinator sets three key parameters that shape the tree-based network:\n",
    "\n",
    "- **Cm ‚Äì Maximum number of children per router**\n",
    "  - Total number of child devices (end devices + routers) a router can support.\n",
    "\n",
    "- **Rm ‚Äì Maximum number of child routers per parent**\n",
    "  - Number of children that can themselves act as routers.\n",
    "\n",
    "- **Lm ‚Äì Maximum network depth**\n",
    "  - Defines how many layers the network tree can have.\n",
    "\n",
    "**Address Allocation Logic**\n",
    "\n",
    "Each parent device uses the parameters **Cm**, **Rm**, and **Lm** to compute a value called **Cskip**.\n",
    "\n",
    "- **Cskip**\n",
    "  - Determines the size of the address block reserved for each child.\n",
    "  - Helps ensure that child routers receive non-overlapping address pools.\n",
    "  - Enables scalable and conflict-free hierarchical addressing.\n",
    "\n",
    "**Formula**\n",
    "\n",
    "Basically Zigbee generates a tree of parents and children and uses a formula to determine how many child a node can have:\n",
    "\n",
    "Let `A_parent` be the parent's network address, `Cskip(d)` the skip value at depth `d`,\n",
    "`Rm` the maximum number of child routers a parent can have, and `n` the 1-based index\n",
    "of the child.\n",
    "\n",
    "### nth child **router** (1 ‚â§ n ‚â§ Rm)\n",
    "\n",
    "    A_parent + (n - 1) * Cskip(d) + 1\n",
    "\n",
    "### nth child **end device** (1-based index for end devices)\n",
    "\n",
    "    A_parent + Rm * Cskip(d) + n\n",
    "\n",
    "### Notes\n",
    "- `Cskip(d)` depends on the coordinator-defined parameters (`Cm`, `Rm`, `Lm`) and the parent's depth `d`.\n",
    "- Router children receive full address blocks of size `Cskip(d)`; end devices receive single addresses placed after all router blocks.\n",
    "- `n` starts at `1` (the first child).\n",
    "\n",
    "When a node receives a packet it checks if the destination is itself or one of the child, relays the data otherwise.\n",
    "\n",
    "### Flooding route discovery\n",
    "\n",
    "Flooding is a basic route discovery technique in wireless networks where a source node broadcasts a route-request packet to all its neighbors.\n",
    "\n",
    "**Directed diffusion**\n",
    "In normal flooding, every node rebroadcasts every Route Request (RREQ) it receives, creating large overhead. Directed diffusion reduces this cost by forwarding RREQs only in the direction of the destination. The node might know something about the direction given some info (geographical position, signal strength etc...)\n",
    "\n",
    "## Directed Diffusion ‚Äî Short Summary (Markdown Source)\n",
    "\n",
    "Directed Diffusion is a **data-centric, distributed routing protocol** for sensor networks where the sink requests data, and sources respond along locally-created gradients. The best path emerges through **reinforcement**, without global network knowledge.\n",
    "\n",
    "1. **Interest Flooding (Sink ‚Üí Network)**\n",
    "   - Sink broadcasts an **Interest** describing the desired data.\n",
    "   - Each node receiving it creates **gradients** pointing to the neighbor it got the Interest from.\n",
    "   - Gradients contain: neighbor ID, data rate, lifetime.\n",
    "   - Prevent duplicate forwarding; interests are soft-state and expire.\n",
    "\n",
    "2. **Exploratory Data Flow (Source ‚Üí Sink)**\n",
    "   - Nodes with matching data send **exploratory data** along all available gradients.\n",
    "   - Intermediate nodes forward data only along known gradients.\n",
    "   - Loops are avoided using data caches.\n",
    "\n",
    "3. **Gradient Reinforcement**\n",
    "   - Sink selects the **best path** (e.g., lowest latency or highest reliability).\n",
    "   - Sends a **reinforced Interest** along that path.\n",
    "   - Nodes strengthen the corresponding gradient, suppressing weaker gradients.\n",
    "\n",
    "4. **Data Transmission**\n",
    "   - Sources now send data along the **reinforced path**.\n",
    "   - Forwarding decisions are **local**; nodes only know their own gradients.\n",
    "\n",
    "### Key Characteristics\n",
    "- Distributed and local; no global routing tables.\n",
    "- Initial flooding is limited; reinforcement ensures efficiency.\n",
    "- Emergent optimal path similar in effect to shortest-path algorithms.\n",
    "- Naturally supports multiple sources and sinks.\n",
    "- Reduces energy consumption and avoids redundant transmissions.\n",
    "\n",
    "\n",
    "Through this method flooding is really poor because of the multiple paths from the origin to its destination, but the diffusion is better than OM (Omniscient Multicast) because duplication is suppressed. Also flood has a really high latency due to MAC collisions but it finds the lowest delay path in the network based on the gradient.\n",
    "\n",
    "### Dynamic Source Routing (DSR)\n",
    "\n",
    "DSR is an **on-demand routing protocol** for ad hoc networks. When a source node (S) wants to send data to a destination (D) and has no known route:\n",
    "\n",
    "1. **Route Discovery:**  \n",
    "   - S broadcasts a Route Request (RREQ) to neighbors.  \n",
    "   - Each node appends its own ID and forwards the RREQ, avoiding revisiting nodes.  \n",
    "2. **Route Reply:**  \n",
    "   - D receives the RREQ and sends a Route Reply (RREP) back along the reverse path.  \n",
    "   - RREP contains the complete route from S to D.  \n",
    "3. **Data Transmission:**  \n",
    "   - S caches the route.  \n",
    "   - Packets include the **source route** in the header.  \n",
    "   - Intermediate nodes forward packets using this source route.\n",
    "\n",
    "**Pros:**  \n",
    "- Routes are maintained only between communicating nodes.  \n",
    "- A single route discovery can yield multiple routes to the destination.\n",
    "\n",
    "**Cons:**  \n",
    "- Packet headers grow with route length.  \n",
    "- Flooding RREQs may reach all nodes ‚Üí collisions and high contention.  \n",
    "- Random delays are often inserted to reduce collisions.  \n",
    "- Multiple RREPs can cause a ‚ÄúRoute Reply Storm.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### Ad Hoc On-Demand Distance Vector (AODV) Routing\n",
    "\n",
    "- Unlike DSR, **AODV maintains routing tables at each node**, avoiding large source-route headers.  \n",
    "- Retains DSR‚Äôs advantage: routes are maintained only as needed.  \n",
    "- Reduces header overhead, especially for small data packets, improving efficiency over DSR.\n",
    "\n",
    "### Ad Hoc On-Demand Distance Vector (AODV)\n",
    "\n",
    "AODV is an **on-demand routing protocol** similar to DSR but uses **routing tables** instead of source routes in packet headers.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Route Discovery:**  \n",
    "   - Source broadcasts a Route Request (RREQ).  \n",
    "   - Each forwarding node sets up a **reverse path** pointing to the source.  \n",
    "   - Assumes **symmetric (bi-directional) links**.\n",
    "\n",
    "2. **Route Reply:**  \n",
    "   - Destination receives the RREQ and sends a Route Reply (RREP).  \n",
    "   - RREP travels back along the **reverse path** created during RREQ forwarding.  \n",
    "\n",
    "**Key Features:**  \n",
    "- Reduces header overhead compared to DSR.  \n",
    "- Routes are created **only when needed**.  \n",
    "- Maintains route information in **routing tables** at intermediate nodes.\n",
    "\n",
    "### AODV: Route Request and Route Reply\n",
    "\n",
    "- **RREQ:**  \n",
    "  - Includes the last known **sequence number** for the destination.  \n",
    "  - Intermediate nodes can reply with a **Route Reply (RREP)** if they know a more recent path than the sender.  \n",
    "- **RREP forwarding:**  \n",
    "  - Intermediate nodes record the **next hop** to the destination.  \n",
    "- **Route expiration:**  \n",
    "  - Reverse path entries expire after a timeout.  \n",
    "  - Forward path entries expire if not used within **active_route_timeout**.\n",
    "\n",
    "### AODV: Key Features\n",
    "\n",
    "- Routes are **not included in packet headers**.  \n",
    "- Nodes maintain **routing tables only for active routes**.  \n",
    "- Each node keeps **at most one next-hop per destination**.  \n",
    "- Sequence numbers ensure **fresh routes** and prevent **routing loops**.  \n",
    "- Unused routes expire automatically, even if topology is unchanged.  \n",
    "- Compared to DSR: DSR may store multiple routes per destination.\n",
    "\n",
    "\n",
    "### In summary\n",
    "AODV is like DSR conceptually (on-demand, route discovery) but relies on local routing table entries (predecessor/next-hop pointers) instead of embedding the entire route in the packet.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775234f",
   "metadata": {},
   "source": [
    "# LECTURE 27/10/2025\n",
    "\n",
    "# BLOCKCHAIN\n",
    "\n",
    "A blockchain is a decentralized, immutable digital ledger that securely records transactions across a network of computers. \n",
    "\n",
    "## Cryptographic Tools (Pages 3-25)\n",
    "\n",
    "### Cryptographic Hash Function (Pages 3-4)\n",
    "A **Cryptographic Hash Function** is defined as $H: X=\\{0,1\\}^{*} \\rightarrow Y=\\{0,1\\}^{L}$ with a fixed length $L$, such as $128/160/256/512$ bits. The informal property is that a **small change in the input produces a completely different output**. The key security properties, where collisions exist but are hard to find, are:\n",
    "* **Pre-image resistance:** It's hard to find an input $x$ for a given output $y$ such that $H(x)=y$.\n",
    "* **Second pre-image resistance:** Given an input $M$, it's hard to find a different $M'$ such that $H(M')=H(M)$.\n",
    "* **Collision-resistance:** It's hard to find two different inputs, $x_1 \\neq x_2$, that produce the same hash output, $H(x_1) = H(x_2)$.\n",
    "\n",
    "### Hash Function Application (Pages 5-8)\n",
    "A case study demonstrates using hash functions to ensure fairness when playing **Rock-paper-scissors** over the internet or email. A robust solution involves the player computing $H(T||M||S)$, where $M$ is the move, $S$ is a random string computed by the player, and $T$ is a string (nonce/timestamp) sent by the opponent.\n",
    "\n",
    "### Secure Hash Algorithm (SHA) (Page 9)\n",
    "The Secure Hash Algorithm (SHA) is a family of cryptographic hash functions published by the National Institute of Standards and Technology (NIST). Notable variants include:\n",
    "* **SHA-256:** Output size 256 bits, used in **Bitcoin**.\n",
    "* **SHA3-256 (Keccak 256):** Output size 256 bits, used in **Ethereum**.\n",
    "\n",
    "### Using Hashes for Data Integrity (Pages 10-12)\n",
    "To verify a large data structure, one can compute a hash $h$ and send it over a secure/expensive channel, then send the data normally.\n",
    "\n",
    "For data split into chunks $c_1...c_k$, instead of sending $k$ hashes, a chain of hashes can be used (chained hashes).\n",
    "$$h_{1}=H(c_{1}), h_{2}=H(c_{2}||h_{1})...h_{k}=H(c_{k}||h_{k-1})$$\n",
    "This allows verification of a sequence of chunks by checking a single hash, e.g., $h_{10}$ verifies the first 10 chunks/blocks.\n",
    "\n",
    "### Digital Signatures and Asymmetric Crypto (Pages 13-17)\n",
    "Digital signatures rely on three algorithms: **KeyGen**, **Sign**, and **Verify**. KeyGen produces a **secret signing key ($sk$)** and a **public verification key ($pk$)**.\n",
    "* Algorithms using private/public keys are very **slow**.\n",
    "* The signature is usually computed on the **hash** of the message for efficiency.\n",
    "* The receiver uses the public key to decrypt the signature and compares the result to the hash of the message.\n",
    "\n",
    "The algorithm used for Bitcoin transactions is **Elliptic Curve Digital Signature Algorithm (ECDSA)** with curve **SECP256K1** and hashing algorithm **SHA256**.\n",
    "\n",
    "### Merkle Tree (Hash Tree) (Pages 18-25)\n",
    "A **Merkle Tree** or Hash Tree is a data structure summarizing information about a large quantity of data to check its content. \n",
    "* It combines **hash functions** ($H$) with a **binary tree structure**.\n",
    "* **Leaves** are $H$ applied to initial symbols.\n",
    "* **Internal nodes** are $H$ applied to the concatenation of their sons' hashes.\n",
    "* If data is corrupted (e.g., data B), it invalidates the hash of its leaf and all nodes up the branch to the root.\n",
    "* The root hash can be stored safely. The work to obtain the root is approximately $2N$ hash function evaluations, where $N$ is the number of leaves.\n",
    "\n",
    "### Blockchain Basics (Pages 27-47)\n",
    "\n",
    "### Definition and Hash Pointers (Pages 27-28)\n",
    "A **blockchain** is a digitized, decentralized, public ledger of all cryptocurrency transactions. It is a distributed database that maintains a continuously growing list of records, called **blocks**.\n",
    "\n",
    "The blocks are linked using a **Hash Pointer (HP)**, which is a **tamper-evident data pointer**. An HP includes a pointer to where the information is stored and a cryptographic hash of that information. This allows retrieval of the data and verification that it hasn't changed. \n",
    "\n",
    "### The Chain of Blocks (Pages 29-32)\n",
    "In the blockchain structure, **each block has a Hash Pointer to the previous block**. \n",
    "* If data in block $i$ is tampered with, its hash changes, requiring the hash pointer in block $i+1$, $i+2$, and so on to be tampered with.\n",
    "* This makes the block addition process tamper-free.\n",
    "\n",
    "### Decentralization and Network (Pages 33-34)\n",
    "The blockchain functions as a decentralized database, providing:\n",
    "* **Consistency:** Information is a shared, continually reconciled, distributed database.\n",
    "* **Robustness:** No centralized version exists for a hacker to corrupt.\n",
    "* **Availability:** Data is stored by millions of computers (nodes) simultaneously and is publicly verifiable. \n",
    "\n",
    "\n",
    "A **Node** is a computer connected to the network that validates and relays data (e.g., transactions). Every node gets a copy of the blockchain and is an \"administrator,\" making the network decentralized.\n",
    "\n",
    "### Nakamoto Consensus (Pages 35-37)\n",
    "To decide which chain to trust in the event of a fork, the **longest blockchain has consensus**. \n",
    "* The system assumes adding a block is **computationally expensive** and that most nodes are honest.\n",
    "* A shorter blockchain is ignored, as it was likely created later or is out of sync.\n",
    "* A transaction is considered **\"accepted\"** (immutable) if it is **buried deep enough** (e.g., several blocks deep).\n",
    "\n",
    "### Double Spending and Proof of Work (PoW) (Pages 38-45)\n",
    "**Double spending** is the result of successfully spending digital money more than once. Protection against this involves verifying that the input for a transaction has not previously been spent.\n",
    "\n",
    "The challenge is preventing the **Sybil attack**, where a malicious actor creates many fake IDs cheaply to distribute a fraudulent fork.\n",
    "\n",
    "This is solved by making block addition **computationally expensive** using **Proof of Work (PoW)**.\n",
    "* The puzzle is **hard to solve** when adding a block but **easy to verify**.\n",
    "* The puzzle is solved by inserting a **nonce** into the header such that $H(\\text{header})$ starts with $n$ zeros. A malicious actor cannot redo the work on their fork (adding $N+1$ blocks) faster than all other miners add blocks on top of the accepted block.\n",
    "\n",
    "### Blockchain Details: Users and Mining (Pages 46-59)\n",
    "\n",
    "### Transactions and Users (Pages 50-54)\n",
    "A **user** is someone who can transfer money and is represented by a wallet, which is a pair of keys: **(sk: private key, pk: public key)**.\n",
    "\n",
    "A **transaction** format is `([input transactions], [output identity pk, how much], signature)`.\n",
    "* The transaction is signed with the user's private key ($sk$).\n",
    "* It specifies the output recipients by their public keys.\n",
    "* **All money from input transactions must be used**.\n",
    "* The output contains a **challenge script** (locking script or scriptPubKey) with the spending conditions.\n",
    "* The script is encoded in a **non-Turing complete language** to protect miners against DOS attacks (e.g., infinite loops).\n",
    "\n",
    "### Block and Transaction Organization (Pages 55-58)\n",
    "Transactions are received continuously and organized into a **Merkle tree**. \n",
    "* The **Root Hash (Tx\\_Root)** of the Merkle tree is part of the new block header, impacting the next block's hash.\n",
    "* The Merkle tree is efficient because it can be computed as transactions are received, and there's no need to recompute the hash of all transactions when one is received.\n",
    "\n",
    "### Miners and Incentives (Page 59)\n",
    "A **miner's job** is to verify transactions, compute the Merkle root hash, solve the PoW puzzle (by finding the nonce), and broadcast the new header.\n",
    "\n",
    "Miners are incentivized by:\n",
    "* **Block Reward:** Bitcoins granted by default for finding the nonce/new block.\n",
    "* **Fees:** A difference between transaction input and output that is paid to the winning miner.\n",
    "* The process involves a vast amount of energy waste.\n",
    "\n",
    "## Basics on Ethereum Smart Contracts (Pages 61-73)\n",
    "\n",
    "### Smart Contracts (Pages 61-63)\n",
    "**Smart Contracts** are computer protocols that facilitate, verify, or enforce the negotiation or performance of a contract, or that make a contractual clause unnecessary. They automatically enforce obligations, often summarized as \"**code is law**\".\n",
    "\n",
    "Smart contracts are the main building blocks of **Ethereum**.\n",
    "* A contract is a computer program that lives inside the distributed Ethereum network.\n",
    "* It has its own Ether balance, memory, and code.\n",
    "* Ethereum uses **Turing complete contracts**.\n",
    "\n",
    "### Execution and Gas (Page 64)\n",
    "A contract can be activated and run by sending a transaction that funds it with **Ether (ETH)**.\n",
    "* The contract runs for a time dependent on how much **gas** (a unit of ETH) is paid. ETH fees go to the winning miner.\n",
    "* Each miner runs the smart contract and produces the same output. Other miners validate the result when the winning miner publishes the block.\n",
    "\n",
    "### Blockchain Use Cases and Characteristics (Pages 71-73)\n",
    "Blockchain use cases fall into categories like **Smart Contracts** (e.g., Escrow, Wagers, Digital Rights), **Digital Currency** (e.g., Global Payments, Microfinance), **Securities** (e.g., Debt, Equity, Derivatives), and **Record Keeping** (e.g., Healthcare, Title Record, Voting). \n",
    "\n",
    "\n",
    "Blockchains are characterized by being a **Public ledger system**, **Distributed**, **Secure & reliable**, and **Immutable**.\n",
    "\n",
    "A five-point test for using a blockchain includes:\n",
    "1.  Are there **multiple parties** in the ecosystem?\n",
    "2.  Is establishing **trust** between all parties an issue?\n",
    "3.  Is it critical to have a **tamper-proof permanent record**?\n",
    "4.  Are we securing the **ownership or management of a finite resource**?\n",
    "5.  Does this ecosystem benefit from improved **transparency**?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4ae53",
   "metadata": {},
   "source": [
    "# LECTURE 28/10/2025\n",
    "\n",
    "# Distributed Algorithms \n",
    "\n",
    "We have seen how to fix all of the problems that were introduced with distributed communicatoin but not really how to implement those kind of solution.\n",
    "\n",
    "Here are some of the algorithms we have seen:\n",
    "\n",
    "**FIFO-broadcast**\n",
    "```\n",
    "on initialisation do\n",
    "    sendSeq:=0; delivered:=<0,0,...,0>; buffer:={}\n",
    "end on\n",
    "\n",
    "on request to broadcast m at node Ni do\n",
    "    send(i,sendSeq,m) via reliable broadcast\n",
    "    sendSeq:=sendSqe+1\n",
    "end on\n",
    "\n",
    "on receiving msg from reliable multicast at node Ni do\n",
    "    buffer:=buffer U {msg}\n",
    "    while exist sender, m. (sender, delivered[sender],m) in buffer do\n",
    "        deliver m to the application\n",
    "        delivered[sender]:=delivered[sender]+1\n",
    "    end while\n",
    "end on\n",
    "```\n",
    "\n",
    "\n",
    "All the algorithm we have seen assume that each process has an unique ID.\n",
    "\n",
    "## The Model\n",
    "The system is represented as an undirected graph $G=(V,E)$:\n",
    "* **Nodes ($V$):** Processes with $n=|V|$.\n",
    "* **Edges ($E$):** Communication channels.\n",
    "* **Knowledge:** Processes know their neighbors (local ports) but not the full global topology.\n",
    "* **IDs:** Algorithms typically assume unique IDs, though randomization is used when IDs are absent or symmetry needs breaking.\n",
    "\n",
    "---\n",
    "\n",
    "## Synchronous Algorithms\n",
    "In this model, computation proceeds in global rounds.\n",
    "\n",
    "### A. Leader Election (Symmetry Breaking)\n",
    "If processes are indistinguishable (no unique IDs), deterministic leader election is impossible due to symmetry.\n",
    "* **Solution:** Processes choose random IDs from a large range $\\{1, ..., r\\}$.\n",
    "* **Probability:** The probability of any two processes picking the same ID is $1/r$.\n",
    "* **Algorithm:** Processes exchange random IDs. If the maximum ID is unique, that node wins. Otherwise, the process repeats.\n",
    "\n",
    "### B. Maximal Independent Set (MIS)\n",
    "\n",
    "**The Problem:** Select a subset of nodes $S$ such that:\n",
    "1.  **Independent:** No two neighbors are both in $S$.\n",
    "2.  **Maximal:** No nodes can be added to $S$ without violating independence (distinct from *Maximum* size).\n",
    "\n",
    "**Luby‚Äôs Algorithm:**\n",
    "The algorithm operates in phases consisting of 2 rounds each.\n",
    "\n",
    "* **Initialization:**\n",
    "    * Set `status = active`.\n",
    "    * Set `just_joined = false`.\n",
    "\n",
    "* **Round $2 \\cdot i$ (Comparison Step):**\n",
    "    * If `status == active`:\n",
    "        1.  Choose a random value: $\\underline{val} = rand(1, n^5)$.\n",
    "        2.  Send $\\underline{val}$ to all neighbors in $N$.\n",
    "        3.  Wait to receive values from active neighbors.\n",
    "        4.  **Check:** Is $\\underline{val} >$ all received values?\n",
    "\n",
    "* **Round $2 \\cdot i + 1$ (Decision Step):**\n",
    "    * **If Check Passed (Winner):**\n",
    "        1.  Trigger output `in`.\n",
    "        2.  Set `just_joined = true`.\n",
    "        3.  Set `status = sleep`.\n",
    "    * **Notification:**\n",
    "        * If `just_joined == true`, send `joined` message to all neighbors.\n",
    "        * Reset `just_joined = false`.\n",
    "    * **Elimination:**\n",
    "        * If a neighbor sent a `joined` message:\n",
    "            1.  Trigger output `out`.\n",
    "            2.  Set `status = inactive`.\n",
    "\n",
    "### C. Breadth-First Spanning Trees (BFST)\n",
    "\n",
    "**The Problem:** Construct a tree rooted at a distinguished vertex $V_0$. A node at distance $d$ from the root must appear at depth $d$ in the tree.\n",
    "\n",
    "**Simple BFS Algorithm:**\n",
    "* **Initialization:** Only the root $V_0$ is marked. It sends a search message to neighbors.\n",
    "* **Execution:** If an unmarked node $i$ receives a search message from node $j$:\n",
    "    1.  Marks itself.\n",
    "    2.  Sets $j$ as its parent ($parent = j$).\n",
    "    3.  Sends the message to its own neighbors in the next round.\n",
    "\n",
    "**Complexity:**\n",
    "* **Time:** Proportional to the graph diameter (number of rounds).\n",
    "* **Message Complexity:** Each edge carries a message at most once, so complexity is $O(E)$.\n",
    "\n",
    "### D. Graph Coloring\n",
    "**The Problem:** Assign colors so no two neighbors share the same color.\n",
    "**Greedy Algorithm:**\n",
    "* Nodes determine colors based on local information.\n",
    "* In a round, if a node's ID is smaller than all its active neighbors, it picks the smallest valid color:\n",
    "    $$color = \\min\\_allowed(forbidden)$$\n",
    "* It sends this color to neighbors, who add it to their `forbidden` set.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Asynchronous Setting\n",
    "In asynchronous systems, there are no global rounds; messages arrive at unpredictable times.\n",
    "\n",
    "* **BFST Anomaly:** The simple flooding algorithm used in synchronous systems fails. A message traveling a \"slow\" short path might arrive *after* a message traveling a \"fast\" long path, resulting in a tree path longer than the shortest path.\n",
    "* **Correction Strategy:**\n",
    "    * Nodes track \"hop distance\" from the root.\n",
    "    * If a node receives a message offering a shorter path than its current parent, it updates its parent and propagates the new distance to its neighbors.\n",
    "    * The system eventually stabilizes to a valid BFST.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
