{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca55a7b0",
   "metadata": {},
   "source": [
    "# LECTURE 09/09/2025\n",
    "\n",
    "# QUESTIONS\n",
    "\n",
    "1. Perform all the preprocessing steps for data tokenisation, normalisation, and stopword removal in the following text. \n",
    "Further determine the type/token ratio, and rank the words based on their frequencies:\\\n",
    "\n",
    "\"Hi there! Did you hear about the new café that just opened downtown?\" asked Sarah.\\\n",
    "\"No, I haven't. What's so special about it?\" replied John, looking curious.\\\n",
    "\"It's supposed to have the best espresso in town—people are raving about it!\" Sarah responded enthusiastically.\\\n",
    "\"Really? I’m always on the lookout for good coffee. When should we check it out?\" John asked.\\\n",
    "\"How about this Saturday? I heard they have a live jazz band in the evening,\" Sarah suggested.\\\n",
    "\"That sounds fantastic! I’m in. Let’s meet there at 7 PM, then,\" John agreed.\\\n",
    "\"Perfect! I'll text you the address later. By the way, did you see the new movie trailer for 'The Last Horizon'? It looks amazing, don't you think?\" Sarah inquired.\\\n",
    "\"Oh, I did! It looks intense. I can’t wait to see it,\" John said with excitement.\\\n",
    "\n",
    "2. Stem the following words using Porter Stemmer\n",
    "   1. Excitement\n",
    "   2. Misunderstanding\n",
    "   3. Transcontinental\n",
    "   4. Substitutional\n",
    "3. Determine the Levenshtein distance for the following:\\\n",
    "“Anthropomorphization” and “Anthropomorphism\"\\\n",
    "“Overcompensation\" and \"Overcompensate”\\\n",
    "“Counterproductive\" and \"Counterproductivity”\n",
    "4. Determine the Hamming distances for the following:\n",
    "M4ch1n3L3arn1ng and M4ch1n3L3arN1ng\n",
    "GATTACA and GATCTAC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561acb1",
   "metadata": {},
   "source": [
    "# ANSWERS\n",
    "\n",
    "1.Tokenization\n",
    "First and foremost we need to remove the special character such as: \",\", \"!\", etc...\\\n",
    "Then we'll need to normalize the text to lowercase.\n",
    "It will result in this list:\n",
    "\n",
    "['hi', 'there', 'did', 'you', 'hear', 'about', 'the', 'new', 'cafe', 'that', 'just', 'opened', 'downtown', 'asked', 'sarah', 'no', 'i', 'havent', 'whats', 'so', 'special', 'about', 'it', 'replied', 'john', 'looking', 'curious', 'its', 'supposed', 'to', 'have', 'the', 'best', 'espresso', 'in', 'town', 'people', 'are', 'raving', 'about', 'it', 'sarah', 'responded', 'enthusiastically', 'really', 'i’m', 'always', 'on', 'the', 'lookout', 'for', 'good', 'coffee', 'when', 'should', 'we', 'check', 'it', 'out', 'john', 'asked', 'how', 'about', 'this', 'saturday', 'i', 'heard', 'they', 'have', 'a', 'live', 'jazz', 'band', 'in', 'the', 'evening', 'sarah', 'suggested', 'that', 'sounds', 'fantastic', 'i’m', 'in', 'let’s', 'meet', 'there', 'at', '7', 'pm', 'then', 'john', 'agreed', 'perfect', 'ill', 'text', 'you', 'the', 'address', 'later', 'by', 'the', 'way', 'did', 'you', 'see', 'the', 'new', 'movie', 'trailer', 'for', 'the', 'last', 'horizon', 'it', 'looks', 'amazing', 'dont', 'you', 'think', 'sarah', 'inquired', 'oh', 'i', 'did', 'it', 'looks', 'intense', 'i', 'can’t', 'wait', 'to', 'see', 'it', 'john', 'said', 'with', 'excitement']\n",
    "\n",
    "After this we'll need to remove common stopwords:\\\n",
    "[a, about, an, are, as, at, be, by, for, from, how, i, in, is, it, on, that, the, this, to, was, what, when, where, who, will, with, you]\n",
    "\n",
    "Now the list results in:\n",
    "['hi', 'hear', 'new', 'cafe', 'just', 'opened', 'downtown', 'asked', 'sarah', 'no', 'havent', 'whats', 'special', 'replied', 'john', 'looking', 'curious', 'its', 'supposed', 'best', 'espresso', 'town', 'people', 'raving', 'sarah', 'responded', 'enthusiastically', 'really', 'i’m', 'always', 'lookout', 'good', 'coffee', 'should', 'we', 'check', 'john', 'asked', 'saturday', 'heard', 'they', 'have', 'live', 'jazz', 'band', 'evening', 'sarah', 'suggested', 'sounds', 'fantastic', 'i’m', 'let’s', 'meet', '7', 'pm', 'then', 'john', 'agreed', 'perfect', 'ill', 'text', 'address', 'later', 'way', 'did', 'see', 'new', 'movie', 'trailer', 'last', 'horizon', 'looks', 'amazing', 'dont', 'think', 'sarah', 'inquired', 'oh', 'did', 'looks', 'intense', 'can’t', 'wait', 'see', 'john', 'said', 'excitement']\n",
    "\n",
    "Now we can calculate the type/token ratio:\\\n",
    "Types: 62\\\n",
    "Tokens: 74\\\n",
    "Type/Token ratio = 62/74 = 0.8378\n",
    "\n",
    "Now we can rank the words based on their frequencies:\n",
    "- sarah: 4\n",
    "- john: 4\n",
    "- asked: 2\n",
    "- did: 2\n",
    "- i’m: 2\n",
    "- looks: 2\n",
    "- new: 2\n",
    "- see: 2\n",
    "\n",
    "The other words are all 1 occurrence each\n",
    "\n",
    "2.Using Porter's Stemmer to calculate the m\n",
    "\n",
    "Excitement\\\n",
    "Removal of suffix -ment\\\n",
    "E X C I T E\\\n",
    "V C C V C V -> m = 2\n",
    "\n",
    "Misunderstanding\\\n",
    "Removal of the prefix Mis-\\\n",
    "Removal of the suffix -ing\\\n",
    "U N D E R S T A N D\\\n",
    "V C C V C C C V C C -> m = 3\n",
    "\n",
    "Transcontinental\\\n",
    "Removal of the prefix Trans-\\\n",
    "Removal of the suffix -al\\\n",
    "C O N T I N E N T\\\n",
    "C V C C V C V C -> m = 3\n",
    "\n",
    "Substitutional\\\n",
    "Removal of the suffix -al\\\n",
    "S U B S T I T U T I O N\\\n",
    "C V C C C V C V C V V C -> m = 4\n",
    "\n",
    "3.Calculate the Levenshtein distance\n",
    "\n",
    "“Anthropomorphization” and “Anthropomorphism\"\\\n",
    "Anthropomorphi is shared in these words so we'll need to substitute\\\n",
    "z -> s\\\n",
    "a -> m\\\n",
    "And delete \"tion\"\\\n",
    "Levenshtein distance = 6 (2 substitutions and 4 deletions)\n",
    "\n",
    "“Overcompensation\" and \"Overcompensate”\\\n",
    "Overcompensat is shared so we'll substitute\\\n",
    "i -> e\\\n",
    "And delete \"on\"\\\n",
    "Levenshtein distance = 3 (1 substitution and 2 deletions)\n",
    "\n",
    "“Counterproductive\" and \"Counterproductivity\"\\\n",
    "Counterproductiv is shared so we'll substitute\\\n",
    "e -> i\\\n",
    "And add \"ty\"\\\n",
    "Levenshtein distance = 3 (1 substitution and 2 additions)\n",
    "\n",
    "4.Determine Hamming distance\n",
    "\n",
    "M4ch1n3L3arn1ng\\\n",
    "M4ch1n3L3arN1ng -> distance = 1 (differs at the n before the 1 it's case sensitive)\n",
    "\n",
    "\n",
    "\n",
    "GATTACA\\\n",
    "GATCTAC -> distance = 4 (differs at the last 4 characters of the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf8243",
   "metadata": {},
   "source": [
    "# LECTURE 16/09/2025\n",
    "\n",
    "## EXERCISE\n",
    "\n",
    "Your task is to build an information retrieval system for a small collection of documents. The system\n",
    "should:\n",
    "1. Calculate the TF-IDF for each document in the corpus.\n",
    "2. Use the Vector Space Model (VSM) to rank documents based on their cosine similarity to a\n",
    "given query.\n",
    "3. Incorporate query expansion using relevant terms from feedback.\\\n",
    "Corpus of Documents:\n",
    "   1. Document 1:\n",
    "    \"Machine learning models are powerful tools for data-driven decision-making.\"\n",
    "   2. Document 2:\n",
    "    \"Deep learning, a subset of machine learning, has revolutionized many industries.\"\n",
    "   3. Document 3:\n",
    "    \"Neural networks are essential for deep learning, which is a branch of artificial intelligence.\"\n",
    "   4. Document 4:\n",
    "    \"Machine learning can be applied to both supervised and unsupervised tasks.\"\n",
    "   5. Document 5:\n",
    "    \"Data science involves machine learning, statistics, and big data technologies.\"\n",
    "Task:\n",
    "You are given the following query:\n",
    "● Query: \"machine learning for decision-making\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c4eed",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "### 1. TF-IDF Calculation\n",
    "\n",
    "First, we process the documents and calculate the Term Frequency (TF) and Inverse Document Frequency (IDF) for each unique term.\n",
    "\n",
    "* **Term Frequency (TF)**: The number of times a term appears in a document.\n",
    "* **Inverse Document Frequency (IDF)**: A measure of how important a term is across the whole collection of documents.\n",
    "\n",
    "---\n",
    "\n",
    "#### A. Term Frequency (TF) Table\n",
    "\n",
    "This table shows the raw count of each term in each document.\n",
    "\n",
    "| Term              | D1  | D2  | D3  | D4  | D5  |\n",
    "| :---------------- | :-: | :-: | :-: | :-: | :-: |\n",
    "| machine           | 1   | 1   | 0   | 1   | 1   |\n",
    "| learning          | 1   | 2   | 1   | 1   | 1   |\n",
    "| decision-making   | 1   | 0   | 0   | 0   | 0   |\n",
    "| models            | 1   | 0   | 0   | 0   | 0   |\n",
    "| deep              | 0   | 1   | 1   | 0   | 0   |\n",
    "| data              | 0   | 0   | 0   | 0   | 2   |\n",
    "| ... *(and so on)* |     |     |     |     |     |\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Inverse Document Frequency (IDF) Calculation Examples\n",
    "\n",
    "The formula is $IDF(t) = \\ln(\\frac{N}{df_t})$, where $N=5$ (total documents) and $df_t$ is the number of documents containing the term $t$.\n",
    "\n",
    "* **Example for \"decision-making\" (a rare term):**\n",
    "    * Appears in only 1 document, so $df_t = 1$.\n",
    "    * $IDF = \\ln(\\frac{5}{1}) = \\ln(5) \\approx 1.609$. This high score reflects its importance.\n",
    "\n",
    "* **Example for \"machine\" (a common term):**\n",
    "    * Appears in 4 documents, so $df_t = 4$.\n",
    "    * $IDF = \\ln(\\frac{5}{4}) = \\ln(1.25) \\approx 0.223$. This lower score indicates it's less unique.\n",
    "\n",
    "* **Example for \"learning\" (a ubiquitous term):**\n",
    "    * Appears in all 5 documents, so $df_t = 5$.\n",
    "    * $IDF = \\ln(\\frac{5}{5}) = \\ln(1) = 0$. A score of 0 means the term has no power to distinguish between documents in this specific collection.\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Final TF-IDF Vectors\n",
    "\n",
    "The final TF-IDF score is $TF \\times IDF$. These scores form the vector for each document.\n",
    "\n",
    "| Term              | D1    | D2    | D3    | D4    | D5    |\n",
    "| :---------------- | :---: | :---: | :---: | :---: | :---: |\n",
    "| **machine** | 0.223 | 0.223 | 0     | 0.223 | 0.223 |\n",
    "| **learning** | 0     | 0     | 0     | 0     | 0     |\n",
    "| **decision-making** | 1.609 | 0     | 0     | 0     | 0     |\n",
    "| **models** | 1.609 | 0     | 0     | 0     | 0     |\n",
    "| **data-driven** | 1.609 | 0     | 0     | 0     | 0     |\n",
    "| **deep** | 0     | 0.916 | 0.916 | 0     | 0     |\n",
    "| **data** | 0     | 0     | 0     | 0     | 3.218 |\n",
    "| ...               | ...   | ...   | ...   | ...   | ...   |\n",
    "\n",
    "***\n",
    "\n",
    "### 2. Initial Ranking with Vector Space Model\n",
    "\n",
    "We use the Vector Space Model (VSM) to rank documents by calculating the **cosine similarity** between the query vector ($Q$) and each document vector ($D$).\n",
    "\n",
    "\n",
    "\n",
    "The formula is:\n",
    "$$\\text{similarity}(Q, D) = \\frac{Q \\cdot D}{\\|Q\\| \\cdot \\|D\\|}$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Calculation Example: `sim(Q, D1)`\n",
    "\n",
    "Let's break down the calculation for the query and Document 1.\n",
    "\n",
    "1.  **Dot Product ($Q \\cdot D1$):** We multiply the matching term scores and add them up.\n",
    "    * The only matching terms with non-zero scores are 'machine' and 'decision-making'.\n",
    "    * $(0.223 \\times 0.223) + (1.609 \\times 1.609) = 0.0497 + 2.5889 = 2.6386$\n",
    "\n",
    "2.  **Vector Magnitudes ($\\|Q\\|$ and $\\|D1\\|$):** We find the square root of the sum of the squares of all term scores for each vector.\n",
    "    * $\\|Q\\| = \\sqrt{0.223^2 + 1.609^2} = \\sqrt{2.6386} \\approx 1.624$\n",
    "    * $\\|D1\\| = \\sqrt{0.223^2 + 1.609^2 + 1.609^2 + ...} = \\sqrt{12.9942} \\approx 3.605$\n",
    "\n",
    "3.  **Final Score:** Divide the dot product by the product of the magnitudes.\n",
    "    * $$\\frac{2.6386}{1.624 \\times 3.605} = \\frac{2.6386}{5.8545} \\approx \\bf 0.451$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Initial Ranking\n",
    "\n",
    "After performing this calculation for all documents, we get the following scores:\n",
    "\n",
    "1.  **Document 1** (Score: 0.451)\n",
    "2.  **Document 2** (Score: 0.010)\n",
    "3.  **Document 4** (Score: 0.009)\n",
    "4.  **Document 5** (Score: 0.007)\n",
    "5.  **Document 3** (Score: 0.000)\n",
    "\n",
    "As expected, **Document 1** is ranked highest.\n",
    "\n",
    "***\n",
    "\n",
    "### 3. Query Expansion and Final Ranking\n",
    "\n",
    "To improve results, we use relevance feedback from the top document (D1) to expand the query.\n",
    "\n",
    "* **Original Query:** \"machine learning decision-making\"\n",
    "* **Expanded Query (Q_exp):** \"machine learning decision-making **models data-driven**\"\n",
    "\n",
    "We then recalculate the cosine similarity with this new, more descriptive query vector.\n",
    "\n",
    "---\n",
    "\n",
    "#### Calculation Example: `sim(Q_exp, D1)`\n",
    "\n",
    "Let's break down the new calculation with the expanded query ($Q_{exp}$).\n",
    "\n",
    "1.  **Dot Product ($Q_{exp} \\cdot D1$):** We now have more matching terms.\n",
    "    * $(0.223 \\times 0.223) + (1.609 \\times 1.609) + (1.609 \\times 1.609) + (1.609 \\times 1.609) = 7.8164$\n",
    "\n",
    "2.  **Vector Magnitudes ($\\|Q_{exp}\\|$ and $\\|D1\\|$):** The magnitude of D1 is unchanged, but the query vector's magnitude increases.\n",
    "    * $\\|Q_{exp}\\| = \\sqrt{0.223^2 + 1.609^2 + 1.609^2 + 1.609^2} = \\sqrt{7.8164} \\approx 2.796$\n",
    "    * $\\|D1\\| \\approx 3.605$ (same as before)\n",
    "\n",
    "3.  **Final Score:**\n",
    "    * $$\\frac{7.8164}{2.796 \\times 3.605} = \\frac{7.8164}{10.0796} \\approx \\bf 0.775$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Ranking\n",
    "\n",
    "1.  **Document 1** (Score: 0.775)\n",
    "2.  **Document 2** (Score: 0.006)\n",
    "3.  **Document 4** (Score: 0.005)\n",
    "4.  **Document 5** (Score: 0.004)\n",
    "5.  **Document 3** (Score: 0.000)\n",
    "\n",
    "The query expansion successfully **amplified the relevance score of Document 1** from 0.451 to 0.775, confirming its top position with much greater certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d7d53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77f1c0e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "503803b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57371f30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "919e68b5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
