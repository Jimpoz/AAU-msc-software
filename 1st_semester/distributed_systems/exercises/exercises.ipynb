{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f49f0f6",
   "metadata": {},
   "source": [
    "# 08/09/2025\n",
    "\n",
    "## EXERCISES:\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. How might the clocks in two computers that are linked by a local network be\n",
    "synchronized without reference to an external time source? What factors limit the\n",
    "accuracy of the procedure you have described? How could the clocks in a large number of computers connected by the Internet be synchronized? Discuss the accuracy of that procedure.\n",
    "\n",
    "2. What is the man disadvantage of distributed systems which exploit the infrastructure offered by the Internet? How can this be overcome?\n",
    "\n",
    "\n",
    "3. The host computers used in peer-to-peer systems are often simply desktop computers in users’ offices or homes. What are the implications of this for the availability and security of any shared data objects that they hold and to what extent can any weaknesses be overcome through the use of replication?\n",
    "\n",
    "\n",
    "4. There exist services (e.g.: Network Time Protocol service) that can be used to synchronize computer clocks. Explain why, even with these service, no guaranteed bound is given for the difference between two clocks\n",
    "\n",
    "\n",
    "\n",
    "5. Speedup Calculation\n",
    "A program spends 60% of its execution time in a part that can be parallelized. The rest (40%) must remain sequential.\n",
    "According to Amdahl's Law, what is the maximum theoretical speedup if the parallel part is executed on:\n",
    "a) 2 processors\n",
    "b) 4 processors\n",
    "c) An infinite number of processors\n",
    "\n",
    "6. Finding the Parallel Fraction\n",
    "An application achieves a speedup of 5 when running on 8 processors. Use Amdahl's Law to determine the fraction of the program that was parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3561113",
   "metadata": {},
   "source": [
    "### ANSWERS\n",
    "\n",
    "1. If two computers are connected through a local network, they can synchronize their clocks without relying on an external time source by exchanging their current time values. To achieve accurate synchronization, the network latency must be measured. By estimating the message transmission delay, the difference between the clocks can be calculated and one machine’s clock adjusted accordingly.\n",
    "\n",
    "2. A major disadvantage of distributed systems is the dependency between components—failure in one can affect the whole system. This issue can be mitigated through redundancy, ensuring backup resources are available when failures occur.\n",
    "\n",
    "3. Host computers in homes or offices are typically just desktop machines. This setup results in poor availability and security since only a single device is used. To improve reliability, redundancy can be applied by duplicating resources digitally.\n",
    "\n",
    "4. Perfect time measurement is fundamentally impossible for digital devices. Even with synchronization using atomic clocks, absolute precision cannot be achieved due to unavoidable latencies in communication.\n",
    "\n",
    "5. Amdahl’s Law is given by:\n",
    "   $$\n",
    "   S(N) = \\frac{1}{(1 - P) + \\frac{P}{N}}\n",
    "   $$\n",
    "   For a parallel fraction $P = 0.60$:\n",
    "   - With $N = 2$ processors:\n",
    "     $$\n",
    "     S(2) = \\frac{1}{(1 - 0.60) + \\frac{0.60}{2}} = 1.43\n",
    "     $$\n",
    "   - With $N = 4$ processors:\n",
    "     $$\n",
    "     S(4) = \\frac{1}{(1 - 0.60) + \\frac{0.60}{4}} = 1.82\n",
    "     $$\n",
    "   - With infinitely many processors:\n",
    "     $$\n",
    "     S(\\infty) = \\frac{1}{1 - P} = \\frac{1}{0.40} = 2.5\n",
    "     $$\n",
    "\n",
    "6. Again, by Amdahl’s Law:\n",
    "   $$\n",
    "   S(N) = \\frac{1}{(1 - P) + \\frac{P}{N}}\n",
    "   $$\n",
    "   Given $S(N) = 5$ and $N = 8$:\n",
    "   $$\n",
    "   5 = \\frac{1}{(1 - P) + \\frac{P}{8}}\n",
    "   $$\n",
    "   Rearranging:\n",
    "   $$\n",
    "   (1 - P) + \\frac{P}{8} = \\frac{1}{5}\n",
    "   $$\n",
    "   $$\n",
    "   1 - \\frac{8P}{8} + \\frac{P}{8} = 0.2\n",
    "   $$\n",
    "   $$\n",
    "   1 - \\frac{7P}{8} = 0.2\n",
    "   $$\n",
    "   $$\n",
    "   0.8 = \\frac{7P}{8}\n",
    "   $$\n",
    "   $$\n",
    "   P = \\frac{0.8 \\times 8}{7} = \\frac{6.4}{7} \\approx 0.914\n",
    "   $$\n",
    "   Hence, the parallel fraction is approximately **91.4%**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcaf61",
   "metadata": {},
   "source": [
    "# LECTURE 09/09/2025\n",
    "\n",
    "# EXERCISES\n",
    "\n",
    "# QUESTIONS\n",
    "\n",
    "- Exercise 1:\n",
    "Explain under which assumptions the fail-recovery and the fail-silent models are similar\n",
    "in (note that in both models any process can commit omission faults).\n",
    "- Exercise 2:\n",
    "Show how to make a stubborn point-to-point link by calling an instance of a fair-loss link\n",
    "API.\n",
    "- Exercise 3:\n",
    "Describe the implementation of a perfect failure detector for a synchronous system\n",
    "- Exercise 4:\n",
    "Does the following statement satisfy the synchronous-computation assumption? \n",
    "On my server, no request ever takes more than 1 week to be processed.\n",
    "- Exercise 5:\n",
    "Is it possible to design a perfect failure detector for byzantine faults?\n",
    "- Exercise 6:\n",
    "Assume you have provided a cloud application alone, and aim to provide an SLA for\n",
    "potential customers. Would you offer 90%, 99%, or 99.9% uptime per month to the\n",
    "customer? If you give a 99% per month SLA, how many crashes of your application do\n",
    "you expect to have to deal with in a month, and how quickly do you think this can be\n",
    "dealt with?\n",
    "Provide a detailed calculation of how many minutes of downtime each option allows\n",
    "you to fix issues.\n",
    "- Exercise 7\n",
    "MS Azure SLA for VMs: https://www.azure.cn/en-us/support/sla/virtual-machines/\n",
    "How much time do they get to fix an issue on their side for each SLA listed there? How\n",
    "do you think they expect to achieve such high numbers. What do you think is their\n",
    "sampling frequency (must search for this online)?\n",
    "- Exercise 8\n",
    "Consider a network as seen here:\n",
    "\n",
    "![image](../images/Screenshot%202025-09-09%20at%2009.35.19.png)\n",
    "\n",
    "In this network, what (minimum) present of messages of node 1 to node 2 go through\n",
    "node 6?\n",
    "- Exercise 8.1\n",
    "Which links would you add so that this network can tolerate any two node crashes\n",
    "without getting any partitions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71444efc",
   "metadata": {},
   "source": [
    "# ANSWERS\n",
    "\n",
    "1. The crash recovery and crash silent are similar due to the fact that in both cases you're not sure if a process is going to fail. A crash silent event halts the process but the failure is not reliably detected while a crash recovery may fail and will recover later. So based on this assumption we can say that in both cases there's no certainty that a process will actually run.\n",
    "\n",
    "---\n",
    "\n",
    "2. To create a stubborn point-to-point link using a fair-loss link API, we can implement a mechanism that continuously retransmits messages until an acknowledgment is received.\n",
    "A stuborn link is built upon a fair-loss link and it guarantees reliability. Implementing stubborn links can be done using a “Retransmit Forever” algorithm that builds upon an underlying fair-loss link. We know that the fair-loss link will drop a certain percentage of messages, but that some messages do reach their destination. If we keep retransmitting all messages sent, in the limit we overcome any possible dropped messages and can guarantee that every message is delivered.\n",
    "\n",
    "```{r, tidy=FALSE, eval=FALSE, highlight=FALSE }\n",
    "Implements:\n",
    "    StubbornPointToPointLink, instance S\n",
    "\n",
    "Uses:\n",
    "    FairLossLink, instance F\n",
    "\n",
    "upon event <S, Init> do\n",
    "    retransmissionSet := ∅; \\\\messages that need sending\n",
    "    delivered := ∅; \\\\prevents duplication\n",
    "    starttimer(Δ);\n",
    "\n",
    "\\\\sending a message m to a process q\n",
    "\\\\it basically adds to the set that message\n",
    "upon event <S, Send | q, m> do\n",
    "    retransmissionSet := retransmissionSet ∪ {(q, m)};\n",
    "\n",
    "\\\\for all the pairs of messages initiates an event that uses F to send a message to p with msg labeled as DATA\n",
    "upon event <Timeout> do\n",
    "    forall (p, msg) ∈ retransmissionSet do\n",
    "        trigger <F, Send | p, [DATA, msg]>;\n",
    "    starttimer(Δ);\n",
    "\n",
    "\\\\when a message is delivered to q using f it initiates a send through F with the acknowledged label and if the pair is not in delivered then it adds it to the set\n",
    "\\\\it then executes the deliver with S of the m message to the q process\n",
    "upon event <F, Deliver | q, [DATA, m]> do\n",
    "    trigger <F, Send | q, [ACK, m]>;\n",
    "    if (q, m) ∉ delivered then\n",
    "        delivered := delivered ∪ {(q, m)};\n",
    "        trigger <S, Deliver | q, m>;\n",
    "\n",
    "\\\\if the message has been acknowledged then it removes the pair from the set and it wont retransmit\n",
    "upon event <F, Deliver | q, [ACK, m]> do\n",
    "    retransmissionSet := retransmissionSet \\ {(q, m)};\n",
    "    \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "3. A perfect failure detector for synchronous system\n",
    "\n",
    "```{r, tidy=FALSE, eval=FALSE, highlight=FALSE }\n",
    "\n",
    "Implements:\n",
    "    PerfectFailureDetector, instance P\n",
    "\n",
    "Uses:\n",
    "    PointToPointLinks, instance pl\n",
    "\n",
    "// δ is the known maximum message delay\n",
    "\n",
    "upon event <P, Init> do\n",
    "    alive := Π; // Set of all processes, initially all are considered alive\n",
    "    starttimer(2δ);\n",
    "\n",
    "upon event <Timeout> do\n",
    "    for all p ∈ alive do\n",
    "        trigger <pl, Send | p, [HEARTBEAT]>;\n",
    "    alive := ∅;\n",
    "    starttimer(2δ);\n",
    "\n",
    "upon event <pl, Deliver | q, [HEARTBEAT]> do\n",
    "    if q ∉ alive then\n",
    "        alive := alive ∪ {q};\n",
    "        // A process is detected as crashed if it was previously alive\n",
    "        // but did not send a new heartbeat in the last round.\n",
    "        // This algorithm implicitly detects crashes by omission from the 'alive' set,\n",
    "        // but we can make it explicit for clarity.\n",
    "        // For simplicity, we consider a process \"crashed\" if it's not in the 'alive' set.\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "4. Yes, that statement satisfies the synchronous-computation assumption.\\\n",
    "The core requirement for the synchronous model is that there is a known and finite upper bound on the time it takes to perform a computation.\n",
    "\n",
    "---\n",
    "\n",
    "5. Perfect failure detector for a byzantine fault is generally considered impossible as it is a condition where a distributed system presents different symptoms to different users,including imperfect information on whether a system component has failed.\n",
    "\n",
    "---\n",
    "\n",
    "6. \n",
    "\n",
    "For a cloud application provided by a single person, I would offer a **99% uptime SLA** per month.\n",
    "\n",
    "This choice represents a realistic balance. A **90% SLA** allows for over 3 days of downtime a month, which is commercially unacceptable and would drive customers away. Conversely, a **99.9% SLA** is extremely demanding for a solo operator, requiring sophisticated automated failover systems and 24/7 availability that are difficult to manage alone. A **99% uptime** is a professional standard that assures customers of reliability while providing a practical buffer for unforeseen issues, maintenance, and manual recovery.\n",
    "\n",
    "\n",
    "\n",
    "**Downtime Calculations**\n",
    "\n",
    "To calculate the allowed downtime, we first find the number of minutes in an average month: 30.4375 days/month * 24 hours/day * 60 minutes/hour = 43,830 minutes/month\n",
    "Using this, the allowed monthly downtime for each SLA is:\n",
    "\n",
    "* **90% Uptime:**\n",
    "    ```\n",
    "    10% downtime * 43,830 min = 4383 minutes (or 73.05 hours)\n",
    "    ```\n",
    "* **99% Uptime:**\n",
    "    ```\n",
    "    1% downtime * 43,830 min = 438.3 minutes (or 7.31 hours)\n",
    "    ```\n",
    "* **99.9% Uptime:**\n",
    "    ```\n",
    "    0.1% downtime * 43,830 min = 43.83 minutes (or 0.73 hours)\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "#### **Managing a 99% SLA**\n",
    "\n",
    "With **438.3 minutes** of downtime allowed per month, the number of crashes you can handle depends entirely on your **Mean Time To Recovery (MTTR)**—the average time it takes to fix a problem from the moment you're alerted.\n",
    "\n",
    "Let's assume your MTTR is 2 hours (120 minutes). This includes the time to be notified, diagnose the problem, and deploy a fix. In this scenario, you could handle: 438.3 minutes / 120 minutes/crash ≈ 3.65 crashes per month\n",
    "\n",
    "So, you could manage about **3 to 4 significant outages** per month.\n",
    "\n",
    "Achieving a 2-hour MTTR as a solo provider is challenging. It requires:\n",
    "* **Excellent Monitoring:** Automated alerts sent directly to your phone.\n",
    "* **Rapid Diagnosis:** Good logging and diagnostic tools to find the root cause quickly.\n",
    "* **Efficient Resolution:** A streamlined process for deploying fixes or restarting services.\n",
    "\n",
    "If a single crash takes longer to fix (e.g., it happens overnight while you're asleep), it could consume your entire monthly downtime budget in one incident.\n",
    "\n",
    "---\n",
    "\n",
    "7. \n",
    "Based on the provided Azure SLA for Virtual Machines, here is the allowed time they have to fix an issue for different configurations. The calculations use the same average of `43,830` minutes per month.\n",
    "\n",
    "* **99.9% (Single VM with Premium Storage):**\n",
    "    ```\n",
    "    0.1% * 43,830 = 43.83 minutes/month\n",
    "    ```\n",
    "* **99.95% (Multiple VMs in an Availability Set):**\n",
    "    ```\n",
    "    0.05% * 43,830 = 21.92 minutes/month\n",
    "    ```\n",
    "* **99.99% (Multiple VMs across Availability Zones):**\n",
    "    ```\n",
    "    0.01% * 43,830 = 4.38 minutes/month\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "#### **Achieving High Availability**\n",
    "\n",
    "Azure achieves these incredibly high uptime numbers through massive investment in **redundancy and automation**.\n",
    "\n",
    "1.  **Redundancy at All Levels:**\n",
    "    * **Hardware:** Individual servers have redundant power supplies, network cards, and RAID storage.\n",
    "    * **Availability Sets:** This feature ensures your VMs are placed in different server racks within the same datacenter. This protects against rack-level failures like a faulty power distribution unit or network switch.\n",
    "    * **Availability Zones:** This is the highest level of protection. It places your VMs in physically separate datacenters within the same region, each with independent power, cooling, and networking. This protects against catastrophic events like a fire or total power loss to an entire building.\n",
    "\n",
    "2.  **Automation and Orchestration:**\n",
    "    * **Health Monitoring:** Systems constantly monitor the health of the underlying hardware.\n",
    "    * **Automated Failover:** If a server is about to fail, Azure can automatically live-migrate a VM to a healthy server with no downtime.\n",
    "    * **Rapid Provisioning:** Software-defined infrastructure allows for the near-instant deployment of new resources to replace failed ones.\n",
    "\n",
    "3.  **Expert Staff:** Microsoft employs a global team of engineers working 24/7/365 in Network Operations Centers (NOCs) to respond to incidents immediately.\n",
    "\n",
    "\n",
    "\n",
    "#### **Sampling Frequency**\n",
    "\n",
    "Microsoft Azure's standard **sampling frequency for VM health metrics is one minute**.\n",
    "\n",
    "The SLA document specifies that \"Downtime is defined as two consecutive minutes of no External Connectivity.\" This implies that their monitoring system must be checking the VM's status at least once per minute to be able to reliably detect a two-minute continuous outage. This one-minute interval is a standard feature of Azure Monitor for collecting performance and health metrics from virtual machines.\n",
    "\n",
    "---\n",
    "\n",
    "8. 1->3->6->8->2\n",
    "   1->4->6->8->2\n",
    "\n",
    "These are the shortest paths from 1 to 2 passing through the node 6.\n",
    "\n",
    "   1. In order to not create partitions for the network to work we'll need to add links to every pair of nodes that are not adiacent to eachother.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5a863",
   "metadata": {},
   "source": [
    "# LECTURE 15/09/2025\n",
    "\n",
    "# EXERCISES\n",
    "# QUESTIONS\n",
    "\n",
    "- Exercise 1\n",
    "Write the exact conversion formula between Unix time and Iso 8601\n",
    "- Exercise 2\n",
    "Give (at least) one reason why not to just switch all timing information regarding software, computers, and distributed systems on the ISO 8601 time.\n",
    "- Exercise 3\n",
    "Assume that the function currentTime() returns the time of your system, which periodically updates via an NTP server and that the function fun() takes 50ms to run on your machine. \n",
    "You run the following code:\n",
    "long startTime = System.currentTime();\n",
    "fun();\n",
    "long endTime = System.currentTime();\n",
    "long elapsedTime = endTime-startTime;\n",
    "Give bounds on elapsedTime.\n",
    "- Exercise 4\n",
    "Calculate all the happens-before pairs in the Lamport diagrams on slides 20 and 21.\n",
    "- Exercise 5\n",
    "Prove that for two events a and b in a distributed system, Either a -> b, b ->a, or a || b.\n",
    "- Exercise 6\n",
    "A relation R is a strict partial order if it is irreflexive ( not exists event a such that (a,a) is in R) and \n",
    "transitive for any events a, b, c : if (a,b) in R and (b,c) in R implies (a,c) in R. \n",
    "(These two conditions also imply that R is asymmetric, i.e. that for any events a,b, if (a,b) in R then (b,a) not in R.) \n",
    "Prove that the happens-before relation is a strict partial order.\n",
    "You may assume that any two nodes are a nonzero distance apart, as well as the physical principle that information\n",
    "cannot travel faster than the speed of light.\n",
    "- Exercise 7\n",
    "Assume an NTP client which at time t1 = 3:31am sends a request to an NTP server, which arrives at time 3:30 (at the server side). \n",
    "The server then responds with a message \"response(3:31, 3:30, 3:31), which is received by the NTP client at time 3:34. \n",
    "What is the estimated network delay, and what is the estimated clock skew of the client?\n",
    "Discuss what a clock correction would look like based on the skew you calculated.\n",
    "- Exercise 8\n",
    "In a fail-stop model, which of the following properties are safety properties?\n",
    "  1. every process that crashes is eventually detected;\n",
    "  2. no process is detected before it crashes;\n",
    "  3. no two processes decide differently;\n",
    "  4. no two correct processes decide differently;\n",
    "  5. every correct process decides before t time units;\n",
    "  6. if some correct process decides then every correct process decides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca989af8",
   "metadata": {},
   "source": [
    "# ANSWERS\n",
    "\n",
    "1. A direct conversion is not a single formula but an algorithm. Here are the mathematical steps to perform the conversions.\n",
    "\n",
    "---\n",
    "\n",
    "## Unix Time to ISO 8601\n",
    "\n",
    "This process decomposes the total number of seconds into date and time components.\n",
    "\n",
    "Let $t_{unix}$ be the input Unix timestamp.\n",
    "\n",
    "First, separate the total seconds into full days and the remaining seconds within the current day.\n",
    "* Seconds per day: $S_{day} = 86400$\n",
    "* Number of full days since epoch: $D_{total} = \\lfloor \\frac{t_{unix}}{S_{day}} \\rfloor$\n",
    "* Remaining seconds into the current day: $s_{rem} = t_{unix} \\pmod{S_{day}}$\n",
    "\n",
    "The remaining seconds are used to find the hour, minute, and second.\n",
    "* Hour ($H$): $H = \\lfloor \\frac{s_{rem}}{3600} \\rfloor$\n",
    "* Minute ($M_{time}$): $M_{time} = \\lfloor \\frac{s_{rem} \\pmod{3600}}{60} \\rfloor$\n",
    "* Second ($S$): $S = s_{rem} \\pmod{60}$\n",
    "\n",
    "This is an iterative algorithm to find the year, month, and day from $D_{total}$. You must account for leap years.\n",
    "\n",
    "**a. Find the Year ($Y$)**\n",
    "Start with the epoch year and subtract days for each year until the remainder is found.\n",
    "1.  Set $Y = 1970$ and $D_{rem} = D_{total}$.\n",
    "2.  Define a function for days in a given year:\n",
    "    $DaysInYear(Y) = \\begin{cases} 366 & \\text{if } (Y \\pmod 4 = 0 \\text{ and } Y \\pmod{100} \\neq 0) \\text{ or } (Y \\pmod{400} = 0) \\\\ 365 & \\text{otherwise} \\end{cases}$\n",
    "3.  Loop: While $D_{rem} \\ge DaysInYear(Y)$:\n",
    "    * $D_{rem} = D_{rem} - DaysInYear(Y)$\n",
    "    * $Y = Y + 1$\n",
    "\n",
    "**b. Find the Month ($M_{date}$) and Day ($D$)**\n",
    "The final $D_{rem}$ is the day of the year (from 0 to 364 or 365). Convert this to a month and day by iterating through the month lengths of year $Y$.\n",
    "\n",
    "Combine the calculated components into the ISO 8601 string format: **`YYYY-MM-DDTHH:MM:SSZ`**.\n",
    "\n",
    "---\n",
    "\n",
    "2. \n",
    "We don't use the human-readable ISO 8601 format for all system timing because it's highly inefficient for machines. Computers use compact numerical timestamps, like Unix time, because they are significantly faster to process, require less storage, and make calculations instantaneous. This numerical approach is also essential for internal system logic that relies on monotonic clocks, which only move forward and cannot be represented by a wall-clock standard like ISO 8601.\n",
    "\n",
    "---\n",
    "\n",
    "3. \n",
    "long startTime = System.currentTime(); -> it reads 10:00:00:00\\\n",
    "fun(); -> it becomes 10:00:00:50\\\n",
    "long endTime = System.currentTime(); -> it reads 10:00:00:50\\\n",
    "long elapsedTime = endTime-startTime; -> it's 50ms\\\n",
    "\n",
    "---\n",
    "\n",
    "4. \n",
    "\n",
    "Slide 20:\n",
    "![image](../images/Screenshot%202025-09-16%20at%2009.59.17.png)\n",
    "\n",
    "happens-before relations:\n",
    "\n",
    "bob1 -> bobf1 -> carolrec1\\\n",
    "bob1 -> bobf1 -> bobs1 -> alicerec3\n",
    "\n",
    "bob2 -> bobs2 -> carolrec2\\\n",
    "bob2 -> bobs3 -> alicerec2\n",
    "\n",
    "bob3 -> bobf3 -> carolrec3\\\n",
    "bob3 -> bobf4 -> alicrec1\n",
    "\n",
    "\n",
    "\n",
    "Slide 21:\n",
    "![image](../images/Screenshot%202025-09-16%20at%2009.59.27.png)\n",
    "\n",
    "happents-before relations:\n",
    "\n",
    "t1 -> t2\\\n",
    "t3 -> t4\\\n",
    "t3 -> t5\\\n",
    "t1 -> t6 (through m1)\n",
    "\n",
    "we can say that t2 happens before t6 due to the fact that t1 -> t2 and then through t2 it will send other messages\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "5. \n",
    "a->b is a happens-before relation and if there is a path, because of the local ordering, massage passing etc. this relation is exclusive\\\n",
    "b->a same thing\\\n",
    "a||b if neither a->b or b->a then there are no relations hence they are concurrent\n",
    "\n",
    "---\n",
    "\n",
    "6. \n",
    "As said in the previous exercise the happens-before relation is a strict partial order.\\\n",
    "e.g.\\\n",
    "given the relations a->b, b->c it implies the relation a->c because of the local order\\\n",
    "also if a->b it means there exists a path that connects a to b following that specific order, making it also an exclusive one\\\n",
    "in fact if there is a a->b relation in a happens-before relation then there won't be a b->a relation\\\n",
    "\n",
    "---\n",
    "\n",
    "7. \n",
    "t1 = 3:31\\\n",
    "t2 = 3:30\\\n",
    "t3 = 3:31\\\n",
    "t4 = 3:34\n",
    "\n",
    "network delay = δ = (t4 − t1) − (t3 − t2) = (3 min) - (1 min) = 180 - 60 = 120s\\\n",
    "\n",
    "clock skew = θ = T3 + (δ ÷ 2) − T4 = 3:31 + 60s - 3:34 = -2 min -> -120s\\\n",
    "\n",
    "The estimated clock skew is -2 minutes (-120,000 ms). The negative value means the client's clock is 2 minutes ahead of the server's clock.\n",
    "\n",
    "---\n",
    "\n",
    "8.  \n",
    "In a fail-stop model, a **safety property** is a property that states \"something bad never happens.\" It can be proven false by observing a finite execution of the system. \n",
    "In contrast, a liveness property states that \"something good will eventually happen,\" which requires an infinite observation to prove false.\n",
    "\n",
    "Based on this, the safety properties from your list are: **2, 3, 4, and 5**.\n",
    "\n",
    "* **1. Every process that crashes is eventually detected.**\n",
    "    * **Liveness:** This is a classic liveness property. It states that something good (detection) will *eventually* happen. To prove it false, you would have to observe the system forever to confirm that a crashed process is *never* detected.\n",
    "\n",
    "* **2. No process is detected before it crashes.**\n",
    "    * **Safety:** This property specifies that a \"bad thing\" (a false detection) should never happen. You can prove it false in a finite execution: if you observe a process being marked as crashed while it is still running, the property is violated.\n",
    "\n",
    "* **3. No two processes decide differently.**\n",
    "    * **Safety:** This is an agreement or consistency property. The \"bad thing\" is having two different decisions. If process A decides '1' and process B decides '0' at any point, you have a finite trace that violates the property.\n",
    "\n",
    "* **4. No two correct processes decide differently.**\n",
    "    * **Safety:** This is a slightly weaker version of the previous property, but it is still a safety property for the same reason. The \"bad thing\" is disagreement between correct processes. A finite observation of two correct processes making different decisions is enough to prove it false.\n",
    "\n",
    "* **5. Every correct process decides before t time units.**\n",
    "    * **Safety:** This is a timeliness property, often called \"bounded liveness,\" but it fits the definition of a safety property. The \"bad thing\" is a correct process not having decided by time `t`. You can prove this property false by simply running the system for `t` units of time. If a correct process hasn't decided by then, the property is violated.\n",
    "\n",
    "* **6. If some correct process decides then every correct process decides.**\n",
    "    * **Liveness:** This property states that if a good thing happens (one process decides), then another good thing (all other correct processes deciding) must *eventually* happen. To prove it false, you would need to see one correct process decide and then watch for an infinite amount of time to confirm another correct process *never* decides.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578bf3c8",
   "metadata": {},
   "source": [
    "# LECTURE 16/09/2025\n",
    "## QUESTIONS \n",
    "\n",
    "- Exercise 1:\n",
    "Write pseudocode for Lamport clock algorithm in the API style from lecture 1 (see slide example for failure detector for formatting ideas).\n",
    "- Exercise 2:\n",
    "Calculate the Lamport clocks for the Lamport diagram in slide 9. Explain if they can be used to fix the issue presented in that slide.\n",
    "- Exercise 3:\n",
    "Prove that If $a \\implies b$ then $ LT(a) < LT(b)$.\n",
    "- Exercise 4:\n",
    "Assume a set of nodes, and two events occurring in two different nodes with $LT(e1) = 5$ and $LT(e2) = 3$. What can you say about the causality of events $e1$ and $e2$?\n",
    "- Exercise 5:\n",
    "In slide 13 list all possible FIFO broadcast allowed orders of the messages a, b, and c. In slide 14 list all possible causal broadcast allowed orders of the messages a, b, and c.\n",
    "- Exercise 6:\n",
    "Prove that all Causal broadcast protocols are also FIFO broadcast protocols.\n",
    "- Exercise 7:\n",
    "Write pseudocode for a causal broadcast. You will be needed to keep track of some vector clock structure.\n",
    "- Exercise 8:\n",
    "Can we devise a uniform reliable broadcast algorithm with an eventually perfect failure detector but without assuming a majority of correct processes?\n",
    "- Exercise 9:\n",
    "Compare the causal delivery property the following property: “If a process delivers messages m1 and m2, and m1 → m2, then the process must deliver m1 before m2.”\n",
    "- Exercise 10:\n",
    "Calculate the vector clocks of the following distributed system. Show the contents of two different exchanged messages. Give an example of 2 different pairs of concurrent events.\n",
    "![image](../images/Screenshot%202025-09-16%20at%2010.22.22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ce4f8",
   "metadata": {},
   "source": [
    "## ANSWERS\n",
    "\n",
    "1. \n",
    "\n",
    "```\n",
    "implements:\n",
    "    LamportClock, instance L\n",
    "\n",
    "upon event <L, init> do\n",
    "    clock := 0\n",
    "\n",
    "// Correct: Increment clock before sending\n",
    "upon event <L, send | p, m>\n",
    "    clock := clock + 1\n",
    "    trigger <Network, Send | p, [m, clock]> // Sending over a network\n",
    "\n",
    "// Corrected logic for receiving a message\n",
    "upon event <Network, Deliver | p, [m_content, sender_clock]>\n",
    "    clock := max(clock, sender_clock) + 1\n",
    "    trigger <L, Deliver | p, m_content> \n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "2. \n",
    "![image](../images/Screenshot%202025-09-16%20at%2009.59.17.png)\n",
    "\n",
    "Solution:\n",
    "\n",
    "![image](../images/lamport_diagram_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "3. \n",
    "a -> b is a happens-before relation which is a strict partial order meaning that it is an exclusive relation and that a happens before b\\\n",
    "therefore LT(a) < LT(b)\n",
    "\n",
    "---\n",
    "\n",
    "4. \n",
    "Given a set of nodes and two nodes with different event so that:\n",
    "LT(e1)=5 and LT(e2)=3 we could say that one happens before the other but we do not know from the given information if they are somehow connected to one other, thus\n",
    "LT(e2) < LT(e1) but it does not imply that e1 -> e2 meaning they are concurrent events.\n",
    "\n",
    "---\n",
    "\n",
    "5. \n",
    "\n",
    "Slide 13 picture\n",
    "![image](../images/Screenshot%202025-09-16%20at%2011.25.41.png)\n",
    "\n",
    "All possible solutions:\n",
    "User A: Must deliver a before c (FIFO). Must also deliver b before c (Causality). And since a → b, the only valid order is (a, b, c).\\\n",
    "User B: Must respect the global causal chain a → b → c. The only valid order is (a, b, c).\\\n",
    "User C: Must respect the global causal chain a → b → c. The only valid order is (a, b, c).\n",
    "\n",
    "Slide 14 picture\n",
    "![image](../images/Screenshot%202025-09-16%20at%2011.25.57.png)\n",
    "\n",
    "All possible solutions:\\\n",
    "The rules we derived are:\\\n",
    "a must be delivered before b.\\\n",
    "a must be delivered before c.\n",
    "\n",
    "This means a must always be the first message delivered. Since b and c can be delivered in any order relative to each other, the two possible total orders are:\\\n",
    "(a, b, c)\\\n",
    "(a, c, b)\n",
    "\n",
    "---\n",
    "\n",
    "6. \n",
    "\n",
    "All causal broadcast protocols are also FIFO broadcast protocols because the \"happens-before\" relationship of causality inherently includes the sequence of events at a single process.\n",
    "\n",
    "---\n",
    "\n",
    "7. \n",
    "\n",
    "```\n",
    "Implements:\n",
    "CausalBroadcast, instance cb.\n",
    "\n",
    "Uses:\n",
    "BestEffortBroadcast, instance beb.\n",
    "\n",
    "upon event < cb, Init > do\n",
    "VC := [0] * N;  // N is the number of processes\n",
    "pending := ∅;\n",
    "\n",
    "upon event < cb, Broadcast | m > do\n",
    "VC[self] := VC[self] + 1;\n",
    "trigger < beb, Broadcast | [VC, m] >;\n",
    "\n",
    "upon event < beb, Deliver | p, [VC_m, m] > do\n",
    "pending := pending ∪ {(p, VC_m, m)};\n",
    "loop do\n",
    "exists (p', VC_m', m') ∈ pending such that\n",
    "VC_m'[p'] = VC[p'] + 1 and\n",
    "(∀ k ≠ p': VC_m'[k] ≤ VC[k]);\n",
    "if exists then\n",
    "pending := pending \\ {(p', VC_m', m')};\n",
    "VC[p'] := VC[p'] + 1;\n",
    "trigger < cb, Deliver | p', m' >;\n",
    "else\n",
    "break loop;\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "8. \n",
    "Can we devise a uniform reliable broadcast algorithm with an eventually perfect failure detector but without assuming a majority of correct processes?\n",
    "\n",
    "No. Uniform Reliable Broadcast requires Uniform Agreement: if any process delivers a message m, all correct processes must also deliver m.\n",
    "\n",
    "Without a majority, the network can partition into groups. A faulty process in one partition could deliver a message m that is never seen by the correct processes in another partition. This violates Uniform Agreement. An eventually perfect failure detector can't prevent this, as the partition and delivery can happen before the detector becomes accurate.\n",
    "\n",
    "---\n",
    "\n",
    "9. \n",
    "Compare the causal delivery property with the following property: “If a process delivers messages m1 and m2, and m1 → m2, then the process must deliver m1 before m2.”\n",
    "\n",
    "Let's call the standard definition Property 1 and the alternative one Property 2.\n",
    "\n",
    "The standard Property 1 is stronger because it includes a liveness guarantee. It means if broadcast(m1) → broadcast(m2), a process is not allowed to deliver m2 unless it has already delivered m1. This forces the delivery of the cause.\n",
    "\n",
    "Property 2 is only an ordering guarantee. It says that if a process happens to deliver both, the order must be correct. It doesn't force the process to deliver m1 just because m2 has arrived.\n",
    "\n",
    "In short, if m1 is lost but m2 arrives at a process:\n",
    "\n",
    "    Property 1 is violated: The process cannot deliver m2.\n",
    "\n",
    "    Property 2 is satisfied: Since the process never delivers both m1 and m2, the condition doesn't apply.\n",
    "---\n",
    "\n",
    "10. \n",
    "\n",
    "![img](../images/IMG_1030.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1efa82e",
   "metadata": {},
   "source": [
    "# LECTURE 23/09/2025\n",
    "\n",
    "## EXERCISES\n",
    "\n",
    "\n",
    "1. Give an example why the centralized algorithm (with leader and token) for Mutex does not satisfy the ordering property. \n",
    "What mechanism could the leader use to fix this? \n",
    "\n",
    "2. Give a summary for the total number of messages for Centralized, Token ring, Maekava, and decentralized (R &A)  algorithm for each one of: client delay , syncronization delay, bandwidth, and a major problem you know with this algorithm. \n",
    "Would you say token ring is \"more\" fault tolerant that leader token based algorithm? Explain why. Hint: Consider if processes can crash-recover what happens each time. \n",
    "\n",
    "3. If the whole system of clients and servers is fully synchronus and each process is single threaded, is mutual exclusion condition ME3, which specifies entry in happened-before order, relevant? Can it be mitigated without the algorithm changing? \n",
    "\n",
    "4. Give a formula for the maximum throughput of a mutual exclusion system in terms of the synchronization delay.\n",
    "\n",
    "5. Adapt the central server algorithm for mutual exclusion to handle the crash failure of any client (in any state), assuming that the server is correct and given a reliable failure detector. Comment on whether the resultant system is fault-tolerant. What would happen if a client that possesses the token is wrongly suspected to have failed?\n",
    "\n",
    "6. Give the module pseudocode for all 4 algorithms seen in the message parsing case. \n",
    "\n",
    "7. In a certain system, each process typically uses a critical section many times before another process requires it. Explain why Ricart and Agrawala’s multicast-based mutual exclusion algorithm is inefficient for this case, and describe how to improve its performance. Does your adaptation satisfy liveness condition ME2?\n",
    "\n",
    "8. Make a process based description of Dekkers algorithm. \n",
    "\n",
    "8.1. In dekker algorimth let process i request the critical resource CS and let a process j with j < i to request it when queue  = k ( k< j) and again when queue = j + 1. How many times before i can j be granted entrance to the resource? \n",
    "\n",
    "9. When we run the two following processes (initially all variables used are 0)\n",
    "\n",
    "P1 \t\t\t            P2\n",
    "\n",
    "1. x := 1\t\t        1. y := 1\n",
    "\n",
    "2. x := 2\t\t        2. b := x\n",
    "\n",
    "3. a := y\t\t        3. if b = 0 then\n",
    "\n",
    "4. if a = 0 then  \t    4. CS\n",
    "\n",
    "5.  if x=0\t\n",
    "\n",
    "6. \tCS\t\n",
    "\n",
    "\n",
    "under TSO, which values is it possible for P1 to read at line 4 and for P2 at line 2? \n",
    "Can both processes enter the CS at the same time?  Why yes or no? What if we replace line 5 for P1 with if x=1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c0a02",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "1. \n",
    "\n",
    "The algorithm with leader and token does not satisfy the order:\n",
    "![img](../images/IMG_0994.jpg)\n",
    "\n",
    "However this could be solved with either lamport logical clock algorithm:\n",
    "![img](../images/IMG_0995.png)\n",
    "\n",
    "This way the leader knows which request has been sent before due to the timestamp increasing\n",
    "\n",
    "2. \n",
    "Centralized: Low message overhead (3 per entry) but the leader is a single point of failure.  \n",
    "\n",
    "Token Ring: Variable delay to enter, constant background traffic. Failure of any node or token loss breaks the ring.  \n",
    "\n",
    "Ricart & Agrawala: High message overhead (2*(N−1) per entry) and poor scalability.  \n",
    "\n",
    "Maekawa: Moderate overhead (~3√N per entry) but can deadlock.  \n",
    "\n",
    "**Fault Tolerance:**  \n",
    "The Token Ring is generally *more fault-tolerant* than the centralized (leader-based) algorithm because no single node controls access. In a ring, a crashed node or lost token can be detected and recovered, while a centralized system halts if the leader fails.\n",
    "\n",
    "3. \n",
    "ME3 relevance in a fully synchronous, single-threaded system\n",
    "No — ME3 (entry in happened-before order) is largely irrelevant there. A fully synchronous system with single-threaded processes already enforces a global round/real-time ordering, so entries naturally follow the system schedule.\n",
    "Mitigation without algorithm change: use the system clock/round number or message timestamps to decide entry order; that requires no change to the mutual-exclusion protocol logic.\n",
    "\n",
    "4. \n",
    "Let $T_{sync}$ = synchronization delay (round-trip / negotiation cost) and $T_{cs}$ = critical-section duration.\n",
    "\n",
    "Maximum throughput (entries per unit time) is given by:\n",
    "$$\\text{Throughput}_{\\max} = \\frac{1}{T_{sync} + T_{cs}}$$\n",
    "\n",
    "If $T_{cs}$ is negligible, the formula simplifies to:\n",
    "$$\\text{Throughput}_{\\max} \\approx \\frac{1}{T_{sync}}$$\n",
    "\n",
    "5. \n",
    "Modify the server to maintain tokenOwner, a waiting queue, and a token epoch; on a failure report remove the client from the queue and if it was the tokenOwner increment epoch, clear tokenOwner, and grant a new token (with the new epoch) to the next requester.\n",
    "With a correct (reliable) failure detector this tolerates crash-stop clients; if a token holder is wrongly suspected and the server reissues a token, duplicate tokens (and thus a safety violation) can occur unless you add epoch/version checks or leases so recovered clients discard stale tokens.\n",
    "\n",
    "6. \n",
    "TODO\n",
    "\n",
    "\n",
    "7. \n",
    "Ricart & Agrawala's algorithm is inefficient in this scenario because it has a high fixed overhead. It always requires 2(N−1) messages for a process to enter the critical section, even when no other process is competing for it.\n",
    "The performance can be significantly improved by allowing a process to cache the permission to the critical section.\n",
    "The adapted rule is simple: When a process exits the critical section, it checks if it has deferred any requests.\n",
    "- If no other process is waiting, it keeps the permission and can re-enter immediately with zero message cost.\n",
    "- If another process has sent a REQUEST, it must release the lock by replying, following the original protocol.\n",
    "\n",
    "\n",
    "8. \n",
    "```\n",
    "Implements:\n",
    "    Dekker, instance D.\n",
    "\n",
    "upon event <Init, D> do:\n",
    "    flag := [FALSE, FALSE]        // flag[i] = TRUE if process i wants to enter CS\n",
    "    turn := 0                     // whose turn it is to enter the critical section\n",
    "    self_id := get_process_id()   // Assume this function returns 0 or 1\n",
    "    other_id := 1 - self_id\n",
    "\n",
    "upon event <D, request> do:\n",
    "    flag[self_id] := TRUE\n",
    "    while flag[other_id] do\n",
    "        if turn ≠ self_id then\n",
    "            flag[self_id] := FALSE\n",
    "            wait until turn = self_id\n",
    "            flag[self_id] := TRUE\n",
    "        end if\n",
    "    end while\n",
    "    trigger <D, enter>         // enter critical section\n",
    "\n",
    "upon event <D, release | > do:\n",
    "    turn := other_id\n",
    "    flag[self_id] := FALSE\n",
    "    trigger <D, exit>          // indicate critical section is released\n",
    "\n",
    "```\n",
    "\n",
    "9. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dedd17",
   "metadata": {},
   "source": [
    "# LECTURE 06/10/2025\n",
    "\n",
    "## EXERCISES\n",
    "\n",
    "1.    We said that consensus is not consistency. How can you implement consistency using consensus? Which kind of consistency did you implement?\n",
    "\n",
    "2.    Argue if non-Partition tolerant systems can still be considered distributed systems.\n",
    "\n",
    "3.    Consider slide 21 (interleavings)\n",
    "    .    Construct an interleaving that is not linearizable but sequentially consistent\n",
    "    .    Construct an interleaving that is not sequentially consistent\n",
    "    .    What is the minimum number of clients with which you can construct a non-sequentially consistent interleaving?\n",
    "\n",
    "4.    What is the message complexity of passive replication? What is its delay?\n",
    "\n",
    "5.    What is the meaning of \"Sacrifice linearizability => offload reads to backups!\"? What kind of consistency do you end up with?`\n",
    "\n",
    "6.    What is the message complexity of active replication? What is its delay?\n",
    "\n",
    "7.    So, why should somebody use active replication?\n",
    "\n",
    "8.    What is the message complexity in the case of the gossip architecture? What is its delay?\n",
    "\n",
    "9.    Read operations in the gossip architecture:\n",
    "    .    What happens when your second read operation ends on an outdated replica?\n",
    "\n",
    "10.    Write operations in the gossip architecture:\n",
    "    .    Should you apply all the updates in the log when you receive a read request?\n",
    "    .    In slide 37, we say \"actually, it uses an Executed operation table not to re-apply them, but keep them forever\". Why can't we delete the updates?\n",
    "\n",
    "11.    Chang-Roberts: how can you overcome a crash, after you detected it?\n",
    "\n",
    "12.    In the bully algorithm, why is safety broken if:\n",
    "    .    too tight deadline?\n",
    "    .    process IDs reappears?\n",
    "    .    system is not synchronous?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ad2ce",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "1. Consistency is not consensus\n",
    "\n",
    "Definition of consistency:\n",
    "It is a consistency model in distributed systems that ensures all operations across processes appear in a single, unified order\n",
    "\n",
    "Definition of consensus:\n",
    "It is the process of agreeing on a single data value among distributed processes or systems.\n",
    "\n",
    "To implement consistency using consensus, we can use a consensus algorithm to agree on the order of operations. Each process proposes its operation to the consensus algorithm, which then decides on a single operation to be executed next. This ensures that all processes see the same sequence of operations, achieving consistency.\n",
    "The kind of consistency is eventual consistency, given the fact that some can crash.\n",
    "\n",
    "2. \n",
    "Non-partition tolerant systems can't be considered distributed systems because network partitions are an unavoidable reality in distributed environments.\n",
    "\n",
    "3. \n",
    "\n",
    "Definition of interleaving:\n",
    "An interleaving is a sequence of operations from multiple processes that reflects the order in which they are executed in a concurrent system.\n",
    "\n",
    "3.1 - A distributed system can have different processes happening at one time, thus the model can be not linearizable as there isn't a specific time order to respect however the processes must respect the causal order.\n",
    "\n",
    "3.2 - \n",
    "\n",
    "3.3 - \n",
    "\n",
    "4. \n",
    "\n",
    "\n",
    "5. \n",
    "\n",
    "\n",
    "6. \n",
    "\n",
    "\n",
    "7. \n",
    "\n",
    "\n",
    "8. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0b439",
   "metadata": {},
   "source": [
    "# LECTURE 07/10/2025\n",
    "\n",
    "1. Why do you have non-POSIX primitives in GFS? In particular, which requirements led you to needing:\n",
    "- snapshot\n",
    "- record append\n",
    "\n",
    "2. Your GFS is configured with the usual chunk size (64MB). \n",
    "   Your GFS master is storing these metadata: /foo.txt (0x2ef0, 172.31.177.226:8081) (0x2551, 172.31.177.223:8082)\n",
    "                                              /bar.txt (0x144f ,172.31.177.223:8082) (0xaaaa, 172.31.177.226:8081) (0x9233, 172.31.177.226:8081)\n",
    "   Your client wants to access the following data in its filesystem. Please write down the queries that the client will issue to the GFS master, and what the GFS master will send back to the client.\n",
    "- /foo.txt, byte 1000\n",
    "- /bar.txt, first byte in its 66th MB\n",
    "- /foo.txt, first byte in its 130th MB\n",
    "\n",
    "3. If I decide to append 10 MBs to a file already 60MB long, what are the actions taken by the GFS master?\n",
    "\n",
    "4. We said that GFS uses some kind of passive replication.\n",
    "   Which characteristics of passive replication are respected by GFS implementation?\n",
    "   Which aspects of GFS implementation are not compliant with the usual passive replication approach?\n",
    "\n",
    "5. Regarding the fault tolerance of the GFS master, we talked about the operations log and the checkpoints.\n",
    "   Why do we need two different mechanisms?\n",
    "   Where are operations logs and checkpoints saved?\n",
    "   How do we decide which one to use, between operations log and checkpoints?\n",
    "\n",
    "6. How many computers are usually served by one Chubby cell?\n",
    "\n",
    "7. Why does Chubby use multi-paxos instead of paxos?\n",
    "\n",
    "8. In Chubby client sessions, \n",
    "   what happens to the client sessions when a client crashes? Why?\n",
    "   what happens to the client sessions when a master crashes? Why? (feel free to consider or ignore the \"jeaopardy\" mechanism)\n",
    "\n",
    "9. In BigTable,\n",
    "   why isn't the master serving meta-data to clients (such as on which tabletserver data are located)?\n",
    "   which operations are responsibility of the master?\n",
    "\n",
    "10. In BigTable, what do you get from Chubby when you look for data?\n",
    "\n",
    "11. Both GFS and Bigtable make the same core design choice – to have a single master. What are the repercussions of a failure of this single master in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137587c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
