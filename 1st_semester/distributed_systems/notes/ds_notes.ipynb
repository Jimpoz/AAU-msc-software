{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2983dd2",
   "metadata": {},
   "source": [
    "## **COURSE OVERVIEW**\n",
    "\n",
    "## **INTRO**\n",
    "### 1. Introduction\n",
    "## **Theory & Algorithms**\n",
    "### 1. Models of DS\n",
    "### 2. Time in DS\n",
    "### 3. Multicast\n",
    "### 4. Consensus\n",
    "### 5. Distributed Mutual Exclusion\n",
    "## **Application**\n",
    "### 7. Distributed Storage\n",
    "### 8. Distributed Computing\n",
    "### 9. Blockchains\n",
    "### 10. Peer-to-Peer Networking\n",
    "### 11. Internet of Things and Routing\n",
    "### 12. Distributed Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27860a15",
   "metadata": {},
   "source": [
    "# Lecture 08/09/2025\n",
    "\n",
    "## What is a distributed systems?\n",
    "A distributed system is one where hardware and software components in/on networked computers communicate and coordinate their activity only by passing messages.\n",
    "\n",
    "## CONCURRENCY\n",
    "\n",
    "Concurrency is the ability of a system to execute multiple tasks or processes simultaneously or at overlapping times, improving efficiency.\n",
    "However it can cause some problems such as:\n",
    "- Deadlocks and livelocks: these are conditions in which the processes do not make progress due to their circumstances.\n",
    "- Non-determinism: occurs when the output or behavior of a concurrent program differs for the same input, depending on the precise, unpredictable timing of events, such as thread interleaving or resource access.\n",
    "\n",
    "Other issues could rise from the absence of shared state that will eventually lead to:\n",
    "- Pass messages to synchronize: for example if 2 people have a shared resource and both are trying to access it at the same time an error could occur for one member of the party.\n",
    "- May not agree on time: \n",
    "\n",
    "Everything can fail in distributed systems:\n",
    "- Devices\n",
    "- Integrity of data\n",
    "- Network\n",
    "    - Security\n",
    "    - Man-in-the-Middle (MITM): attack occurs when an attacker intercepts and potentially alters communication between two legitimate parties, unbeknownst to them.\n",
    "    - Zibantine failure: is a condition of a system, particularly a distributed computing system, where a fault occurs such that different symptoms are presented to different observers, including imperfect information on whether a system component has failed.\n",
    "\n",
    "Distributed systems are used for domain, redundancy, and performance.\n",
    "\n",
    "---\n",
    "## Domain\n",
    "A domain is a specific area of knowledge or activity that a distributed system is designed to address.\n",
    "Some examples of domains are:\n",
    "- The internet\n",
    "- Wireless Mesh Networks\n",
    "- Industrail systems\n",
    "- Ledgers (bitcoin, ethereum)\n",
    "However these domains can encounter some limits that can be physical and logical.\n",
    "- Physical limits: are constraints imposed by the physical properties of the system's components or environment, such as hardware limitations, network bandwidth, latency, and geographical distribution.   \n",
    "- Logical limits: defined by its bounded context. This is a core concept from Domain-Driven Design (DDD). The logical limit of a domain in a distributed system is defined by its bounded context. This is a core concept from Domain-Driven Design (DDD), a software development approach that focuses on aligning software design with the business domain. A bounded context is an explicit boundary within a distributed system where a specific domain model and its language (ubiquitous language) are consistent and applicable.\n",
    "---\n",
    "## Redundancy\n",
    "A system with redundacy means that it has duplicate ocmponents, processes or data.\n",
    "Given these specifications the system will result:\n",
    "- Robust: more resilient to failure\n",
    "- Available: as in system availability which is measured in uptime.\n",
    "\n",
    "A system with redundancy can:\n",
    "- offer 99.9% uptime or \"five nines\": means it's designed to be operational and available 99.9% of the time, with a small amount of planned or unplanned downtime.\n",
    "- be a backup: in an active-passive configuration, the redundant server acts as a hot or cold backup. The primary server handles all the workload, while the backup server waits on standby. Given the duplication of information and components if one fails the other automatically takes over.\n",
    "- be a database: In an active-active configuration, all redundant servers are considered active and work together simultaneously. They are not simply waiting as a backup. Incoming requests are distributed among all the servers using a load balancer.\n",
    "- used in the banking sector: any downtime can lead to significant financial losses.\n",
    "\n",
    "---\n",
    "## Performance\n",
    "To ensure a performant system we need:\n",
    "- Economics\n",
    "- Scalability\n",
    "Here we talk about different topics such as:\n",
    "- Video streaming: requires a lot of procesing power\n",
    "- Cloud computing: Offers on-demand, scalable resources, eliminating the need for upfront hardware investment.\n",
    "- Supercomputers: Excel at massive, specialized calculations but are very expensive and not scalable for general-purpose use.\n",
    "- Many inexpensive vs many expensive specialized: Distributing workloads across many inexpensive machines is often more economical and scalable than using a few expensive, specialized ones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b169279",
   "metadata": {},
   "source": [
    "CHECK CONTENT\n",
    "\n",
    "# Lecture 09/09/2025\n",
    "\n",
    "# Models of distributed systems\n",
    "\n",
    "## Aspects of models\n",
    "\n",
    "Why do we build distributed systems?\n",
    "\n",
    "- **Inherent distribution**: By definition, distributed systems span multiple computers, often connected through networks such as telecommunications systems.\n",
    "- **Reliability**: Even if one node fails, the system as a whole can continue functioning, avoiding single points of failure.\n",
    "- **Performance**: Workloads can be shared among multiple machines, and data can be accessed from geographically closer nodes to reduce latency.\n",
    "- **Scalability for large problems**: Some datasets and computations are simply too large to fit into a single machine, requiring distributed processing.\n",
    "\n",
    "### Modelling the process – API Style\n",
    "\n",
    "A distributed system can be described in terms of modules that exchange **events** through well-defined interfaces:\n",
    "\n",
    "- **Event representation**:  \n",
    "  \\{Event\\_type | Attributes, …\\}  \n",
    "  or  \n",
    "  \\{Process\\_ID | Event\\_type | Attributes, …\\}\n",
    "\n",
    "- **Module behavior**:  \n",
    "  Each module reacts to incoming events and produces outputs according to specified rules:\n",
    "upon event {condition | Event | attributes} such that condition holds do\n",
    "perform some action\n",
    "\n",
    "\n",
    "Multiple modules together (one per process or subsystem) should collectively satisfy desired **global properties** (e.g., safety, liveness).\n",
    "\n",
    "### What we want/will make\n",
    "\n",
    "We aim to:\n",
    "- Design APIs for modules and prove that their composition satisfies global system properties.\n",
    "- Implement modules that guarantee **local properties**.\n",
    "- Use pseudocode and mathematics to formally demonstrate when such guarantees are possible—or prove impossibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Failures\n",
    "\n",
    "Failures are inevitable in distributed systems. They can arise due to hardware breakdowns, software bugs, network disruptions, or even human mistakes. Designing robust systems requires understanding different types of failures and strategies to mitigate them.\n",
    "\n",
    "### Types of failures\n",
    "\n",
    "1. **Crash-stop**: A process halts and all other processes can reliably detect the failure. *Easiest to handle.*\n",
    "2. **Crash-silent**: A process halts but failures cannot be detected reliably.\n",
    "3. **Crash-noisy**: Failures may be detected, but only with eventual accuracy (false positives or delays are possible).\n",
    "4. **Crash-recovery**: Processes may fail and later recover, rejoining the system. Requires care to avoid state inconsistencies.\n",
    "5. **Crash-arbitrary (Byzantine failures)**: Processes behave arbitrarily or maliciously, deviating from the protocol. *Hardest to handle.*\n",
    "6. **Randomized behavior**: Processes make decisions probabilistically. Correctness is argued via probability theory rather than strict guarantees.\n",
    "\n",
    "---\n",
    "\n",
    "## Communication\n",
    "\n",
    "Is communication always required? In distributed systems, yes—but it can be realized in different ways:\n",
    "\n",
    "- **Message passing**:\n",
    "1. Types of links and their potential failures.\n",
    "2. Network topology (commonly assumed fully connected).\n",
    "3. Routing algorithms for multi-hop communication.\n",
    "4. Broadcast and multicast primitives.\n",
    "\n",
    "- **Shared memory**:\n",
    "1. Which process can read or write to which location?\n",
    "2. How do we guarantee reading the *freshest* value? (Consistency models)\n",
    "\n",
    "### On types of links\n",
    "\n",
    "A **link** is a module implementing send/receive operations with certain properties.\n",
    "\n",
    "- **TCP/IP**: Enables reliable communication between a pair of nodes (or none).\n",
    "- **SSH**: Adds protection against corruption, interception, and tampering.\n",
    "\n",
    "**Network reliability models**:\n",
    "1. **Perfect links**: Reliable delivery, no duplication, no spurious messages.\n",
    "2. **Fair-loss links**: Messages may be lost occasionally, but infinitely many attempts guarantee eventual delivery; finite duplication possible.\n",
    "3. **Stubborn links**: Messages are retransmitted until delivery is guaranteed but still no creation (this model is built upon the fair-loss).\n",
    "4. **Logged-perfect links**: Perfect delivery with persistent logs for auditing/recovery.\n",
    "5. **Authenticated links**: Reliable delivery, no duplication, and sender authenticity.\n",
    "\n",
    "### Can networks fail?\n",
    "\n",
    "While TCP/IP and lower-level protocols often give us the illusion of **perfect links** and **fail-stop crashes**, failures still happen.\n",
    "\n",
    "- **Network partitions**: Occur when many links fail simultaneously, dividing the system into disconnected components. This is rare but catastrophic.\n",
    "\n",
    "### Crashes vs Failures\n",
    "\n",
    "Having discussed both **network** and **process** failures, it is important to distinguish between the two levels:\n",
    "\n",
    "- A **process can crash** (e.g., by crashing, halting, or misbehaving).  \n",
    "- A **system fails** when the combination of process crashes and communication assumptions no longer allows correct operation.\n",
    "\n",
    "For the remainder of our discussion, we usually assume **perfect links** (thanks to TCP/IP and lower-level reliability mechanisms). This means that:\n",
    "- Messages are delivered reliably,\n",
    "- No duplicates are created,\n",
    "- No spurious (phantom) messages appear.\n",
    "\n",
    "Under this assumption, we can define **system failure models** in terms of process behavior:\n",
    "\n",
    "- **Fail-stop system**: Processes may experience crash-stop failures, but links are perfect.  \n",
    "- More complex models (e.g., crash-recovery, Byzantine failures) are defined similarly, always considering both the **process failure type** and the **communication assumptions**.\n",
    "\n",
    "In short, a system failure model = (process failure model) + (assumed link properties).\n",
    "\n",
    "---\n",
    "\n",
    "## Timing\n",
    "\n",
    "Timing plays a central role in distributed systems, especially when considering **synchronization** and **failure detection**.\n",
    "\n",
    "- Systems may be **synchronous** (bounded delays) or **asynchronous** (no timing guarantees).\n",
    "- Links are still modeled as modules with send/receive properties.\n",
    "\n",
    "### Synchronous vs. Asynchronous Systems\n",
    "\n",
    "Distributed systems can be broadly classified according to their **timing assumptions**:\n",
    "\n",
    "1. **Asynchronous systems**:\n",
    "   - No bounds on message transmission delays.\n",
    "   - No assumptions about process execution speeds (relative speeds may differ arbitrarily).\n",
    "   - Failure detection is unreliable, since a slow process cannot be distinguished from a failed one.\n",
    "   - Coordination and ordering rely on **logical clocks** (e.g., Lamport clocks, vector clocks), rather than real time.\n",
    "\n",
    "2. **Synchronous systems**:\n",
    "   - Bounds exist on message transmission delays and process execution speeds.\n",
    "   - **Timed failure detection** is possible: if a message or heartbeat is not received within a known bound, a failure can be suspected reliably.\n",
    "   - Transit delays can be measured and incorporated into algorithms.\n",
    "   - Coordination can be based on **real-time clocks** rather than purely logical clocks.\n",
    "   - Performance is often analyzed in terms of **worst-case bounds**, since timing assumptions provide guarantees.\n",
    "   - Processes may maintain **synchronized clocks** (to some degree of precision), enabling algorithms such as consensus and coordinated actions.\n",
    "\n",
    "**Key question**: *Can processes in an asynchronous system with fair-loss links reach agreement (e.g., on coordinated attack time)?*\n",
    "\n",
    "### Proof via contradiction (Two Generals Problem)\n",
    "\n",
    "1. Assume a protocol exists where a fixed sequence of messages guarantees agreement.\n",
    "2. Consider the last message in this sequence that is successfully delivered.\n",
    "3. If this message is lost, the receiving general decides **not** to attack.\n",
    "4. But the sender cannot distinguish whether the message was delivered or lost, so must behave deterministically and decide the same action in both cases.\n",
    "5. This creates a contradiction: one general attacks, the other does not.  \n",
    " $\\Rightarrow$ Perfect agreement is impossible under these assumptions.\n",
    "\n",
    "### Which crash/link/timing assumptions implement distributed systems?\n",
    "\n",
    "A **failure detector** can be modeled as just another module that provides (possibly imperfect) information about which processes are alive. Different combinations of timing assumptions and failure detectors allow different guarantees in distributed systems.  \n",
    "\n",
    "### Example\n",
    "\n",
    "![image](../images/Screenshot%202025-09-09%20at%2009.56.39.png)\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "This algorithm describes a **Perfect Failure Detector** for distributed systems using a heartbeat mechanism.\n",
    "\n",
    "In short, here's what it does:\n",
    "\n",
    "1.  **Sends Heartbeats:** Periodically, on a **timeout**, every process sends a `HEARTBEATREQUEST` message to all other processes in the system.\n",
    "2.  **Waits for Replies:** It assumes no one is alive and waits for `HEARTBEATREPLY` messages. When a process receives a reply, it marks the sender as `alive`.\n",
    "3.  **Detects Failures:** At the next timeout, any process that has not sent a reply is considered to have **crashed**. The algorithm then triggers a `Crash` event for that process.\n",
    "\n",
    "Because it assumes **perfect communication links** (messages are never lost), this method guarantees that a non-responsive process has truly failed, making the failure detection \"perfect.\"\n",
    "\n",
    "### Network latency and bandwith\n",
    "\n",
    "When discussing communication performance, two key metrics matter:\n",
    "\n",
    "- **Latency**: The time it takes for a single message (or bit) to travel from sender to receiver.  \n",
    "- **Bandwidth**: The rate at which data can be transmitted, usually measured in bits per second (bps) or bytes per second (B/s).\n",
    "\n",
    "#### Physical Link\n",
    "Sometimes, surprisingly “low-tech” physical methods can provide high bandwidth, even if latency is poor:\n",
    "- **Hard drives in a van**  \n",
    "- Messengers carrying storage devices  \n",
    "- Smoke signals (extreme latency, minimal bandwidth)  \n",
    "- Radio signals or laser communication\n",
    "\n",
    "#### Network Links\n",
    "More conventional digital communication technologies include:\n",
    "- DSL (Digital Subscriber Line)  \n",
    "- Cellular data (e.g., 3G, 4G, 5G)  \n",
    "- Wi-Fi (various standards)  \n",
    "- Ethernet/fiber cables  \n",
    "- Satellite links  \n",
    "\n",
    "#### Latency examples\n",
    "1. Hard drives transported by van: $\\approx$ 1 day latency  \n",
    "2. Intra-continent fiber-optic cable: $\\approx$ 100 ms latency  \n",
    "\n",
    "#### Bandwidth examples\n",
    "1. Hard drives in a van: $\\frac{50 \\, \\text{TB}}{1 \\, \\text{day}}$ = **very high bandwidth** despite huge latency  \n",
    "2. 3G cellular network: $\\approx 1 \\, \\text{Mbit/s}$ bandwidth  \n",
    "\n",
    "---\n",
    "\n",
    "## Performance\n",
    "\n",
    "### Performance measures\n",
    "\n",
    "- **SLI (Service Level Indicator)**: What aspect of the system do we measure?  \n",
    "Examples: bandwidth, latency, fault tolerance, uptime, failure detection time.\n",
    "- **SLO (Service Level Objective)**: What target values do we aim for?  \n",
    "Example: latency < 200ms.\n",
    "- **SLA (Service Level Agreement)**: An SLO backed with contractual consequences.  \n",
    "Example: \"99% uptime, otherwise partial refund.\"\n",
    "\n",
    "Why should we study these?\n",
    "- Measuring means we can improve\n",
    "- Spend time improving when it is needed.\n",
    "- Reliability is kind of the point with distributed systems.\n",
    "\n",
    "### Reading SLAs\n",
    "\n",
    "When evaluating claims like *“This solution offers 99% uptime”*, consider:\n",
    "\n",
    "- **Sampling frequency**: How often is system availability checked?\n",
    "- **Responsibility scope**: Does the SLA cover only server uptime, or also account for client/network failures?\n",
    "- **Time interval**: Does 99% apply per day, per month, or per year?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c259a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d663c53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0599cc37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b71429",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "948eb7f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95896e72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be0e3f02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "395dea2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b3d905",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c775234f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eabe455b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
