{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4800eab8",
   "metadata": {},
   "source": [
    "# COURSE OVERVIEW\n",
    "\n",
    "# The core topics that will be covered in the course include:\n",
    "- Information retrieval, search, and ranking\n",
    "- Neural networks\n",
    "- Language models\n",
    "- Recommender systems\n",
    "- Network modelling and embedding\n",
    "\n",
    "# LECTURE 02/09/2025\n",
    "--- \n",
    "## What is the Web?\n",
    "The Web is a\n",
    "- global decentralized hypertext-based information system.\n",
    "- Hyperlinks are used to navigate from one document to another.\n",
    "- This information space build on a set of technical standards for\n",
    "the identification, retrieval and representation of content.\n",
    "---\n",
    "## World Wide Web (WWW)\n",
    "- The World Wide Web (referred to as WWW or W3 or\n",
    "simply Web), developed by Tim Berners-Lee in 1989 at\n",
    "CERN, Switzerland\n",
    "- The project document described a \"hypertext project\"\n",
    "called \"WorldWideWeb\" in which a \"web\" of\n",
    "\"hypertext documents\" could be viewed by\n",
    "“browsers”.[1]\n",
    "- First Website in 1991: info.cern.ch\n",
    "- First Webpage address:\n",
    "http://info.cern.ch/hypertext/WWW/TheProject.html\n",
    "- First Browser\n",
    "- In 1993: Graphical Web Browsers such as\n",
    "Mosiac and Netscape Navigator were made\n",
    "accessible outside of academia.\n",
    "- In 1994: W3C (World Wide Web Consortium)\n",
    "was founded by Tim Berners Lee. W3C\n",
    "publishes recommendations, that are\n",
    "considered web standards.\n",
    "- Web standards are blueprints –or building\n",
    "blocks– of a consistent and harmonious\n",
    "digitally connected world. They are\n",
    "implemented in browsers, blogs, search\n",
    "engines, and other software that power our\n",
    "experience on the web.\n",
    "---\n",
    "## World Wide Web - Data Model\n",
    "\n",
    "W3 data model enables:\n",
    "- Information need only be represented once, as a reference may be made instead of making a copy.\n",
    "- Links allow the topology of the information to evolve, so modelling the state of human knowledge at any time is without constraint.\n",
    "- The web stretches seamlessly from small personal notes on the local workstation to large databases on other continents.\n",
    "- Indexes such as phone books are presented as documents, and so may themselves be found by searches and/or following links.\n",
    "- The documents in the web do not have to exist as files; they can be \"virtual\" documents generated by a server in response to a query\n",
    "or document name. They can therefore represent views of databases, or snapshots of changing data (such as the weather forecasts,\n",
    "financial information, etc.).\n",
    "Advantages:\n",
    "- Information access doesn’t require expert knowledge\n",
    "- Information Retrieval via search engines\n",
    "---\n",
    "## World Wide Web - Components\n",
    "The World Wide Web (WWW) comprise of different components:\n",
    "- Identification: Universal Resource Identifiers (URIs)- Address system;\n",
    "globally unique identification of the web resources.\n",
    "For e.g., the URI of the main page for the first WWW project is\n",
    "http://info.cern.ch/hypertext/WWW/TheProject.html\n",
    "- Interaction: Hypertext Transfer Protocol (HTTP) - network protocol used\n",
    "for transferring information/interacting between the web resources. The\n",
    "data transferred can be plain text, hypertext, images, etc.\n",
    "- Content Format: Hypertext Markup Language (HTML) - a markup\n",
    "language, used to define the structure and the content of the webpage.\n",
    "HTML supports various content types, including text, images, video,\n",
    "audio, scripts, and hyperlinks for easy web resource access.\n",
    "---\n",
    "## Topology of the Web\n",
    "\n",
    "The web's structure can be broken down into three overlapping layers:\n",
    "1. Classic Document Web (Web 1.0): A network of static pages connected by hyperlinks. Its topology is a simple web of linked documents.\n",
    "2. Social & Application Web (Web 2.0): A platform for dynamic applications where the connections are user interactions, social ties, and data exchange via APIs.\n",
    "3. Web of Data (Semantic Web): A network of machine-readable data, not pages. The connections are defined relationships between concepts, forming a global knowledge graph.\n",
    "\n",
    "These layers coexist and intersect. A modern website is a document (1), that hosts an interactive application (2), and contains structured data for machines (3).\n",
    "\n",
    "---\n",
    "\n",
    "## What is Web Intelligence?\n",
    "- Intelligent ways to extract information and knowledge from the web:\n",
    "- finding relevant information available on the web\n",
    "- obtaining new knowledge by analyzing web data: the web itself, but also how it evolves, and how users interact on and with the\n",
    "web\n",
    "Some applications:\n",
    "- Intelligent Search\n",
    "- Recommender Systems\n",
    "- Business Analytics\n",
    "- Crowd Sourcing\n",
    "- Not so nice ones: advertising, manipulation, surveillance\n",
    "---\n",
    "## Intelligent Search\n",
    "\n",
    "- Keyword Search: the words in the query appear frequently in\n",
    "the document, in any order (bag of words).\n",
    "- Disadvantages:\n",
    "- May not retrieve relevant documents that include\n",
    "synonymous terms (e.g., cannot distinguish between\n",
    "“restaurant” and “café”)\n",
    "- May retrieve irrelevant documents that include\n",
    "ambiguous terms (e.g., cannot distinguish between “bat”\n",
    "mammal and “bat” baseball)\n",
    "- Beyond Keywords:\n",
    "- Considering the meaning of the words used\n",
    "- Adapting to user feedback (direct or indirect)\n",
    "- Considering the authority of the source\n",
    "---\n",
    "## Recommender Systems\n",
    "Kinds of recommendations:\n",
    "- Product Based (collaborative filtering):\n",
    "similar books\n",
    "- User- Based (content based filtering):\n",
    "based on search history\n",
    "- Hybrid\n",
    "---\n",
    "## Node classification\n",
    "We can represent a domain as a network of nodes, each node represents a different category.\n",
    "- Example: suppose aau.dk is a domain and is connected by multiple nodes, then each color represents something different like research projects, educational programs, etc...\n",
    "---\n",
    "## Levels of the Web\n",
    "We can distinguish 3 levels of modeling and analytics:\n",
    "1. Web Content\n",
    "2. Web Structure\n",
    "3. Web Dynamics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87199867",
   "metadata": {},
   "source": [
    "# LECTURE 09/09/2025\n",
    "---\n",
    "# NLP BASICS\n",
    "\n",
    "Natural Language Processing is a sub field of Artificial Intelligence  and Computational LInguistics, comprising of computational methods for understanding or generating Natural Languages.\n",
    "The goals of this process is to:\n",
    "- read human language into machine understandable\n",
    "- decipher human languages\n",
    "- understand and makes sense of the text\n",
    "\n",
    "Natural Languages consist of:\n",
    "- phonology: sounds of the words\n",
    "- semantics: meaning of the words\n",
    "- syntax: grammatical rules according to which words are put together\n",
    "\n",
    "It is really difficult to analyze the text because of the ambiguity of the language, for example a word could have 2 different meanings depending on the context, or the positioning of a word could change the meaning, etc...\n",
    "\n",
    "The data of languages is unstructured and in order to make it machine understandable is to tokenize it however this process is really language dependant.\n",
    "\n",
    "---\n",
    "## Tokenization\n",
    "\n",
    "Given the phrase:\n",
    "\n",
    "J.K. Rowling’s book, Harry Potter and the Philosopher's Stone was published in 1997. It’s still a bestseller and truly amazing\n",
    "how much people enjoy it!\n",
    "\n",
    "- Word level tokenization:\n",
    "Tokenized words: [\"J.K.\", \"Rowling’s\", \"book\", \",\", \"Harry\", \"Potter\", \"and\", \"the\", \"Philosopher's\", \"Stone\", \"was\", \"published\", \"in\",\n",
    "\"1997\", \".\", \"It’s\", \"still\", \"a\", \"bestseller\", \"and\", \"truly\", \"amazing\", \"how\", \"much\", \"people\", \"enjoy\", \"it\", \"!\"]\n",
    "- Handling contractions:\n",
    "The contraction \"it’s” needs to be split into two tokens: ['it', \"'s\"]. Some systems might tokenize this as two tokens\n",
    "- Punctuation handling:\n",
    "Commas, apostrophes, and periods are separated from words, ensuring they are individual tokens\n",
    "- Named entity recognition (NER):\n",
    "The book title and the author are multi-word entities, which ideally should be handled as a single token or entity for proper semantic\n",
    "analysis.\n",
    "  - Harry Potter and the Philosopher's Stone\n",
    "  - J.K. Rowlin\n",
    "\n",
    "Most tokenizers are rule-based and have differental conventions for example for the world \"don't\":\n",
    "- Peen Treebank: it will divide the word into \"do\" and \"n't\"\n",
    "- Moses: it will divide the word into \"don\" and \"'t\"\n",
    "In this case moses is rule-based.\n",
    "\n",
    "Also we need to take into consideration composed words or names since the proces does involte removal of white spaces but it does not add them.\n",
    "Another important factor is the encoding of the text since the same symbol could have different ASCII values (in decimal) depending on the encoding.\n",
    "\n",
    "### Tokenization in Encoding\n",
    "\n",
    "**Tokenization** in encoding assigns a unique value (token) to each word so that the system can recognize and process words based on their assigned values.\n",
    "Example:\n",
    "\n",
    "I live in Copenhagen = [“I”, ”live”, “in”, “Copenhagen”]\\\n",
    "I = 1\\\n",
    "live = 2\\\n",
    "in = 3\\\n",
    "Copenhagen = 4\\\n",
    "\n",
    "We live in Copenhagen = [“We”, ”live”, “in”, “Copenhagen”]\\\n",
    "We = 5\\\n",
    "live = 2\\\n",
    "in = 3\\\n",
    "Copenhagen = 4\\\n",
    "\n",
    "---\n",
    "\n",
    "## Normalization\n",
    "\n",
    "This process transforms distinct 'equivalent' tokens into one normalized form, e.g.:\n",
    "- lower-casing: This -> this\n",
    "- use non-hyphonated words: anti-discriminatory -> antidiscriminatory\n",
    "- delete periods: U.S.A. -> USA or usa\n",
    "\n",
    "However we need to balance the normalization because the result will change depending on how the amount of normalization:\n",
    "- more normalization: means that if you search the word U.S.A. it also returns the results for usa or USA\n",
    "- less normalization: if you search for the word C.A.T. it doesn't show the results of cat\n",
    "\n",
    "---\n",
    "\n",
    "## Stop words\n",
    "These are common words that usually do not add significant meaning to a sentence and can be filtered out to reduce noise in text analysis.\n",
    "However the removal of stop words could cause issues.\n",
    "For example in a phrase such as \"to be or not to be\" which is full of stop words but are important part of the expression.\n",
    "\n",
    "---\n",
    "\n",
    "## Stemming\n",
    "It's a similar process to normalization in which it reduces and removes their different grammatical variants and transforms them into their underlying word.\n",
    "Here are a few examples:\n",
    "- learn, learnds, learned -> learn\n",
    "- organize, organizer, organization -> organ\n",
    "\n",
    "The terms that result from stemming will be included in the index.\n",
    "\n",
    "### Porter introduced an algorithm for the english language (1980):\n",
    "\n",
    "- A rule-based stemmer with rules for mostly suffix-stripping such as:\n",
    "- “ing” → “-” connecting → connect\n",
    "- “sses” → “ss” caresses → caress\n",
    "- “ies” → “i” ponies → poni\n",
    "- “s” → “-” cats → cat\n",
    "- `[C](VC){m}[V]`\n",
    "where C is the consonant, V is the vowel i.e., A,E,I,O,U and other than Y preceded by a consonant, m>0 is the number of times (VC) occurs.\n",
    "- m = 0, TREE, BY\n",
    "- m = 1, TROUBLE, OATS, TREES\n",
    "- m = 2, TROUBLES, OATEN, PRIVATE\n",
    "- The rules for removing suffix: (condition) S1 -> S2\n",
    "\n",
    "Porter’s Stemming Algorithm\n",
    "- `[C](VC){m}[V]`\n",
    "- Stem the word “REPLACEMENT”\n",
    "- m >1, remove EMENT\n",
    "- Generates a stem “replac”\n",
    "- Advantage:\n",
    "  - It produces the best output as compared to other stemmers, and it has less error rate\n",
    "- Disadvantage:\n",
    "  - Morphological variants produced are not always real words (produces stems)\n",
    "\n",
    "Use the algorithm to stem the word:\n",
    "MULTIDIMENSIONAL\n",
    "Remove common suffix: from this word there is no ing, sses, ies, s, ation, ization, etc...\n",
    "Removal of the suffix AL\n",
    "  - M U L T I D I M E N S I O N A L\n",
    "  - C V C C V C V C V C C V V C\n",
    "  -   V C   V C V C V C     V C\n",
    "  - m = 5\n",
    "\n",
    "There are three criteria for evaluating stemmers:\n",
    "- Correctness\n",
    "- Efficiency of the task\n",
    "- Compression performance\n",
    "\n",
    "However stemming is also language dependant (for example it does not work with Chinese), also stemming could cause an error depending on the amount:\n",
    "- overstemming: can remove too much from a word changing its meaning.\n",
    "- understemming: if wrongly reduced it can lead to different root words.\n",
    "\n",
    "---\n",
    "\n",
    "## Zipf's law\n",
    "Zipf's flaw in NLP describes the power-law distribution of word frequencies in a corpus, stating that the frequency of a word is inversely proportional to its rank.\n",
    "\n",
    "Example:\n",
    "“The cat sat on the mat, and the dog sat on the mat.”\n",
    "Tokenize:\n",
    "[“The”, “cat”, “sat”, “on”, “the”, “mat”, “,”, “and”, “the”, “dog”, “sat”, “on”,“the”, “mat”, ”.”] -> results in 13\n",
    "Type: a unique word\n",
    "Token: an instance of a type in a corpus (including repetitions)\n",
    "Word Frequencies:\n",
    "the: 3\n",
    "sat: 2\n",
    "on: 2\n",
    "mat: 2\n",
    "cat: 1\n",
    "dog: 1\n",
    "and: 1\n",
    "\n",
    "So the type/token ratio is 7/13, therefore more data -> lower type/token ratio\n",
    "\n",
    "Positive Implications of Zipf's law:\n",
    "- Any document/text will contain a number of words that are very common.\n",
    "- Help us understand the structure (and possibly meaning) of the text\n",
    "Negative Implications:\n",
    "- Any document/text will have a large number of rare words known as Out-of-vocabulary (OOV) words\n",
    "Handling OOV words:\n",
    "- Replace the set of OOV words in the training data with an unknown word token (-UNK)\n",
    "- Use of statistical models\n",
    "- Use sub-string based representations such as Byte-Pair Encoding\n",
    "Byte-Pair Encoding: learn which character sequences are common in the vocabulary of the language, and treat those common sequences\n",
    "as atomic units of the vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocessing\n",
    "Is the process in which a sentence is broken down through:\n",
    "- Tokenization\n",
    "- Normalization\n",
    "- Stop word removal\n",
    "- Stemming\n",
    "\n",
    "Definitions: \n",
    "- Corpus: collection of documents (e.g., all web-pages that have been crawled)\n",
    "- Raw corpora have only minimal (or no) processing:\\\n",
    "Sentence boundaries may or may not be identified.\\\n",
    "There may or may not be metadata.\\\n",
    "Typos (written text) or disfluencies (spoken language) may or may not be corrected.\n",
    "- Annotated corpora contain some labels (e.g. POS tags, sentiment labels), or linguistic structures (e.g syntax trees,\n",
    "semantic interpretations), etc.\n",
    "- Vocabulary: all terms that appear in the corpus\n",
    "\n",
    "---\n",
    "\n",
    "## Part of Speech (POS)\n",
    "Part-of-speech tagging, or POS tagging, is a task that entails\n",
    "classifying words in a text according to their grammatical\n",
    "categories (such as noun, verb, and adjective).\\\n",
    "Advantages:\n",
    "- Helps in identifying the syntactic structure\n",
    "- POS tagging often helps disambiguate the meaning of such\n",
    "words based on their role in the sentence.\n",
    "- Aids Named Entity Recognition\n",
    "\n",
    "---\n",
    "\n",
    "## String Similarity\n",
    "String similarity can be measured through:\n",
    "- Hamming Distance: between two equal-length strings of symbols is the number of positions at which the corresponding symbols are different.\n",
    "- Levenshtein Distance: measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another.\n",
    "- Jaccard Similarity: is a statistic to determine the similarity or the overlap between the two sets. Jaccard similarity = $\\frac{A \\cap B}{A \\cup B}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf0bf7",
   "metadata": {},
   "source": [
    "# LECTURE 16/09/2025\n",
    "\n",
    "# Text Processing\n",
    "\n",
    "\n",
    "## Scoring with Jaccard Coefficient\n",
    "\n",
    "The Jaccard Coefficient measures similarity by dividing the **size of the overlap** between two sets by the **size of their combined total**. It essentially asks: \"Of all the items present, what fraction are in both sets?\"\n",
    "\n",
    "The formula is:\n",
    "$$Jaccard(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "* **$|A \\cap B|$ (Intersection):** The number of items found in **both** set A and set B.\n",
    "* **$|A \\cup B|$ (Union):** The total number of **unique** items across both sets combined.\n",
    "\n",
    "\n",
    "\n",
    "[Image of a Venn diagram showing intersection and union]\n",
    "\n",
    "\n",
    "The score is always between 0 and 1:\n",
    "* **1** means the sets are a **perfect match** (identical).\n",
    "* **0** means the sets have **no items in common**.\n",
    "\n",
    "\n",
    "### Worked Example\n",
    "\n",
    "Let's find the similarity between a query and two sentences.\n",
    "\n",
    "* **Query (Q):** `{\"march\"}`\n",
    "* **Sentence 1 (S1):** `{\"Memories\", \"in\", \"March\", \"is\", \"a\", \"good\", \"movie\"}`\n",
    "* **Sentence 2 (S2):** `{\"Flowers\", \"start\", \"blooming\", \"in\", \"March\"}`\n",
    "\n",
    "**1. Jaccard(Q, S1)**\n",
    "* **Overlap:** `{\"march\"}` (Size = **1**)\n",
    "* **Total Unique Words:** `{\"march\", \"Memories\", \"in\", \"is\", \"a\", \"good\", \"movie\"}` (Size = **7**)\n",
    "* **Score:** $\\frac{1}{7}$\n",
    "\n",
    "**2. Jaccard(Q, S2)**\n",
    "* **Overlap:** `{\"march\"}` (Size = **1**)\n",
    "* **Total Unique Words:** `{\"march\", \"Flowers\", \"start\", \"blooming\", \"in\"}` (Size = **5**)\n",
    "* **Score:** $\\frac{1}{5}$\n",
    "\n",
    "**Conclusion:** Sentence S2 is considered more similar to the query, as its score ($\\frac{1}{5}$) is higher than S1's ($\\frac{1}{7}$).\n",
    "\n",
    "### Limitations of Jaccard's coefficient\n",
    "- Term frequency (no. of occurrences of a term) is not considered\n",
    "- Rare terms in a collection are more informative than frequent terms. Jaccard doesn’t consider this information\n",
    "  - For e.g., adjectives such as good, beautiful, etc.\n",
    "- Order of the terms not considered\n",
    "\n",
    "---\n",
    "\n",
    "## Text Similarity from Unstructured Data\n",
    "It's a technique used by computers to measure how alike two pieces of raw text are. The goal is to take text from sources like articles, emails, or social media posts and produce a similarity score that quantifies their relationship.\n",
    "\n",
    "---\n",
    "\n",
    "## Ranked Retrieval Model\n",
    "- In a ranked retrieval model, the system returns an ordering over the (top) documents in the collection for a query\n",
    "- More relevant results are ranked higher than less relevant ones\n",
    "- Queries are in form of one or more words in natural language text (human languages) and not query languages\n",
    "- Large result set is not an issue in ranked retrieval systems, usually top k ≈ 10 results are returned\n",
    "- The output documents (results) are ranked according to how relevant they are to a query, i.e., how well document and query\n",
    "“match”\n",
    "- A score – say in [0, 1] – is assigned to each document\n",
    "For e.g., query term is “Aalborg”\n",
    "- Score =0, if “Aalborg” does not occur in the document\n",
    "- More frequent the occurrence of “Aalborg” in the document, higher the score.\n",
    "\n",
    "---\n",
    "\n",
    "## Binary Term-Document Incidence Matrix\n",
    "A binary term-document incidence matrix is a simple way to represent the presence or absence of terms in documents. Each row corresponds to a term, and each column corresponds to a document. A cell contains a 1 if the term is present in the document, and a 0 if it is absent.\n",
    "Each document is represented by a binary vector ∈ {0,1}^|V|\n",
    "\n",
    "There is also a Count Vector which is the same thing but instead of 1 it returns the amount of occurrences of that word in the document.\n",
    "Each document is a count vector (i.e., each column) ∈ ℕ^|V|\n",
    "\n",
    "---\n",
    "\n",
    "## Bag of Words model\n",
    "The Bag of Words (BoW) model is a simple and widely used method for representing text data in natural language processing. In this model, a text (such as a sentence or a document) is represented as an unordered collection of words, disregarding grammar and word order but keeping multiplicity.\n",
    "\n",
    "Vector representation doesn’t consider the ordering of words in a document\n",
    "Let's consider a small dataset of three sentences:\n",
    "  1. Sentence 1: \"I like programming.\"\n",
    "  2. Sentence 2: \"Programming is fun.\"\n",
    "  3. Sentence 3: \"I like to code.”\n",
    "Vocabulary: [\"I\", ”like\", \"programming\", \"is\", \"fun\", \"to\", \"code\"]\n",
    "Sentence 1: \"I like programming.” Vector: [1, 1, 1, 0, 0, 0, 0]\n",
    "Sentence 2: \"Programming is fun.” Vector: [0, 0, 1, 1, 1, 0, 0]\n",
    "Sentence 3: \"I like to code.” Vector: [1, 1, 0, 0, 0, 1, 1]\n",
    "Represents text as a collection of word counts, enabling the analysis and processing of text data while ignoring the order of words\n",
    "\n",
    "Another example:\n",
    "\n",
    "Sentence 1: John runs faster than Mary.\n",
    "Sentence 2: Mary runs faster than John.\n",
    "Generate the vectors\n",
    "Vocabulary: [\"John\", \"runs\", \"faster\", \"than\", \"Mary\"]\n",
    "  - Sentence 1: \"John runs faster than Mary.\"\n",
    "    • Vector: [1, 1, 1, 1, 1]\n",
    "  - Sentence 2: \"Mary runs faster than John.\"\n",
    "    • Vector: [1, 1, 1, 1, 1]\n",
    "\n",
    "---\n",
    "\n",
    "## Term Frequency\n",
    "\n",
    "Term Frequency\n",
    "The term frequency tft,d of term t in document d is defined as the number of times that t occurs in d.\n",
    "Raw term frequency is not what we want:\n",
    "  - A document with 10 occurrences of the term is more relevant than a document with 1 occurrence of the term.\n",
    "  - But not 10 times more relevant\n",
    "Relevance does not increase proportionally with term frequency\n",
    "\n",
    "tf(t,d) = $\\frac{Number of times term t appears in document d}{Total number of terms in document d}$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Log-Frequency Weighting\n",
    "\n",
    "Log-frequency weighting is a method to score a document's relevance to a query. It improves on simple word counting by using a logarithm to reduce the impact of highly frequent words, reflecting the idea of **diminishing returns**.\n",
    "\n",
    "### Formulas\n",
    "\n",
    "The weight `$w$` for a term `t` with a term frequency `$tf_{t,d}$` in a document `d` is:\n",
    "\n",
    "$$w_{t,d} = 1 + \\log_{10}(tf_{t,d})$$\n",
    "(This is calculated only if the term is present in the document, i.e., `$tf_{t,d} > 0$`).\n",
    "\n",
    "\n",
    "\n",
    "The final **relevance score** for the document is the sum of these weights for all terms that appear in **both** the query `q` and the document `d`:\n",
    "\n",
    "$$Score(q, d) = \\sum_{t \\in q \\cap d} (1 + \\log_{10}(tf_{t,d}))$$\n",
    "\n",
    "examples:\n",
    "\n",
    "Sentence 1: \"The dog barks loudly\"\n",
    "\n",
    "First, let's process the sentence and find the term frequencies (tf). We'll ignore common \"stop words\" like \"The\" and punctuation.\n",
    "\n",
    "* **Document (D):** \"The dog barks loudly.”\n",
    "* **Terms:** `{\"dog\", \"barks\", \"loudly\"}`\n",
    "* **Term Frequencies:**\n",
    "    * `tf` of \"dog\" = 1\n",
    "    * `tf` of \"barks\" = 1\n",
    "    * `tf` of \"loudly\" = 1\n",
    "\n",
    "Next, we calculate the log-frequency weight for each term using the formula $w = 1 + \\log_{10}(tf)$:\n",
    "\n",
    "* Weight of \"dog\": $1 + \\log_{10}(1) = 1 + 0 = 1.0$\n",
    "* Weight of \"barks\": $1 + \\log_{10}(1) = 1 + 0 = 1.0$\n",
    "* Weight of \"loudly\": $1 + \\log_{10}(1) = 1 + 0 = 1.0$\n",
    "\n",
    "Step 2: Calculate Scores for Assumed Queries\n",
    "\n",
    "The final score is the sum of the weights for the terms that appear in **both** the query and the document. 🐾\n",
    "\n",
    "#### **Query 1: \"barks\"**\n",
    "The only matching term is \"barks\".\n",
    "* **Score** = Weight(\"barks\") = **1.0**\n",
    "\n",
    "#### **Query 2: \"loud dog\"**\n",
    "The matching terms are \"loudly\" and \"dog\".\n",
    "* **Score** = Weight(\"loudly\") + Weight(\"dog\") = `1.0 + 1.0` = **2.0**\n",
    "\n",
    "#### **Query 3: \"the cat\"**\n",
    "There are no matching terms.\n",
    "* **Score** = **0.0**\n",
    "\n",
    "---\n",
    "\n",
    "## Document Frequency (df)\n",
    "\n",
    "The core idea in search relevance is that not all words are equally important. **Frequent terms are less informative than rare terms.**\n",
    "\n",
    "Think of it like searching for a name in a phone book. The name \"Smith\" appears in thousands of entries (high frequency), so a match is not very informative. The name \"Pendleton\" is rare (low frequency), so a match is a very strong signal you've found the right person.\n",
    "\n",
    "To quantify this, we use **document frequency ($df_t$)**, which is the number of documents in a collection that contain a specific term `t`. A high `$df_t$` means the term is common; a low `$df_t$` means it's rare.\n",
    "\n",
    "---\n",
    "\n",
    "## Inverse Document Frequency (idf)\n",
    "\n",
    "While `df` tells us how common a term is, for scoring we want a value that is **high** for rare, informative terms and **low** for common ones. This is exactly what **Inverse Document Frequency (idf)** provides. It's a measure of a term's importance or informativeness. ⚖️\n",
    "\n",
    "The formula is:\n",
    "$$idf_t = \\log_{10}\\left(\\frac{N}{df_t}\\right)$$\n",
    "\n",
    "Let's break it down:\n",
    "* **$N$** is the total number of documents in the collection.\n",
    "* **$\\frac{N}{df_t}$** is the \"inverse\" part. This fraction will be large for rare terms (e.g., a term in 10 documents out of 1,000,000) and small for common terms (e.g., a term in 500,000 documents out of 1,000,000).\n",
    "* **$\\log_{10}(...)$** is used to \"dampen\" the effect. This ensures that very rare terms don't completely dominate the score. It smooths the weights so they don't grow exponentially.\n",
    "\n",
    "The result is that terms with a high `idf` are considered significant and are given more weight in relevance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is one of the most important and widely used weighting schemes in information retrieval. It's a score that evaluates how relevant a word is to a document within a collection of documents.\n",
    "\n",
    "The core idea is to find a balance: a word is important if it appears **frequently in one document** (high TF), but **rarely across all other documents** (high IDF). 🎯\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "#### **Term Frequency (TF)**\n",
    "This measures how often a term appears in a **single document**. The more a term appears, the more important it is *to that specific document*. We often use the log-weighted formula to dampen the effect of very high counts.\n",
    "$$tf-weight_{t,d} = 1 + \\log_{10}(tf_{t,d})$$\n",
    "\n",
    "#### **Inverse Document Frequency (IDF)**\n",
    "This measures how **rare and informative** a term is across the entire collection of documents. Common words like \"the\" or \"is\" will have a very low IDF score, while specific, rare terms will have a high IDF score.\n",
    "$$idf_t = \\log_{10}\\left(\\frac{N}{df_t}\\right)$$\n",
    "\n",
    "\n",
    "*The TF-IDF Weight*\n",
    "\n",
    "To get the final TF-IDF weight for a term `t` in a document `d`, you simply multiply its TF weight by its IDF weight.\n",
    "\n",
    "$$w_{t,d} = \\text{tf-weight}_{t,d} \\times idf_t$$\n",
    "\n",
    "Or, written out in full:\n",
    "$$w_{t,d} = (1 + \\log_{10}(tf_{t,d})) \\times \\log_{10}\\left(\\frac{N}{df_t}\\right)$$\n",
    "\n",
    "A high TF-IDF score indicates that a term is a strong keyword or topic for that particular document.\n",
    "\n",
    "\n",
    "*Scoring a Document for a Query*\n",
    "\n",
    "To calculate the total relevance score of a document for a given query, you sum the TF-IDF weights of all terms that appear in **both** the query and the document.\n",
    "\n",
    "$$Score(q, d) = \\sum_{t \\in q \\cap d} w_{t,d}$$\n",
    "\n",
    "### TF-IDF Variants\n",
    "tf(t,d) = $\\frac{Number of times term t appears in document d}{Total number of terms in document d}$\n",
    "\n",
    "---\n",
    "\n",
    "## Vector Similarity in Information Retrieval – Use Case\n",
    "- Threshold:\n",
    "  - For query q, retrieve all documents with similarity above a threshold, e.g., similarity > 0.50.\n",
    "- Ranking:\n",
    "  - For query q, return the n most similar documents ranked in order of similarity.\n",
    "\n",
    "---\n",
    "\n",
    "## Documents and Queries as Vectors\n",
    "- Documents\n",
    "  - So we have a |V|-dimensional vector space\n",
    "  - Terms are axes of the space\n",
    "  - Documents are points or vectors in this space\n",
    "  - Very high-dimensional: tens of millions of dimensions when you apply this to a web search engine\n",
    "  - These are very sparse vectors - most entries are zero.\n",
    "- Queries\n",
    "  - Key idea 1: Do the same for queries: represent them as vectors in the space\n",
    "  - Key idea 2: Rank documents according to their proximity to the query in this space\n",
    "  - proximity = similarity of vectors\n",
    "  - proximity ≈ inverse of distance\n",
    "  - Recall: We do this because we want to get away from the you’re-either-in-or-out Boolean model.\n",
    "  - Instead: rank more relevant documents higher than less relevant documents\n",
    "\n",
    "---\n",
    "\n",
    "## Comparing Vectors: Euclidean Distance vs. Cosine Similarity\n",
    "\n",
    "When comparing two vectors (e.g., representing a query and a document), we can use different metrics to measure their relationship. Two common methods are Euclidean distance and Cosine similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "**Euclidean distance** measures the straight-line distance between the endpoints of two vectors. It calculates how \"far apart\" the two vectors are in a multi-dimensional space.\n",
    "\n",
    "\n",
    "$$\n",
    "d(q,p) = \\sqrt{{(p_1 - q_1)^2}-{(p_2 - q_2)^2}}\n",
    "$$\n",
    "\n",
    "A major drawback of this method for document analysis is that **it is highly sensitive to vector length (magnitude)**. For example, a very long document and a short document on the same topic will have a large Euclidean distance, suggesting they are dissimilar even though they are thematically related.\n",
    "\n",
    "---\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "**Cosine similarity** measures the cosine of the angle between two vectors. Instead of measuring distance, it measures orientation. This makes it an excellent choice for tasks like document similarity where the topic (direction) is more important than the document's length (magnitude).\n",
    "\n",
    "The formula to find the cosine of the angle ($\\theta$) between a query vector ($\\vec{q}$) and a document vector ($\\vec{d}$) is:\n",
    "\n",
    "$$\n",
    "\\text{similarity} = \\cos(\\theta) = \\frac{\\vec{q} \\cdot \\vec{d}}{||\\vec{q}|| \\ ||\\vec{d}||} = \\frac{\\sum_{i=1}^{n} q_i d_i}{\\sqrt{\\sum_{i=1}^{n} q_i^2} \\sqrt{\\sum_{i=1}^{n} d_i^2}}\n",
    "$$\n",
    "\n",
    "The angle between the vectors indicates their similarity:\n",
    "* An angle of **0°** corresponds to a cosine value of **1**, representing **maximum similarity**. The vectors are pointing in the same direction.\n",
    "* An angle of **90°** corresponds to a cosine value of **0**, representing **no similarity**. The vectors are orthogonal.\n",
    "\n",
    "#### Ranking Results\n",
    "Since the cosine function is monotonically decreasing on the interval $[0^\\circ, 180^\\circ]$, a smaller angle implies a higher cosine value (and thus higher similarity). Therefore, to find the most relevant documents for a query:\n",
    "* You rank documents in **decreasing order** of the cosine similarity score (from 1 down to 0).\n",
    "* This is equivalent to ranking documents in **increasing order** of the angle between the query and document (from 0° up to 90°).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498ddb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b289176",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b376f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3a8deb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f03f0793",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f6ec100",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45407d1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
